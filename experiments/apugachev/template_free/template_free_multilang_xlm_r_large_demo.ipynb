{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecdeae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import os, operator\n",
    "from progressbar import progressbar as pb\n",
    "from nltk import word_tokenize\n",
    "from typing import List, Tuple, Dict\n",
    "import json\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForMaskedLM\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "\n",
    "import gdown #! pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aea73f-9bb9-4d3a-82f3-689c9067d047",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! gdown https://drive.google.com/uc?id=1-ZYVbUN691AD6hsMbkJfGsrC3zSayU4z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc250ef-3be3-4ec8-b74c-5fdb1b0a550c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! tar xzf trained_model_template_free.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4555b857",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../SemEval2022-Task11_Train-Dev/\"\n",
    "pretrained_model_path = \"../../pretrained/xlm-roberta-large/\"\n",
    "trained_model_path = \"trained_model_template_free/\"\n",
    "device = 'cuda'\n",
    "\n",
    "max_len = 96\n",
    "batch_size = 24\n",
    "num_epochs = 25\n",
    "patience = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7623f549",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"label2word_label.json\") as f:\n",
    "    label2word_label = json.load(f)\n",
    "    \n",
    "word_label2label = {}\n",
    "\n",
    "for lang, val in label2word_label.items():\n",
    "    word_label2label[lang] = {v:k for k, v in val.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b999c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_files = []\n",
    "\n",
    "for folder in os.listdir(data_path):\n",
    "    files = os.listdir(os.path.join(data_path, folder))\n",
    "    dev_file = files[0] if \"dev\" in files[0] else files[1]\n",
    "    \n",
    "    dev_files.append(os.path.join(data_path, folder, dev_file))\n",
    "    \n",
    "len(dev_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8c06e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_conll(files) -> Tuple[Dict, Dict]:\n",
    "    \n",
    "    fin_texts, fin_labels = {}, {}\n",
    "    \n",
    "    for filename in pb(files):\n",
    "    \n",
    "        with open(filename) as f:\n",
    "            data = f.read().splitlines()\n",
    "\n",
    "        lang = os.path.basename(filename).split(\"_\")[0].upper()\n",
    "        texts, labels = [], []\n",
    "\n",
    "        for row in data:\n",
    "            if row.startswith(\"# id \"):\n",
    "                new_texts, new_labels = [], []\n",
    "                continue\n",
    "\n",
    "            if row == \"\":\n",
    "                texts.append(new_texts)\n",
    "                labels.append(new_labels)\n",
    "\n",
    "            else:\n",
    "                parts = row.split()\n",
    "                new_texts.append(parts[0])\n",
    "                new_labels.append(parts[-1])\n",
    "                \n",
    "        fin_texts[lang] = texts\n",
    "        fin_labels[lang] = labels\n",
    "\n",
    "    return fin_texts, fin_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0fea6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    }
   ],
   "source": [
    "dev_texts, dev_labels = parse_conll(dev_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f1c832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for texts, labels in zip(dev_texts.values(), dev_labels.values()):\n",
    "    assert len(texts) == 800\n",
    "    assert len(labels) == 800"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacca353",
   "metadata": {},
   "source": [
    "# Prepare Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "436c96c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_target(tokens, labels, lang, label2word_label=label2word_label):\n",
    "    \"\"\"\n",
    "        Replace entities with label words\n",
    "    \"\"\"\n",
    "    new_tokens = []\n",
    "    for token, label in zip(tokens, labels):\n",
    "        if label.startswith(\"B-\"):\n",
    "            prefix, tag = label.split(\"-\")\n",
    "            new_token = label2word_label[lang][tag]\n",
    "            new_tokens.append(new_token)\n",
    "        elif label.startswith(\"I-\"):\n",
    "            continue\n",
    "        else:\n",
    "            new_tokens.append(token)\n",
    "    \n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2216d6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    }
   ],
   "source": [
    "dev_targets = defaultdict(list)\n",
    "\n",
    "for lang in pb(dev_texts):        \n",
    "    texts, labels = dev_texts[lang], dev_labels[lang]\n",
    "    \n",
    "    for text, label in zip(texts, labels):\n",
    "        target = prepare_target(text, label, lang)\n",
    "        dev_targets[lang].append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "204c0c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "for targets, labels in zip(dev_targets.values(), dev_labels.values()):\n",
    "    assert len(targets) == 800\n",
    "    assert len(labels) == 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54fe6c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8800"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data = []\n",
    "\n",
    "for lang in dev_texts:\n",
    "    for x, y in zip(dev_texts[lang], dev_targets[lang]):\n",
    "        x = \" \".join(x)\n",
    "        y = \" \".join(y)\n",
    "        dev_data.append((x,y))\n",
    "        \n",
    "len(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8dc5e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('важным традиционным промыслом является производство пальмового масла .',\n",
       " 'важным традиционным промыслом является производство dvd .')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data[3200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb876a33",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c8abf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = XLMRobertaTokenizer.from_pretrained(pretrained_model_path, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16252f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = XLMRobertaForMaskedLM.from_pretrained(trained_model_path, local_files_only=True)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2b1e274",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (8800 of 8800) |####################| Elapsed Time: 0:00:05 Time:  0:00:05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((8800, 96), (8800, 96))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_data(data, max_len=max_len):\n",
    "    X_data, y_data = [], []\n",
    "\n",
    "    for item in pb(data):\n",
    "        x, y = item\n",
    "        x_enc = tokenizer.encode(x, max_length=max_len, padding=\"max_length\", truncation=True)\n",
    "        y_enc = tokenizer.encode(y, max_length=max_len, padding=\"max_length\", truncation=True)\n",
    "        X_data.append(x_enc)\n",
    "        y_data.append(y_enc)\n",
    "        \n",
    "    return np.array(X_data), np.array(y_data)\n",
    "    \n",
    "X_dev, y_dev = encode_data(dev_data)\n",
    "X_dev.shape, y_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15f96c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8800, 2, 96)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev = np.stack((X_dev, y_dev), axis=1)\n",
    "dev_batches = DataLoader(dev, batch_size=batch_size, shuffle=False)\n",
    "dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19e35cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(trained_model=model, batches=dev_batches, tokenizer=tokenizer):\n",
    "    pred_labels = []\n",
    "\n",
    "    for item in pb(batches):\n",
    "        item = item[:, 0, :]\n",
    "        out = trained_model(item.to(device))\n",
    "        logits = out.logits\n",
    "        tokens_encoded = logits.argmax(axis=-1).tolist()\n",
    "        for enc in tokens_encoded:\n",
    "            decoded = tokenizer.decode(enc, skip_special_tokens=True)\n",
    "            pred_labels.append(decoded)\n",
    "        \n",
    "    return pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "157da00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (367 of 367) |######################| Elapsed Time: 0:01:32 Time:  0:01:32\n"
     ]
    }
   ],
   "source": [
    "dev_x = [item[0] for item in dev_data]\n",
    "dev_y_true = [item[1] for item in dev_data]\n",
    "dev_y_pred = get_predictions(model, dev_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f461099e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'настоящие лемуры почти исключительно травоядны : они питаются цветами, dvd, листьями, однако в неволе известны примеры питания насекомыми.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idd = 800*4 + 70 # 800*2 + 80    -- *495\n",
    "dev_y_pred[idd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a490710d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'настоящие лемуры почти исключительно травоядны : они питаются цветами , dvd , листьями , однако в неволе известны примеры питания насекомыми .'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_y_true[idd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27c5d8d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'настоящие лемуры почти исключительно травоядны : они питаются цветами , фрукт , листьями , однако в неволе известны примеры питания насекомыми .'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_x[idd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8080d824",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2lang = {}\n",
    "\n",
    "for key, items in dev_texts.items():\n",
    "    for item in items:\n",
    "        sentence2lang[\" \".join(item)] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b6b36fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_texts2labels = defaultdict(dict)\n",
    "\n",
    "for lang in dev_texts:\n",
    "    for text, labels in zip(dev_texts[lang], dev_labels[lang]):\n",
    "        dev_texts2labels[lang][\" \".join(text)] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49f08db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pred_labels(input_sent, \n",
    "                      pred_sent, \n",
    "                      word_label2label=word_label2label, \n",
    "                      sentence2lang=sentence2lang,\n",
    "                      nltk_tokenizer=WhitespaceTokenizer()):\n",
    "    \n",
    "    lang = sentence2lang[input_sent]\n",
    "    res, matched_spans = [], []\n",
    "\n",
    "    pred_tokens = word_tokenize(pred_sent)\n",
    "    input_tokens = input_sent.split()\n",
    "\n",
    "    i, j = 0, 0  \n",
    "    res = []\n",
    "    \n",
    "    while i < len(pred_tokens) and j < len(input_tokens):\n",
    "        \n",
    "        pred = pred_tokens[i]\n",
    "        inp = input_tokens[j]\n",
    "        if pred == inp and pred not in word_label2label[lang]:\n",
    "            res.append(\"O\")\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif pred in word_label2label[lang]:\n",
    "            res.append(word_label2label[lang][pred])\n",
    "            j += 1\n",
    "            while j < len(input_tokens) and i+1 < len(pred_tokens) and input_tokens[j] != pred_tokens[i+1]:\n",
    "                res.append(word_label2label[lang][pred])\n",
    "                j += 1\n",
    "                \n",
    "            i += 1\n",
    "        elif pred != inp:\n",
    "            break\n",
    "\n",
    "    if len(res) < len(input_tokens):\n",
    "        res.extend([\"O\"] * (len(input_tokens) - len(res)))\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "94b7a703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_iob(labels):\n",
    "    iob_labels = []\n",
    "    \n",
    "    if labels[0] != \"O\":\n",
    "        iob_labels.append(\"B-\" + labels[0])\n",
    "    else:\n",
    "        iob_labels.append(\"O\")\n",
    "    \n",
    "    for i, label in enumerate(labels[1:], 1):\n",
    "\n",
    "        if label == \"O\":\n",
    "            iob_labels.append(\"O\")\n",
    "            \n",
    "        elif labels[i-1] == \"O\" or (label != labels[i-1] and labels[i-1] != \"O\"):\n",
    "            iob_labels.append(\"B-\" + label)\n",
    "            \n",
    "        elif not labels[i-1].startswith(\"O\") and label != \"O\":\n",
    "            iob_labels.append(\"I-\" + label)\n",
    "        \n",
    "        \n",
    "    return iob_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6a4a068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broken = 0\n",
    "dev_labels_pred = defaultdict(list)\n",
    "\n",
    "for x, y_pred in zip(dev_x, dev_y_pred):\n",
    "    lang = sentence2lang[x]\n",
    "    \n",
    "    true_labels = dev_texts2labels[lang][x]\n",
    "    try:\n",
    "        pred_labels = build_pred_labels(x, y_pred)\n",
    "    except:\n",
    "        broken += 1\n",
    "        pred_labels = [\"O\"] * len(true_labels)\n",
    "        \n",
    "    pred_labels = add_iob(pred_labels)\n",
    "    assert len(true_labels) == len(pred_labels)\n",
    "    \n",
    "    dev_labels_pred[lang].append((true_labels, pred_labels))\n",
    "broken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d28ca3e",
   "metadata": {},
   "source": [
    "# Calc metrics by span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5af79e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Set\n",
    "from overrides import overrides\n",
    "\n",
    "from allennlp.training.metrics.metric import Metric\n",
    "\n",
    "\n",
    "class SpanF1(Metric):\n",
    "    def __init__(self, non_entity_labels=['O']) -> None:\n",
    "        self._num_gold_mentions = 0\n",
    "        self._num_recalled_mentions = 0\n",
    "        self._num_predicted_mentions = 0\n",
    "        self._TP, self._FP, self._GT = defaultdict(int), defaultdict(int), defaultdict(int)\n",
    "        self.non_entity_labels = set(non_entity_labels)\n",
    "\n",
    "    @overrides\n",
    "    def __call__(self, batched_predicted_spans, batched_gold_spans, sentences=None):\n",
    "        non_entity_labels = self.non_entity_labels\n",
    "        for predicted_spans, gold_spans in zip(batched_predicted_spans, batched_gold_spans):\n",
    "            gold_spans_set = set([x for x, y in gold_spans.items() if y not in non_entity_labels])\n",
    "            pred_spans_set = set([x for x, y in predicted_spans.items() if y not in non_entity_labels])\n",
    "\n",
    "            self._num_gold_mentions += len(gold_spans_set)\n",
    "            self._num_recalled_mentions += len(gold_spans_set & pred_spans_set)\n",
    "            self._num_predicted_mentions += len(pred_spans_set)\n",
    "\n",
    "            for ky, val in gold_spans.items():\n",
    "                if val not in non_entity_labels:\n",
    "                    self._GT[val] += 1\n",
    "\n",
    "            for ky, val in predicted_spans.items():\n",
    "                if val in non_entity_labels:\n",
    "                    continue\n",
    "                if ky in gold_spans and val == gold_spans[ky]:\n",
    "                    self._TP[val] += 1\n",
    "                else:\n",
    "                    self._FP[val] += 1\n",
    "\n",
    "    @overrides\n",
    "    def get_metric(self, reset: bool = False) -> float:\n",
    "        all_tags: Set[str] = set()\n",
    "        all_tags.update(self._TP.keys())\n",
    "        all_tags.update(self._FP.keys())\n",
    "        all_tags.update(self._GT.keys())\n",
    "        all_metrics = {}\n",
    "\n",
    "        for tag in all_tags:\n",
    "            precision, recall, f1_measure = self.compute_prf_metrics(true_positives=self._TP[tag],\n",
    "                                                                     false_negatives=self._GT[tag] - self._TP[tag],\n",
    "                                                                     false_positives=self._FP[tag])\n",
    "            all_metrics['P@{}'.format(tag)] = precision\n",
    "            all_metrics['R@{}'.format(tag)] = recall\n",
    "            all_metrics['F1@{}'.format(tag)] = f1_measure\n",
    "\n",
    "        # Compute the precision, recall and f1 for all spans jointly.\n",
    "        precision, recall, f1_measure = self.compute_prf_metrics(true_positives=sum(self._TP.values()),\n",
    "                                                                 false_positives=sum(self._FP.values()),\n",
    "                                                                 false_negatives=sum(self._GT.values())-sum(self._TP.values()))\n",
    "        all_metrics[\"micro@P\"] = precision\n",
    "        all_metrics[\"micro@R\"] = recall\n",
    "        all_metrics[\"micro@F1\"] = f1_measure\n",
    "\n",
    "        if self._num_gold_mentions == 0:\n",
    "            entity_recall = 0.0\n",
    "        else:\n",
    "            entity_recall = self._num_recalled_mentions / float(self._num_gold_mentions)\n",
    "\n",
    "        if self._num_predicted_mentions == 0:\n",
    "            entity_precision = 0.0\n",
    "        else:\n",
    "            entity_precision = self._num_recalled_mentions / float(self._num_predicted_mentions)\n",
    "\n",
    "        all_metrics['MD@R'] = entity_recall\n",
    "        all_metrics['MD@P'] = entity_precision\n",
    "        all_metrics['MD@F1'] = 2. * ((entity_precision * entity_recall) / (entity_precision + entity_recall + 1e-13))\n",
    "        all_metrics['ALLTRUE'] = self._num_gold_mentions\n",
    "        all_metrics['ALLRECALLED'] = self._num_recalled_mentions\n",
    "        all_metrics['ALLPRED'] = self._num_predicted_mentions\n",
    "        if reset:\n",
    "            self.reset()\n",
    "        return all_metrics\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_prf_metrics(true_positives: int, false_positives: int, false_negatives: int):\n",
    "        precision = float(true_positives) / float(true_positives + false_positives + 1e-13)\n",
    "        recall = float(true_positives) / float(true_positives + false_negatives + 1e-13)\n",
    "        f1_measure = 2. * ((precision * recall) / (precision + recall + 1e-13))\n",
    "        return precision, recall, f1_measure\n",
    "\n",
    "    @overrides\n",
    "    def reset(self):\n",
    "        self._num_gold_mentions = 0\n",
    "        self._num_recalled_mentions = 0\n",
    "        self._num_predicted_mentions = 0\n",
    "        self._TP.clear()\n",
    "        self._FP.clear()\n",
    "        self._GT.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd0e1af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spans(labels):\n",
    "    fin_spans = []\n",
    "    for item_ in labels:\n",
    "\n",
    "        item = deepcopy(item_)\n",
    "        item.insert(0, \"O\")\n",
    "        item.append(\"O\")\n",
    "\n",
    "        new_spans = {}\n",
    "        for i, label in enumerate(item[1:-1], 1):\n",
    "\n",
    "            if item[i] == \"O\":\n",
    "                new_spans[(i-1, i-1)] = \"O\"\n",
    "            else:\n",
    "                if item[i-1] == 'O':\n",
    "                    start_i = i\n",
    "                if item[i+1] == 'O':\n",
    "                    new_spans[(start_i-1, i-1)] = item[i].split('-')[1]\n",
    "                    \n",
    "        fin_spans.append(new_spans)\n",
    "                \n",
    "    return fin_spans    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39cd943e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}\n",
    "\n",
    "for lang in [\"BN\", \"DE\", \"ES\", \"TR\", \"FA\", \"RU\", \"ZH\", \"NL\", \"KO\", \"EN\", \"HI\"]:\n",
    "    true_spans = get_spans([true for true, pred in dev_labels_pred[lang]])\n",
    "    pred_spans = get_spans([pred for true, pred in dev_labels_pred[lang]])\n",
    "    \n",
    "    span_f1 = SpanF1()\n",
    "    span_f1(pred_spans, true_spans)\n",
    "    cur_metric = span_f1.get_metric()\n",
    "    metrics[lang] = cur_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a872e66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "df = pd.DataFrame(index=list(metrics[\"RU\"].keys()))\n",
    "\n",
    "for lang, metric in metrics.items():\n",
    "    df[lang] = list(metric.values())\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c8839587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BN</th>\n",
       "      <th>DE</th>\n",
       "      <th>ES</th>\n",
       "      <th>TR</th>\n",
       "      <th>FA</th>\n",
       "      <th>RU</th>\n",
       "      <th>ZH</th>\n",
       "      <th>NL</th>\n",
       "      <th>KO</th>\n",
       "      <th>EN</th>\n",
       "      <th>HI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P@PER</th>\n",
       "      <td>0.797</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.884</td>\n",
       "      <td>0.788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R@PER</th>\n",
       "      <td>0.653</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.709</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.742</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1@PER</th>\n",
       "      <td>0.718</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.763</td>\n",
       "      <td>0.689</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P@CW</th>\n",
       "      <td>0.667</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.743</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.752</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R@CW</th>\n",
       "      <td>0.367</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.693</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1@CW</th>\n",
       "      <td>0.473</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P@GRP</th>\n",
       "      <td>0.802</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R@GRP</th>\n",
       "      <td>0.619</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1@GRP</th>\n",
       "      <td>0.699</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.734</td>\n",
       "      <td>0.694</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.798</td>\n",
       "      <td>0.697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P@LOC</th>\n",
       "      <td>0.741</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.906</td>\n",
       "      <td>0.523</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R@LOC</th>\n",
       "      <td>0.624</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.523</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.801</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.791</td>\n",
       "      <td>0.669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1@LOC</th>\n",
       "      <td>0.677</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.801</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.843</td>\n",
       "      <td>0.728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P@CORP</th>\n",
       "      <td>0.815</td>\n",
       "      <td>0.720</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.858</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.899</td>\n",
       "      <td>0.702</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.867</td>\n",
       "      <td>0.779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R@CORP</th>\n",
       "      <td>0.591</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1@CORP</th>\n",
       "      <td>0.685</td>\n",
       "      <td>0.568</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.743</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P@PROD</th>\n",
       "      <td>0.738</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.877</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R@PROD</th>\n",
       "      <td>0.474</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.641</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.723</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.646</td>\n",
       "      <td>0.408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1@PROD</th>\n",
       "      <td>0.577</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.723</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.659</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro@P</th>\n",
       "      <td>0.765</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.836</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.854</td>\n",
       "      <td>0.739</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro@R</th>\n",
       "      <td>0.549</td>\n",
       "      <td>0.506</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.768</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro@F1</th>\n",
       "      <td>0.639</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.723</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MD@R</th>\n",
       "      <td>0.581</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.711</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MD@P</th>\n",
       "      <td>0.810</td>\n",
       "      <td>0.814</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MD@F1</th>\n",
       "      <td>0.677</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.523</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALLTRUE</th>\n",
       "      <td>800.000</td>\n",
       "      <td>1195.000</td>\n",
       "      <td>1166.000</td>\n",
       "      <td>1223.000</td>\n",
       "      <td>1142.000</td>\n",
       "      <td>1033.000</td>\n",
       "      <td>1251.000</td>\n",
       "      <td>1136.000</td>\n",
       "      <td>1170.000</td>\n",
       "      <td>1218.000</td>\n",
       "      <td>818.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALLRECALLED</th>\n",
       "      <td>465.000</td>\n",
       "      <td>637.000</td>\n",
       "      <td>829.000</td>\n",
       "      <td>826.000</td>\n",
       "      <td>475.000</td>\n",
       "      <td>679.000</td>\n",
       "      <td>393.000</td>\n",
       "      <td>890.000</td>\n",
       "      <td>703.000</td>\n",
       "      <td>937.000</td>\n",
       "      <td>468.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALLPRED</th>\n",
       "      <td>574.000</td>\n",
       "      <td>783.000</td>\n",
       "      <td>947.000</td>\n",
       "      <td>972.000</td>\n",
       "      <td>675.000</td>\n",
       "      <td>792.000</td>\n",
       "      <td>513.000</td>\n",
       "      <td>1021.000</td>\n",
       "      <td>927.000</td>\n",
       "      <td>1062.000</td>\n",
       "      <td>573.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 BN       DE       ES       TR       FA       RU       ZH  \\\n",
       "P@PER         0.797    0.776    0.885    0.827    0.792    0.860    0.500   \n",
       "R@PER         0.653    0.593    0.687    0.709    0.610    0.672    0.048   \n",
       "F1@PER        0.718    0.672    0.773    0.763    0.689    0.754    0.087   \n",
       "P@CW          0.667    0.672    0.748    0.855    0.566    0.743    0.000   \n",
       "R@CW          0.367    0.441    0.635    0.602    0.554    0.627    0.000   \n",
       "F1@CW         0.473    0.532    0.687    0.707    0.560    0.680    0.000   \n",
       "P@GRP         0.802    0.802    0.779    0.903    0.000    0.802    0.455   \n",
       "R@GRP         0.619    0.519    0.695    0.564    0.000    0.593    0.208   \n",
       "F1@GRP        0.699    0.630    0.734    0.694    0.000    0.682    0.286   \n",
       "P@LOC         0.741    0.820    0.922    0.853    0.685    0.829    0.813   \n",
       "R@LOC         0.624    0.477    0.699    0.754    0.523    0.601    0.573   \n",
       "F1@LOC        0.677    0.603    0.795    0.801    0.593    0.697    0.672   \n",
       "P@CORP        0.815    0.720    0.885    0.858    0.000    0.899    0.702   \n",
       "R@CORP        0.591    0.470    0.777    0.660    0.000    0.682    0.349   \n",
       "F1@CORP       0.685    0.568    0.828    0.746    0.000    0.775    0.466   \n",
       "P@PROD        0.738    0.859    0.831    0.719    0.758    0.877    0.705   \n",
       "R@PROD        0.474    0.508    0.641    0.590    0.583    0.667    0.358   \n",
       "F1@PROD       0.577    0.638    0.723    0.648    0.659    0.758    0.475   \n",
       "micro@P       0.765    0.773    0.846    0.836    0.687    0.833    0.749   \n",
       "micro@R       0.549    0.506    0.687    0.665    0.406    0.639    0.307   \n",
       "micro@F1      0.639    0.612    0.758    0.741    0.511    0.723    0.435   \n",
       "MD@R          0.581    0.533    0.711    0.675    0.416    0.657    0.314   \n",
       "MD@P          0.810    0.814    0.875    0.850    0.704    0.857    0.766   \n",
       "MD@F1         0.677    0.644    0.785    0.753    0.523    0.744    0.446   \n",
       "ALLTRUE     800.000 1195.000 1166.000 1223.000 1142.000 1033.000 1251.000   \n",
       "ALLRECALLED 465.000  637.000  829.000  826.000  475.000  679.000  393.000   \n",
       "ALLPRED     574.000  783.000  947.000  972.000  675.000  792.000  513.000   \n",
       "\n",
       "                  NL       KO       EN      HI  \n",
       "P@PER          0.923    0.805    0.884   0.788  \n",
       "R@PER          0.742    0.606    0.782   0.600  \n",
       "F1@PER         0.822    0.692    0.830   0.681  \n",
       "P@CW           0.752    0.762    0.760   0.632  \n",
       "R@CW           0.693    0.654    0.648   0.429  \n",
       "F1@CW          0.721    0.704    0.699   0.511  \n",
       "P@GRP          0.744    0.805    0.864   0.791  \n",
       "R@GRP          0.827    0.549    0.741   0.623  \n",
       "F1@GRP         0.784    0.653    0.798   0.697  \n",
       "P@LOC          0.906    0.523    0.901   0.798  \n",
       "R@LOC          0.801    0.443    0.791   0.669  \n",
       "F1@LOC         0.850    0.480    0.843   0.728  \n",
       "P@CORP         0.929    0.820    0.867   0.779  \n",
       "R@CORP         0.802    0.680    0.781   0.458  \n",
       "F1@CORP        0.861    0.743    0.822   0.577  \n",
       "P@PROD         0.868    0.797    0.766   0.711  \n",
       "R@PROD         0.723    0.624    0.646   0.408  \n",
       "F1@PROD        0.789    0.700    0.701   0.519  \n",
       "micro@P        0.854    0.739    0.850   0.756  \n",
       "micro@R        0.768    0.585    0.741   0.529  \n",
       "micro@F1       0.809    0.653    0.792   0.623  \n",
       "MD@R           0.783    0.601    0.769   0.572  \n",
       "MD@P           0.872    0.758    0.882   0.817  \n",
       "MD@F1          0.825    0.670    0.822   0.673  \n",
       "ALLTRUE     1136.000 1170.000 1218.000 818.000  \n",
       "ALLRECALLED  890.000  703.000  937.000 468.000  \n",
       "ALLPRED     1021.000  927.000 1062.000 573.000  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5aa2553",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
