{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import os, operator\n",
    "from progressbar import progressbar as pb\n",
    "from nltk import word_tokenize\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../SemEval2022-Task11_Train-Dev/RU-Russian/\"\n",
    "model_path = \"../../pretrained/sbert_large_nlu_ru/\"\n",
    "SAVE_PATH = \"models/template_free/sbert_large_nlu_ru/\"\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_path, \"ru_train.conll\")) as f:\n",
    "    train_file = f.read().splitlines()\n",
    "    \n",
    "with open(os.path.join(data_path, \"ru_dev.conll\")) as f:\n",
    "    dev_file = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_conll(file) -> Tuple[List, List]:\n",
    "    texts, labels = [], []\n",
    "    \n",
    "    for row in file:\n",
    "        if row.startswith(\"#\"):\n",
    "            new_texts, new_labels = [], []\n",
    "            continue\n",
    "\n",
    "        if row == \"\":\n",
    "            texts.append(new_texts)\n",
    "            labels.append(new_labels)\n",
    "\n",
    "        else:\n",
    "            parts = row.split()\n",
    "            new_texts.append(parts[0])\n",
    "            new_labels.append(parts[-1])\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "train_texts, train_labels = parse_conll(train_file)\n",
    "dev_texts, dev_labels = parse_conll(dev_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15300, 800)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_texts), len(dev_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(train_texts) == len(train_labels)\n",
    "assert len(dev_texts) == len(dev_labels)\n",
    "\n",
    "for t, l in zip(train_texts, train_labels):\n",
    "    assert len(t) == len(l)\n",
    "    \n",
    "for t, l in zip(dev_texts, dev_labels):\n",
    "    assert len(t) == len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['через', 'год', 'дирекция', 'влезает', 'в', 'долги', 'и', 'берёт', 'под', 'определённый', 'процент', 'акций', 'кредит', 'в', 'казкоммерцбанк', '.'] ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CORP', 'O']\n"
     ]
    }
   ],
   "source": [
    "ind = 33\n",
    "print(train_texts[ind], train_labels[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find label words distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_entity(text, labels):\n",
    "    res_text, res_labels = [], []\n",
    "    new_text = []\n",
    "    \n",
    "    for t, l in zip(text, labels):\n",
    "        if l.startswith(\"B-\"):\n",
    "            new_text = [t]\n",
    "            new_label = l.split(\"-\")[-1]\n",
    "            \n",
    "        elif l.startswith(\"I-\"):\n",
    "            new_text.append(t)\n",
    "            \n",
    "        elif l == 'O' and len(new_text):\n",
    "            new_text = \" \".join(new_text)\n",
    "            res_text.append(new_text)\n",
    "            res_labels.append(new_label)\n",
    "            new_text, new_label = [], []\n",
    "            \n",
    "    return res_text, res_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dct = defaultdict(list)\n",
    "\n",
    "for text, label in zip(train_texts, train_labels):\n",
    "    parsed_text, parsed_labels = parse_entity(text, label)\n",
    "    for t, l in zip(parsed_text, parsed_labels):\n",
    "        freq_dct[l].append(t)\n",
    "        \n",
    "for k, v in freq_dct.items():\n",
    "    freq_dct[k] = sorted(dict(Counter(v)).items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['GRP', 'PER', 'CW', 'PROD', 'CORP', 'LOC'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dct.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRP\n",
      "[('бюро переписи населения сша', 25), ('колхоз', 25)]\n",
      "PER\n",
      "[('а. п. чехова', 6), ('женщин', 5)]\n",
      "CW\n",
      "[('государственный геральдический регистр российской федерации', 16), ('сингл', 15)]\n",
      "PROD\n",
      "[('dvd', 39), ('пулемёт', 25)]\n",
      "CORP\n",
      "[('rotten tomatoes', 70), ('mtv', 29)]\n",
      "LOC\n",
      "[('париж', 43), ('германии', 31)]\n"
     ]
    }
   ],
   "source": [
    "for k, v in freq_dct.items():\n",
    "    print(k)\n",
    "    print(v[:2]) # per - человек\n",
    "    # попробовать вместо слов использовать сущности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2word_label = { # label words dict\n",
    "    \"GRP\": \"колхоз\",\n",
    "    \"PER\": \"человек\",\n",
    "    \"CW\": \"сингл\",\n",
    "    \"PROD\": \"dvd\",\n",
    "    \"CORP\": \"mtv\",\n",
    "    \"LOC\": \"париж\"\n",
    "}\n",
    "\n",
    "word_label2label = {v:k for k, v in label2word_label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_target(tokens, labels, label2word_label=label2word_label):\n",
    "    \"\"\"\n",
    "        Replace entities with label words\n",
    "    \"\"\"\n",
    "    new_tokens = []\n",
    "    for token, label in zip(tokens, labels):\n",
    "        if label.startswith(\"B-\"):\n",
    "            prefix, tag = label.split(\"-\")\n",
    "            new_token = label2word_label[tag]\n",
    "            new_tokens.append(new_token)\n",
    "        elif label.startswith(\"I-\"):\n",
    "            continue\n",
    "        else:\n",
    "            new_tokens.append(token)\n",
    "    \n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, dev_data = [], []\n",
    "\n",
    "for tokens, label_list in zip(train_texts, train_labels):\n",
    "    target = prepare_target(tokens, label_list)\n",
    "    x = \" \".join(tokens)\n",
    "    y = \" \".join(target)\n",
    "    train_data.append((x,y))\n",
    "    \n",
    "for tokens, label_list in zip(dev_texts, dev_labels):\n",
    "    target = prepare_target(tokens, label_list)\n",
    "    x = \" \".join(tokens)\n",
    "    y = \" \".join(target)\n",
    "    dev_data.append((x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('важным традиционным промыслом является производство пальмового масла .',\n",
       " 'важным традиционным промыслом является производство dvd .')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_id_to_entity_map = {}\n",
    "\n",
    "for i, (text, labels) in enumerate(zip(dev_texts, dev_labels)):\n",
    "    dev_id_to_entity_map[i] = defaultdict(list)\n",
    "    for token, label in zip(text, labels):\n",
    "        if label == \"O\":\n",
    "            continue\n",
    "        prefix, tag = label.split(\"-\")\n",
    "        if prefix == \"B\":\n",
    "            dev_id_to_entity_map[i][tag].append([])\n",
    "            dev_id_to_entity_map[i][tag][-1].append(label)\n",
    "        elif prefix == \"I\":\n",
    "            dev_id_to_entity_map[i][tag][-1].append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['специальный', 'агент', 'секретной', 'службы', 'сша', 'джеззи', 'фланниган', ',', 'ответственная', 'за', 'нарушение', 'безопасности', ',', 'объединяется', 'с', 'кроссом', ',', 'чтобы', 'найти', 'пропавшую', 'девушку', '.'] ['O', 'O', 'B-GRP', 'I-GRP', 'I-GRP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "ind = 2\n",
    "print(dev_texts[ind], dev_labels[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list, {'GRP': [['B-GRP', 'I-GRP', 'I-GRP']]})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_id_to_entity_map[ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 64\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(model_path, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15300, 64), (15300, 64), (800, 64), (800, 64))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_data(data):\n",
    "    X_data, y_data = [], []\n",
    "\n",
    "    for item in data:\n",
    "        x, y = item\n",
    "        x_enc = tokenizer.encode(x, max_length=max_len, padding=\"max_length\", truncation=True)\n",
    "        y_enc = tokenizer.encode(y, max_length=max_len, padding=\"max_length\", truncation=True)\n",
    "        X_data.append(x_enc)\n",
    "        y_data.append(y_enc)\n",
    "        \n",
    "    return np.array(X_data), np.array(y_data)\n",
    "    \n",
    "X_train, y_train = encode_data(train_data)\n",
    "X_dev, y_dev = encode_data(dev_data)\n",
    "X_train.shape, y_train.shape, X_dev.shape, y_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15300, 2, 64), (800, 2, 64))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = np.stack((X_train, y_train), axis=1)\n",
    "dev = np.stack((X_dev, y_dev), axis=1)\n",
    "train.shape, dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "dev_batches = DataLoader(dev, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at ../../pretrained/sbert_large_nlu_ru/ and are newly initialized: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "427030858"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForMaskedLM.from_pretrained(model_path, local_files_only=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "model.to(device)\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train(model, train_loader, optimizer, epoch_num):\n",
    "    train_loss, train_acc, train_f1 = [], [], []\n",
    "    model.train()\n",
    "\n",
    "    for batch_num, batch in enumerate(train_loader):\n",
    "        X_batch, y_batch = batch[:, 0, :], batch[:, 1, :]\n",
    "        X_batch = X_batch.type(torch.LongTensor).to(device)\n",
    "        y_batch = y_batch.type(torch.LongTensor).to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(input_ids=X_batch, labels=y_batch.contiguous(), return_dict=True)\n",
    "        loss = out.loss\n",
    "        y_pred = out.logits\n",
    "        y_pred = torch.argmax(y_pred, dim=2)\n",
    "\n",
    "        y_pred_flatten = torch.flatten(y_pred).cpu().numpy()\n",
    "        y_batch_flatten = torch.flatten(y_batch).cpu().numpy()\n",
    "        f1 = f1_score(y_batch_flatten, y_pred_flatten, average=\"micro\")\n",
    "        accuracy = accuracy_score(y_batch_flatten, y_pred_flatten)\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "        train_acc.append(accuracy)\n",
    "        train_f1.append(f1)\n",
    "        \n",
    "        if batch_num % 50 == 0:\n",
    "            print(f\"TRAIN: Epoch: {epoch_num}, Batch: {batch_num + 1} / {len(train_loader)}, \"\n",
    "                          f\"Loss: {loss.item():.3f}, Accuracy: {accuracy:.3f}, F1: {f1:.3f}\")\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return np.mean(train_loss), np.mean(train_acc), np.mean(train_f1)\n",
    "\n",
    "def _val(model, val_loader, epoch_num):\n",
    "    val_loss, val_acc, val_f1 = [], [], []\n",
    "    model.eval()\n",
    "\n",
    "    for batch_num, batch in enumerate(val_loader):\n",
    "        X_batch, y_batch = batch[:, 0, :], batch[:, 1, :]\n",
    "        X_batch = X_batch.type(torch.LongTensor).to(device)\n",
    "        y_batch = y_batch.type(torch.LongTensor).to(device)\n",
    "\n",
    "        out = model(input_ids=X_batch, labels=y_batch.contiguous())\n",
    "        loss = out.loss\n",
    "        y_pred = out.logits\n",
    "        y_pred = torch.argmax(y_pred, dim=2)\n",
    "\n",
    "        y_pred_flatten = torch.flatten(y_pred).cpu().numpy()\n",
    "        y_batch_flatten = torch.flatten(y_batch).cpu().numpy()\n",
    "        f1 = f1_score(y_batch_flatten, y_pred_flatten, average=\"micro\")\n",
    "        accuracy = accuracy_score(y_batch_flatten, y_pred_flatten)\n",
    "\n",
    "        val_loss.append(loss.item())\n",
    "        val_acc.append(accuracy)\n",
    "        val_f1.append(f1)\n",
    "        \n",
    "        if batch_num % 50 == 0:\n",
    "            print(f\"VAL: Epoch: {epoch_num}, Batch: {batch_num + 1} / {len(val_loader)}, \"\n",
    "                          f\"Loss: {loss.item():.3f}, Accuracy: {accuracy:.3f}, F1: {f1:.3f}\")\n",
    "\n",
    "    return np.mean(val_loss), np.mean(val_acc), np.mean(val_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Epoch: 1, Batch: 1 / 479, Loss: 12.386, Accuracy: 0.000, F1: 0.000\n",
      "TRAIN: Epoch: 1, Batch: 51 / 479, Loss: 2.446, Accuracy: 0.679, F1: 0.679\n",
      "TRAIN: Epoch: 1, Batch: 101 / 479, Loss: 1.882, Accuracy: 0.741, F1: 0.741\n",
      "TRAIN: Epoch: 1, Batch: 151 / 479, Loss: 1.433, Accuracy: 0.776, F1: 0.776\n",
      "TRAIN: Epoch: 1, Batch: 201 / 479, Loss: 1.365, Accuracy: 0.783, F1: 0.783\n",
      "TRAIN: Epoch: 1, Batch: 251 / 479, Loss: 1.095, Accuracy: 0.833, F1: 0.833\n",
      "TRAIN: Epoch: 1, Batch: 301 / 479, Loss: 1.092, Accuracy: 0.816, F1: 0.816\n",
      "TRAIN: Epoch: 1, Batch: 351 / 479, Loss: 1.180, Accuracy: 0.815, F1: 0.815\n",
      "TRAIN: Epoch: 1, Batch: 401 / 479, Loss: 0.663, Accuracy: 0.882, F1: 0.882\n",
      "TRAIN: Epoch: 1, Batch: 451 / 479, Loss: 1.051, Accuracy: 0.834, F1: 0.834\n",
      "VAL: Epoch: 1, Batch: 1 / 25, Loss: 0.934, Accuracy: 0.844, F1: 0.844\n",
      "After epoch #1:\n",
      "Train loss: 1.468, Train Accuracy: 0.785, Train F1: 0.785\n",
      "Dev loss: 0.959, Dev Accuracy: 0.836, Dev F1: 0.836\n",
      "\n",
      "TRAIN: Epoch: 2, Batch: 1 / 479, Loss: 0.911, Accuracy: 0.849, F1: 0.849\n",
      "TRAIN: Epoch: 2, Batch: 51 / 479, Loss: 0.630, Accuracy: 0.895, F1: 0.895\n",
      "TRAIN: Epoch: 2, Batch: 101 / 479, Loss: 1.029, Accuracy: 0.826, F1: 0.826\n",
      "TRAIN: Epoch: 2, Batch: 151 / 479, Loss: 0.636, Accuracy: 0.879, F1: 0.879\n",
      "TRAIN: Epoch: 2, Batch: 201 / 479, Loss: 0.689, Accuracy: 0.878, F1: 0.878\n",
      "TRAIN: Epoch: 2, Batch: 251 / 479, Loss: 0.626, Accuracy: 0.882, F1: 0.882\n",
      "TRAIN: Epoch: 2, Batch: 301 / 479, Loss: 0.784, Accuracy: 0.863, F1: 0.863\n",
      "TRAIN: Epoch: 2, Batch: 351 / 479, Loss: 0.940, Accuracy: 0.837, F1: 0.837\n",
      "TRAIN: Epoch: 2, Batch: 401 / 479, Loss: 0.882, Accuracy: 0.835, F1: 0.835\n",
      "TRAIN: Epoch: 2, Batch: 451 / 479, Loss: 0.811, Accuracy: 0.845, F1: 0.845\n",
      "VAL: Epoch: 2, Batch: 1 / 25, Loss: 0.653, Accuracy: 0.871, F1: 0.871\n",
      "After epoch #2:\n",
      "Train loss: 0.750, Train Accuracy: 0.865, Train F1: 0.865\n",
      "Dev loss: 0.771, Dev Accuracy: 0.862, Dev F1: 0.862\n",
      "\n",
      "TRAIN: Epoch: 3, Batch: 1 / 479, Loss: 0.535, Accuracy: 0.882, F1: 0.882\n",
      "TRAIN: Epoch: 3, Batch: 51 / 479, Loss: 0.457, Accuracy: 0.905, F1: 0.905\n",
      "TRAIN: Epoch: 3, Batch: 101 / 479, Loss: 0.476, Accuracy: 0.897, F1: 0.897\n",
      "TRAIN: Epoch: 3, Batch: 151 / 479, Loss: 0.466, Accuracy: 0.908, F1: 0.908\n",
      "TRAIN: Epoch: 3, Batch: 201 / 479, Loss: 0.618, Accuracy: 0.872, F1: 0.872\n",
      "TRAIN: Epoch: 3, Batch: 251 / 479, Loss: 0.366, Accuracy: 0.924, F1: 0.924\n",
      "TRAIN: Epoch: 3, Batch: 301 / 479, Loss: 0.527, Accuracy: 0.881, F1: 0.881\n",
      "TRAIN: Epoch: 3, Batch: 351 / 479, Loss: 0.525, Accuracy: 0.883, F1: 0.883\n",
      "TRAIN: Epoch: 3, Batch: 401 / 479, Loss: 0.346, Accuracy: 0.924, F1: 0.924\n",
      "TRAIN: Epoch: 3, Batch: 451 / 479, Loss: 0.389, Accuracy: 0.910, F1: 0.910\n",
      "VAL: Epoch: 3, Batch: 1 / 25, Loss: 0.735, Accuracy: 0.858, F1: 0.858\n",
      "After epoch #3:\n",
      "Train loss: 0.485, Train Accuracy: 0.895, Train F1: 0.895\n",
      "Dev loss: 0.714, Dev Accuracy: 0.866, Dev F1: 0.866\n",
      "\n",
      "TRAIN: Epoch: 4, Batch: 1 / 479, Loss: 0.410, Accuracy: 0.901, F1: 0.901\n",
      "TRAIN: Epoch: 4, Batch: 51 / 479, Loss: 0.410, Accuracy: 0.911, F1: 0.911\n",
      "TRAIN: Epoch: 4, Batch: 101 / 479, Loss: 0.356, Accuracy: 0.911, F1: 0.911\n",
      "TRAIN: Epoch: 4, Batch: 151 / 479, Loss: 0.415, Accuracy: 0.904, F1: 0.904\n",
      "TRAIN: Epoch: 4, Batch: 201 / 479, Loss: 0.406, Accuracy: 0.899, F1: 0.899\n",
      "TRAIN: Epoch: 4, Batch: 251 / 479, Loss: 0.340, Accuracy: 0.918, F1: 0.918\n",
      "TRAIN: Epoch: 4, Batch: 301 / 479, Loss: 0.363, Accuracy: 0.924, F1: 0.924\n",
      "TRAIN: Epoch: 4, Batch: 351 / 479, Loss: 0.330, Accuracy: 0.921, F1: 0.921\n",
      "TRAIN: Epoch: 4, Batch: 401 / 479, Loss: 0.446, Accuracy: 0.891, F1: 0.891\n",
      "TRAIN: Epoch: 4, Batch: 451 / 479, Loss: 0.905, Accuracy: 0.827, F1: 0.827\n",
      "VAL: Epoch: 4, Batch: 1 / 25, Loss: 0.873, Accuracy: 0.838, F1: 0.838\n",
      "After epoch #4:\n",
      "Train loss: 0.373, Train Accuracy: 0.912, Train F1: 0.912\n",
      "Dev loss: 0.829, Dev Accuracy: 0.849, Dev F1: 0.849\n",
      "\n",
      "TRAIN: Epoch: 5, Batch: 1 / 479, Loss: 0.638, Accuracy: 0.849, F1: 0.849\n",
      "TRAIN: Epoch: 5, Batch: 51 / 479, Loss: 0.340, Accuracy: 0.917, F1: 0.917\n",
      "TRAIN: Epoch: 5, Batch: 101 / 479, Loss: 0.218, Accuracy: 0.936, F1: 0.936\n",
      "TRAIN: Epoch: 5, Batch: 151 / 479, Loss: 0.321, Accuracy: 0.921, F1: 0.921\n",
      "TRAIN: Epoch: 5, Batch: 201 / 479, Loss: 0.373, Accuracy: 0.908, F1: 0.908\n",
      "TRAIN: Epoch: 5, Batch: 251 / 479, Loss: 0.193, Accuracy: 0.950, F1: 0.950\n",
      "TRAIN: Epoch: 5, Batch: 301 / 479, Loss: 0.281, Accuracy: 0.921, F1: 0.921\n",
      "TRAIN: Epoch: 5, Batch: 351 / 479, Loss: 0.346, Accuracy: 0.918, F1: 0.918\n",
      "TRAIN: Epoch: 5, Batch: 401 / 479, Loss: 0.302, Accuracy: 0.925, F1: 0.925\n",
      "TRAIN: Epoch: 5, Batch: 451 / 479, Loss: 0.262, Accuracy: 0.927, F1: 0.927\n",
      "VAL: Epoch: 5, Batch: 1 / 25, Loss: 0.574, Accuracy: 0.892, F1: 0.892\n",
      "After epoch #5:\n",
      "Train loss: 0.312, Train Accuracy: 0.921, Train F1: 0.921\n",
      "Dev loss: 0.681, Dev Accuracy: 0.880, Dev F1: 0.880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "last_epoch = 0\n",
    "dev_losses = []\n",
    "patience = 1\n",
    "\n",
    "for epoch in range(1, 25 + 1):\n",
    "    train_loss, train_acc, train_f1 = _train(model, train_batches, optimizer, epoch)\n",
    "    dev_loss, dev_acc, dev_f1 = _val(model, dev_batches, epoch)\n",
    "\n",
    "    if len(dev_losses) == 0 or dev_loss < dev_losses[-1]:\n",
    "        model.save_pretrained(SAVE_PATH)\n",
    "\n",
    "    elif last_epoch == 0:\n",
    "        last_epoch = epoch + patience\n",
    "\n",
    "    print(f\"After epoch #{epoch}:\")\n",
    "    print(f\"Train loss: {train_loss:.3f}, Train Accuracy: {train_acc:.3f}, Train F1: {train_f1:.3f}\")\n",
    "    print(f\"Dev loss: {dev_loss:.3f}, Dev Accuracy: {dev_acc:.3f}, Dev F1: {dev_f1:.3f}\\n\")\n",
    "\n",
    "    dev_losses.append(dev_loss)\n",
    "    if epoch == last_epoch:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = dev_data[ind][0]\n",
    "\n",
    "tokenized = torch.LongTensor([\n",
    "        tokenizer.encode(phrase, max_length=max_len, padding=\"max_length\", truncation=True)\n",
    "    ]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'специальный агент секретной службы сша джеззи фланниган , ответственная за нарушение безопасности , объединяется с кроссом , чтобы найти пропавшую девушку .'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'специальныи сингл секретно за за за за, за за за засяся,,,,м,,вшую..'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(tokenized).logits\n",
    "out = torch.argmax(out, dim=2)\n",
    "tokenizer.decode(out[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# decode predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = BertForMaskedLM.from_pretrained(SAVE_PATH, local_files_only=True)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(text, model=model):\n",
    "    tokenized = torch.LongTensor([\n",
    "        tokenizer.encode(text, max_length=max_len, padding=\"max_length\", truncation=True)\n",
    "    ]).to(device)\n",
    "    out = model(tokenized).logits\n",
    "    out = torch.argmax(out, dim=2)\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (800 of 800) |######################| Elapsed Time: 0:00:45 Time:  0:00:45\n"
     ]
    }
   ],
   "source": [
    "dev_x = [item[0] for item in dev_data]\n",
    "dev_y_true = [item[1] for item in dev_data]\n",
    "dev_y_pred = [get_pred(item) for item in pb(dev_x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('выпущен 15 февраля 2019 года лейблом mtv .',\n",
       " 'выпущен 15 февраля 2019 года леtлом mtv.')"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = 7\n",
    "dev_y_true[ind], dev_y_pred[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_pred(y_true, y_pred):\n",
    "    pred_labels = []\n",
    "\n",
    "    y_pred = y_pred.replace(\"ё\", \"е\").replace(\"й\", \"и\")\n",
    "    y_pred = word_tokenize(y_pred)\n",
    "    \n",
    "    y_true = y_true.replace(\"ё\", \"е\").replace(\"й\", \"и\")\n",
    "    y_true = word_tokenize(y_true)\n",
    "    \n",
    "    true_labels = [word_label2label.get(token, \"O\") for token in y_true]\n",
    "    pred_labels = [word_label2label.get(token, \"O\") for token in y_pred]\n",
    "    \n",
    "    true_len = len(true_labels)\n",
    "    pred_len = len(pred_labels)\n",
    "    \n",
    "    if true_len > pred_len:\n",
    "        pred_labels.extend([\"O\"] * (true_len - pred_len))\n",
    "    elif pred_len > true_len:\n",
    "        true_labels.extend([\"O\"] * (pred_len - true_len))\n",
    "            \n",
    "    assert len(true_labels) == len(pred_labels)\n",
    "    return true_labels, pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "# dev_labels\n",
    "\n",
    "dev_id_to_entity_map_cp = deepcopy(dev_id_to_entity_map)\n",
    "\n",
    "final_dev_pred = []\n",
    "\n",
    "for i, (t, p, labels) in enumerate(zip(dev_y_true, dev_y_pred, dev_labels)):\n",
    "    t, p  = decode_pred(t, p)\n",
    "    fin = []\n",
    "\n",
    "    for label in p:\n",
    "        if label == \"O\":\n",
    "            fin.append(\"O\")\n",
    "        else:\n",
    "            if label in dev_id_to_entity_map_cp[i]:\n",
    "                fin.extend(dev_id_to_entity_map_cp[i][label][0])\n",
    "                if len(dev_id_to_entity_map_cp[i][label]) > 1:\n",
    "                    dev_id_to_entity_map_cp[i][label] = dev_id_to_entity_map_cp[i][label][1:]\n",
    "            else:\n",
    "                fin.append(\"B-\" + label)\n",
    "    \n",
    "    true_len = len(labels)\n",
    "    pred_len = len(fin)\n",
    "    \n",
    "    if true_len > pred_len:\n",
    "        fin.extend([\"O\"] * (true_len - pred_len))\n",
    "    elif pred_len > true_len:\n",
    "        fin = fin[:true_len]\n",
    "    assert len(fin) == len(labels)\n",
    "    final_dev_pred.append(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_true = [item for sublist in dev_labels for item in sublist]\n",
    "flat_pred = [item for sublist in final_dev_pred for item in sublist]\n",
    "assert len(flat_true) == len(flat_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-CORP     0.8205    0.4051    0.5424       158\n",
      "        B-CW     0.4129    0.4940    0.4499       168\n",
      "       B-GRP     0.5221    0.4702    0.4948       151\n",
      "       B-LOC     0.4193    0.6109    0.4972       221\n",
      "       B-PER     0.4211    0.5417    0.4738       192\n",
      "      B-PROD     0.6667    0.4503    0.5375       151\n",
      "      I-CORP     0.8929    0.4132    0.5650       121\n",
      "        I-CW     0.5774    0.5389    0.5575       180\n",
      "       I-GRP     0.6762    0.5000    0.5749       142\n",
      "       I-LOC     0.3793    0.3438    0.3607        32\n",
      "       I-PER     0.6337    0.6598    0.6465       194\n",
      "      I-PROD     0.6667    0.5000    0.5714        56\n",
      "           O     0.9379    0.9446    0.9412     11002\n",
      "\n",
      "    accuracy                         0.8852     12768\n",
      "   macro avg     0.6174    0.5286    0.5548     12768\n",
      "weighted avg     0.8890    0.8852    0.8844     12768\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(flat_true, flat_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
