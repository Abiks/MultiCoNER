{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33e5fd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import os, operator\n",
    "from progressbar import progressbar as pb\n",
    "from nltk import word_tokenize\n",
    "from typing import List, Tuple, Dict\n",
    "import json\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForMaskedLM\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "from copy import deepcopy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6085128",
   "metadata": {},
   "source": [
    "# Read data from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b9d47ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../SemEval2022-Task11_Train-Dev/\"\n",
    "model_path = \"../../pretrained/xlm-roberta-large/\"\n",
    "SAVE_PATH = \"models/template_free/multilang_xlm-r/\"\n",
    "device = 'cuda'\n",
    "\n",
    "max_len = 96\n",
    "batch_size = 24\n",
    "num_epochs = 25\n",
    "patience = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e90cd40a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 11)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files, dev_files = [], []\n",
    "\n",
    "for folder in os.listdir(data_path):\n",
    "    files = os.listdir(os.path.join(data_path, folder))\n",
    "    train_file = files[0] if \"train\" in files[0] else files[1]\n",
    "    dev_file = files[0] if \"dev\" in files[0] else files[1]\n",
    "    \n",
    "    train_files.append(os.path.join(data_path, folder, train_file))\n",
    "    dev_files.append(os.path.join(data_path, folder, dev_file))\n",
    "    \n",
    "len(train_files), len(dev_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1a24f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_conll(files) -> Tuple[Dict, Dict]:\n",
    "    \n",
    "    fin_texts, fin_labels = {}, {}\n",
    "    \n",
    "    for filename in pb(files):\n",
    "    \n",
    "        with open(filename) as f:\n",
    "            data = f.read().splitlines()\n",
    "\n",
    "        lang = os.path.basename(filename).split(\"_\")[0].upper()\n",
    "        texts, labels = [], []\n",
    "\n",
    "        for row in data:\n",
    "            if row.startswith(\"# id \"):\n",
    "                new_texts, new_labels = [], []\n",
    "                continue\n",
    "\n",
    "            if row == \"\":\n",
    "                texts.append(new_texts)\n",
    "                labels.append(new_labels)\n",
    "\n",
    "            else:\n",
    "                parts = row.split()\n",
    "                new_texts.append(parts[0])\n",
    "                new_labels.append(parts[-1])\n",
    "                \n",
    "        fin_texts[lang] = texts\n",
    "        fin_labels[lang] = labels\n",
    "\n",
    "    return fin_texts, fin_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23c5f68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (11 of 11) |########################| Elapsed Time: 0:00:03 Time:  0:00:03\n",
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    }
   ],
   "source": [
    "train_texts, train_labels = parse_conll(train_files)\n",
    "dev_texts, dev_labels = parse_conll(dev_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8547d1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = []\n",
    "\n",
    "for lang, texts in train_texts.items():\n",
    "    for t in texts:\n",
    "        lens.append(len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7ea84c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for texts, labels in zip(train_texts.values(), train_labels.values()):\n",
    "    assert len(texts) == 15300\n",
    "    assert len(labels) == 15300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43353322",
   "metadata": {},
   "outputs": [],
   "source": [
    "for texts, labels in zip(dev_texts.values(), dev_labels.values()):\n",
    "    assert len(texts) == 800\n",
    "    assert len(labels) == 800"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346fde04",
   "metadata": {},
   "source": [
    "# Find label words distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dc7c863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_entity(text, labels):\n",
    "    res = []\n",
    "    new_text = []\n",
    "    \n",
    "    for t, l in zip(text, labels):\n",
    "        if l.startswith(\"B-\"):\n",
    "            new_text = [t]\n",
    "            new_label = l.split(\"-\")[-1]\n",
    "            \n",
    "        elif l.startswith(\"I-\"):\n",
    "            new_text.append(t)\n",
    "            \n",
    "        elif l == 'O' and len(new_text):\n",
    "            new_text = \" \".join(new_text)\n",
    "            res.append((new_text, new_label))\n",
    "            new_text, new_label = [], []\n",
    "            \n",
    "    new_text = \" \".join(new_text)\n",
    "    if new_text:\n",
    "        res.append((new_text, new_label))\n",
    "            \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f9c1748",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dct_by_lang = dict()\n",
    "\n",
    "for lang in train_texts.keys():\n",
    "    texts = train_texts[lang]\n",
    "    labels = train_labels[lang]\n",
    "    \n",
    "    freq_dct = defaultdict(list)\n",
    "    \n",
    "    for text, label in zip(texts, labels):\n",
    "        parsed_entities = parse_entity(text, label)\n",
    "        for item in parsed_entities:\n",
    "            t, l = item\n",
    "            freq_dct[l].append(t)\n",
    "            \n",
    "    for k, v in freq_dct.items():\n",
    "        freq_dct[k] = sorted(dict(Counter(v)).items(), key=operator.itemgetter(1), reverse=True)\n",
    "        \n",
    "    freq_dct_by_lang[lang] = freq_dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26f92d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2word_label = defaultdict(dict)\n",
    "\n",
    "for lang, val in freq_dct_by_lang.items():\n",
    "    for label, label_words in val.items():\n",
    "        for word, count in label_words:\n",
    "            if \" \" not in word:\n",
    "                label2word_label[lang][label] = word\n",
    "                break\n",
    "            \n",
    "word_label2label = {}\n",
    "\n",
    "for lang, val in label2word_label.items():\n",
    "    word_label2label[lang] = {v:k for k, v in val.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a77a739e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PER': 'jesus',\n",
       " 'GRP': 'nato',\n",
       " 'CW': 'single',\n",
       " 'LOC': 'france',\n",
       " 'CORP': 'bbc',\n",
       " 'PROD': 'stucco'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2word_label[\"EN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c997eb17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'колхоз': 'GRP',\n",
       " 'женщин': 'PER',\n",
       " 'сингл': 'CW',\n",
       " 'dvd': 'PROD',\n",
       " 'mtv': 'CORP',\n",
       " 'париж': 'LOC'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_label2label[\"RU\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b55646a",
   "metadata": {},
   "source": [
    "# Prepare Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9878f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_target(tokens, labels, lang, label2word_label=label2word_label):\n",
    "    \"\"\"\n",
    "        Replace entities with label words\n",
    "    \"\"\"\n",
    "    new_tokens = []\n",
    "    for token, label in zip(tokens, labels):\n",
    "        if label.startswith(\"B-\"):\n",
    "            prefix, tag = label.split(\"-\")\n",
    "            new_token = label2word_label[lang][tag]\n",
    "            new_tokens.append(new_token)\n",
    "        elif label.startswith(\"I-\"):\n",
    "            continue\n",
    "        else:\n",
    "            new_tokens.append(token)\n",
    "    \n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df84cbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (11 of 11) |########################| Elapsed Time: 0:00:01 Time:  0:00:01\n"
     ]
    }
   ],
   "source": [
    "train_targets, dev_targets = defaultdict(list), defaultdict(list)\n",
    "\n",
    "for lang in pb(train_texts):\n",
    "    texts, labels = train_texts[lang], train_labels[lang]\n",
    "    \n",
    "    for text, label in zip(texts, labels):\n",
    "        target = prepare_target(text, label, lang)\n",
    "        train_targets[lang].append(target)\n",
    "        \n",
    "    texts, labels = dev_texts[lang], dev_labels[lang]\n",
    "    \n",
    "    for text, label in zip(texts, labels):\n",
    "        target = prepare_target(text, label, lang)\n",
    "        dev_targets[lang].append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0bfcd708",
   "metadata": {},
   "outputs": [],
   "source": [
    "for targets, labels in zip(train_targets.values(), train_labels.values()):\n",
    "    assert len(targets) == 15300\n",
    "    assert len(labels) == 15300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0b419fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for targets, labels in zip(dev_targets.values(), dev_labels.values()):\n",
    "    assert len(targets) == 800\n",
    "    assert len(labels) == 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6da523f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168300, 8800)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, dev_data = [], []\n",
    "\n",
    "for lang in train_texts:\n",
    "    for x, y in zip(train_texts[lang], train_targets[lang]):\n",
    "        x = \" \".join(x)\n",
    "        y = \" \".join(y)\n",
    "        train_data.append((x,y))\n",
    "    for x, y in zip(dev_texts[lang], dev_targets[lang]):\n",
    "        x = \" \".join(x)\n",
    "        y = \" \".join(y)\n",
    "        dev_data.append((x,y))\n",
    "        \n",
    "len(train_data), len(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ce83f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('퀸 메리 런던 대학교 주도하는 18개의 영국대학 연합에 의해 건설되었고 , 2009년 12월에 유럽 남방 천문대 인도되었다 .',\n",
       " '퀸 메리 초등학교 주도하는 18개의 영국대학 연합에 의해 건설되었고 , 2009년 12월에 초등학교 인도되었다 .')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f1b36e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_id_to_entity_map = defaultdict(dict)\n",
    "\n",
    "for lang in train_texts:\n",
    "    for i, (text, labels) in enumerate(zip(dev_texts[lang], dev_labels[lang])):\n",
    "        dev_id_to_entity_map[lang][i] = defaultdict(list)\n",
    "        for token, label in zip(text, labels):\n",
    "            if label == \"O\":\n",
    "                continue\n",
    "            prefix, tag = label.split(\"-\")\n",
    "            if prefix == \"B\":\n",
    "                dev_id_to_entity_map[lang][i][tag].append([])\n",
    "                dev_id_to_entity_map[lang][i][tag][-1].append(label)\n",
    "            elif prefix == \"I\":\n",
    "                dev_id_to_entity_map[lang][i][tag][-1].append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "907a81ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list, {'PROD': [['B-PROD', 'I-PROD']]})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_id_to_entity_map[\"RU\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b45d071",
   "metadata": {},
   "source": [
    "# Prepare for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c03c976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = XLMRobertaTokenizer.from_pretrained(model_path, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73435bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (168300 of 168300) |################| Elapsed Time: 0:01:10 Time:  0:01:10\n",
      "100% (8800 of 8800) |####################| Elapsed Time: 0:00:03 Time:  0:00:03\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((168300, 96), (168300, 96), (8800, 96), (8800, 96))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_data(data, max_len=max_len):\n",
    "    X_data, y_data = [], []\n",
    "\n",
    "    for item in pb(data):\n",
    "        x, y = item\n",
    "        x_enc = tokenizer.encode(x, max_length=max_len, padding=\"max_length\", truncation=True)\n",
    "        y_enc = tokenizer.encode(y, max_length=max_len, padding=\"max_length\", truncation=True)\n",
    "        X_data.append(x_enc)\n",
    "        y_data.append(y_enc)\n",
    "        \n",
    "    return np.array(X_data), np.array(y_data)\n",
    "    \n",
    "X_train, y_train = encode_data(train_data)\n",
    "X_dev, y_dev = encode_data(dev_data)\n",
    "X_train.shape, y_train.shape, X_dev.shape, y_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d92fbed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((168300, 2, 96), (8800, 2, 96))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = np.stack((X_train, y_train), axis=1)\n",
    "dev = np.stack((X_dev, y_dev), axis=1)\n",
    "train.shape, dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ad8cb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "dev_batches = DataLoader(dev, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df6ff3d",
   "metadata": {},
   "source": [
    "# Training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e74de2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "560142482"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XLMRobertaForMaskedLM.from_pretrained(model_path, local_files_only=True)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "model.to(device)\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9896c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train(model, train_loader, optimizer, epoch_num):\n",
    "    train_loss, train_acc, train_f1 = [], [], []\n",
    "    model.train()\n",
    "\n",
    "    for batch_num, batch in enumerate(train_loader):\n",
    "        X_batch, y_batch = batch[:, 0, :], batch[:, 1, :]\n",
    "        X_batch = X_batch.type(torch.LongTensor).to(device)\n",
    "        y_batch = y_batch.type(torch.LongTensor).to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(input_ids=X_batch, labels=y_batch.contiguous(), return_dict=True)\n",
    "        loss = out.loss\n",
    "        y_pred = out.logits\n",
    "        y_pred = torch.argmax(y_pred, dim=2)\n",
    "\n",
    "        y_pred_flatten = torch.flatten(y_pred).cpu().numpy()\n",
    "        y_batch_flatten = torch.flatten(y_batch).cpu().numpy()\n",
    "        f1 = f1_score(y_batch_flatten, y_pred_flatten, average=\"macro\")\n",
    "        accuracy = accuracy_score(y_batch_flatten, y_pred_flatten)\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "        train_acc.append(accuracy)\n",
    "        train_f1.append(f1)\n",
    "        \n",
    "        if batch_num % 100 == 0:\n",
    "            print(f\"TRAIN: Epoch: {epoch_num}, Batch: {batch_num + 1} / {len(train_loader)}, \"\n",
    "                          f\"Loss: {loss.item():.3f}, Accuracy: {accuracy:.3f}, F1: {f1:.3f}\")\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return np.mean(train_loss), np.mean(train_acc), np.mean(train_f1)\n",
    "\n",
    "def _val(model, val_loader, epoch_num):\n",
    "    val_loss, val_acc, val_f1 = [], [], []\n",
    "    model.eval()\n",
    "\n",
    "    for batch_num, batch in enumerate(val_loader):\n",
    "        X_batch, y_batch = batch[:, 0, :], batch[:, 1, :]\n",
    "        X_batch = X_batch.type(torch.LongTensor).to(device)\n",
    "        y_batch = y_batch.type(torch.LongTensor).to(device)\n",
    "\n",
    "        out = model(input_ids=X_batch, labels=y_batch.contiguous())\n",
    "        loss = out.loss\n",
    "        y_pred = out.logits\n",
    "        y_pred = torch.argmax(y_pred, dim=2)\n",
    "\n",
    "        y_pred_flatten = torch.flatten(y_pred).cpu().numpy()\n",
    "        y_batch_flatten = torch.flatten(y_batch).cpu().numpy()\n",
    "        f1 = f1_score(y_batch_flatten, y_pred_flatten, average=\"macro\")\n",
    "        accuracy = accuracy_score(y_batch_flatten, y_pred_flatten)\n",
    "\n",
    "        val_loss.append(loss.item())\n",
    "        val_acc.append(accuracy)\n",
    "        val_f1.append(f1)\n",
    "        \n",
    "        if batch_num % 100 == 0:\n",
    "            print(f\"VAL: Epoch: {epoch_num}, Batch: {batch_num + 1} / {len(val_loader)}, \"\n",
    "                          f\"Loss: {loss.item():.3f}, Accuracy: {accuracy:.3f}, F1: {f1:.3f}\")\n",
    "\n",
    "    return np.mean(val_loss), np.mean(val_acc), np.mean(val_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ba742fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Epoch: 1, Batch: 1 / 7013, Loss: 18.653, Accuracy: 0.138, F1: 0.206\n",
      "TRAIN: Epoch: 1, Batch: 101 / 7013, Loss: 1.393, Accuracy: 0.838, F1: 0.356\n",
      "TRAIN: Epoch: 1, Batch: 201 / 7013, Loss: 1.396, Accuracy: 0.818, F1: 0.318\n",
      "TRAIN: Epoch: 1, Batch: 301 / 7013, Loss: 1.105, Accuracy: 0.844, F1: 0.392\n",
      "TRAIN: Epoch: 1, Batch: 401 / 7013, Loss: 1.490, Accuracy: 0.785, F1: 0.259\n",
      "TRAIN: Epoch: 1, Batch: 501 / 7013, Loss: 0.992, Accuracy: 0.845, F1: 0.354\n",
      "TRAIN: Epoch: 1, Batch: 601 / 7013, Loss: 1.085, Accuracy: 0.822, F1: 0.318\n",
      "TRAIN: Epoch: 1, Batch: 701 / 7013, Loss: 1.036, Accuracy: 0.836, F1: 0.338\n",
      "TRAIN: Epoch: 1, Batch: 801 / 7013, Loss: 0.921, Accuracy: 0.829, F1: 0.389\n",
      "TRAIN: Epoch: 1, Batch: 901 / 7013, Loss: 1.079, Accuracy: 0.802, F1: 0.354\n",
      "TRAIN: Epoch: 1, Batch: 1001 / 7013, Loss: 0.983, Accuracy: 0.810, F1: 0.323\n",
      "TRAIN: Epoch: 1, Batch: 1101 / 7013, Loss: 0.747, Accuracy: 0.864, F1: 0.434\n",
      "TRAIN: Epoch: 1, Batch: 1201 / 7013, Loss: 0.558, Accuracy: 0.886, F1: 0.475\n",
      "TRAIN: Epoch: 1, Batch: 1301 / 7013, Loss: 0.612, Accuracy: 0.866, F1: 0.438\n",
      "TRAIN: Epoch: 1, Batch: 1401 / 7013, Loss: 0.689, Accuracy: 0.857, F1: 0.370\n",
      "TRAIN: Epoch: 1, Batch: 1501 / 7013, Loss: 0.753, Accuracy: 0.849, F1: 0.408\n",
      "TRAIN: Epoch: 1, Batch: 1601 / 7013, Loss: 0.750, Accuracy: 0.843, F1: 0.468\n",
      "TRAIN: Epoch: 1, Batch: 1701 / 7013, Loss: 0.672, Accuracy: 0.855, F1: 0.445\n",
      "TRAIN: Epoch: 1, Batch: 1801 / 7013, Loss: 0.633, Accuracy: 0.854, F1: 0.480\n",
      "TRAIN: Epoch: 1, Batch: 1901 / 7013, Loss: 0.363, Accuracy: 0.918, F1: 0.595\n",
      "TRAIN: Epoch: 1, Batch: 2001 / 7013, Loss: 0.591, Accuracy: 0.869, F1: 0.457\n",
      "TRAIN: Epoch: 1, Batch: 2101 / 7013, Loss: 0.718, Accuracy: 0.854, F1: 0.436\n",
      "TRAIN: Epoch: 1, Batch: 2201 / 7013, Loss: 0.837, Accuracy: 0.830, F1: 0.340\n",
      "TRAIN: Epoch: 1, Batch: 2301 / 7013, Loss: 0.647, Accuracy: 0.852, F1: 0.414\n",
      "TRAIN: Epoch: 1, Batch: 2401 / 7013, Loss: 0.516, Accuracy: 0.877, F1: 0.498\n",
      "TRAIN: Epoch: 1, Batch: 2501 / 7013, Loss: 0.475, Accuracy: 0.895, F1: 0.508\n",
      "TRAIN: Epoch: 1, Batch: 2601 / 7013, Loss: 0.476, Accuracy: 0.878, F1: 0.455\n",
      "TRAIN: Epoch: 1, Batch: 2701 / 7013, Loss: 0.633, Accuracy: 0.868, F1: 0.428\n",
      "TRAIN: Epoch: 1, Batch: 2801 / 7013, Loss: 0.506, Accuracy: 0.877, F1: 0.564\n",
      "TRAIN: Epoch: 1, Batch: 2901 / 7013, Loss: 0.560, Accuracy: 0.858, F1: 0.419\n",
      "TRAIN: Epoch: 1, Batch: 3001 / 7013, Loss: 0.501, Accuracy: 0.879, F1: 0.507\n",
      "TRAIN: Epoch: 1, Batch: 3101 / 7013, Loss: 0.631, Accuracy: 0.861, F1: 0.496\n",
      "TRAIN: Epoch: 1, Batch: 3201 / 7013, Loss: 0.605, Accuracy: 0.859, F1: 0.466\n",
      "TRAIN: Epoch: 1, Batch: 3301 / 7013, Loss: 0.502, Accuracy: 0.888, F1: 0.605\n",
      "TRAIN: Epoch: 1, Batch: 3401 / 7013, Loss: 0.528, Accuracy: 0.878, F1: 0.498\n",
      "TRAIN: Epoch: 1, Batch: 3501 / 7013, Loss: 0.536, Accuracy: 0.882, F1: 0.490\n",
      "TRAIN: Epoch: 1, Batch: 3601 / 7013, Loss: 0.563, Accuracy: 0.875, F1: 0.483\n",
      "TRAIN: Epoch: 1, Batch: 3701 / 7013, Loss: 0.628, Accuracy: 0.856, F1: 0.423\n",
      "TRAIN: Epoch: 1, Batch: 3801 / 7013, Loss: 0.516, Accuracy: 0.884, F1: 0.524\n",
      "TRAIN: Epoch: 1, Batch: 3901 / 7013, Loss: 0.482, Accuracy: 0.891, F1: 0.524\n",
      "TRAIN: Epoch: 1, Batch: 4101 / 7013, Loss: 0.548, Accuracy: 0.879, F1: 0.521\n",
      "TRAIN: Epoch: 1, Batch: 4201 / 7013, Loss: 0.456, Accuracy: 0.891, F1: 0.589\n",
      "TRAIN: Epoch: 1, Batch: 4301 / 7013, Loss: 0.524, Accuracy: 0.875, F1: 0.535\n",
      "TRAIN: Epoch: 1, Batch: 4401 / 7013, Loss: 0.426, Accuracy: 0.885, F1: 0.558\n",
      "TRAIN: Epoch: 1, Batch: 4501 / 7013, Loss: 0.519, Accuracy: 0.872, F1: 0.485\n",
      "TRAIN: Epoch: 1, Batch: 4601 / 7013, Loss: 0.610, Accuracy: 0.851, F1: 0.424\n",
      "TRAIN: Epoch: 1, Batch: 4701 / 7013, Loss: 0.425, Accuracy: 0.898, F1: 0.580\n",
      "TRAIN: Epoch: 1, Batch: 4801 / 7013, Loss: 0.559, Accuracy: 0.870, F1: 0.500\n",
      "TRAIN: Epoch: 1, Batch: 4901 / 7013, Loss: 0.596, Accuracy: 0.851, F1: 0.454\n",
      "TRAIN: Epoch: 1, Batch: 5001 / 7013, Loss: 0.359, Accuracy: 0.896, F1: 0.504\n",
      "TRAIN: Epoch: 1, Batch: 5101 / 7013, Loss: 0.576, Accuracy: 0.862, F1: 0.445\n",
      "TRAIN: Epoch: 1, Batch: 5201 / 7013, Loss: 0.440, Accuracy: 0.905, F1: 0.506\n",
      "TRAIN: Epoch: 1, Batch: 5301 / 7013, Loss: 0.424, Accuracy: 0.885, F1: 0.490\n",
      "TRAIN: Epoch: 1, Batch: 5401 / 7013, Loss: 0.433, Accuracy: 0.890, F1: 0.568\n",
      "TRAIN: Epoch: 1, Batch: 5501 / 7013, Loss: 0.507, Accuracy: 0.865, F1: 0.466\n",
      "TRAIN: Epoch: 1, Batch: 5601 / 7013, Loss: 0.639, Accuracy: 0.843, F1: 0.424\n",
      "TRAIN: Epoch: 1, Batch: 5701 / 7013, Loss: 0.421, Accuracy: 0.882, F1: 0.489\n",
      "TRAIN: Epoch: 1, Batch: 5801 / 7013, Loss: 0.422, Accuracy: 0.892, F1: 0.529\n",
      "TRAIN: Epoch: 1, Batch: 5901 / 7013, Loss: 0.213, Accuracy: 0.935, F1: 0.652\n",
      "TRAIN: Epoch: 1, Batch: 6001 / 7013, Loss: 0.623, Accuracy: 0.849, F1: 0.412\n",
      "TRAIN: Epoch: 1, Batch: 6101 / 7013, Loss: 0.422, Accuracy: 0.901, F1: 0.547\n",
      "TRAIN: Epoch: 1, Batch: 6201 / 7013, Loss: 0.436, Accuracy: 0.891, F1: 0.502\n",
      "TRAIN: Epoch: 1, Batch: 6301 / 7013, Loss: 0.480, Accuracy: 0.888, F1: 0.544\n",
      "TRAIN: Epoch: 1, Batch: 6401 / 7013, Loss: 0.374, Accuracy: 0.900, F1: 0.603\n",
      "TRAIN: Epoch: 1, Batch: 6501 / 7013, Loss: 0.506, Accuracy: 0.873, F1: 0.432\n",
      "TRAIN: Epoch: 1, Batch: 6601 / 7013, Loss: 0.588, Accuracy: 0.863, F1: 0.549\n",
      "TRAIN: Epoch: 1, Batch: 6701 / 7013, Loss: 0.419, Accuracy: 0.898, F1: 0.591\n",
      "TRAIN: Epoch: 1, Batch: 6801 / 7013, Loss: 0.679, Accuracy: 0.850, F1: 0.430\n",
      "TRAIN: Epoch: 1, Batch: 6901 / 7013, Loss: 0.295, Accuracy: 0.910, F1: 0.572\n",
      "TRAIN: Epoch: 1, Batch: 7001 / 7013, Loss: 0.397, Accuracy: 0.901, F1: 0.560\n",
      "VAL: Epoch: 1, Batch: 1 / 367, Loss: 0.383, Accuracy: 0.884, F1: 0.560\n",
      "VAL: Epoch: 1, Batch: 101 / 367, Loss: 0.449, Accuracy: 0.906, F1: 0.616\n",
      "VAL: Epoch: 1, Batch: 201 / 367, Loss: 0.169, Accuracy: 0.944, F1: 0.789\n",
      "VAL: Epoch: 1, Batch: 301 / 367, Loss: 0.558, Accuracy: 0.855, F1: 0.494\n",
      "After epoch #1:\n",
      "Train loss: 0.658, Train Accuracy: 0.866, Train F1: 0.467\n",
      "Dev loss: 0.398, Dev Accuracy: 0.895, Dev F1: 0.589\n",
      "\n",
      "TRAIN: Epoch: 2, Batch: 1 / 7013, Loss: 0.228, Accuracy: 0.927, F1: 0.683\n",
      "TRAIN: Epoch: 2, Batch: 101 / 7013, Loss: 0.425, Accuracy: 0.889, F1: 0.543\n",
      "TRAIN: Epoch: 2, Batch: 201 / 7013, Loss: 0.414, Accuracy: 0.889, F1: 0.572\n",
      "TRAIN: Epoch: 2, Batch: 301 / 7013, Loss: 0.405, Accuracy: 0.877, F1: 0.536\n",
      "TRAIN: Epoch: 2, Batch: 401 / 7013, Loss: 0.403, Accuracy: 0.901, F1: 0.559\n",
      "TRAIN: Epoch: 2, Batch: 501 / 7013, Loss: 0.313, Accuracy: 0.921, F1: 0.665\n",
      "TRAIN: Epoch: 2, Batch: 601 / 7013, Loss: 0.373, Accuracy: 0.890, F1: 0.498\n",
      "TRAIN: Epoch: 2, Batch: 701 / 7013, Loss: 0.402, Accuracy: 0.897, F1: 0.523\n",
      "TRAIN: Epoch: 2, Batch: 801 / 7013, Loss: 0.435, Accuracy: 0.867, F1: 0.539\n",
      "TRAIN: Epoch: 2, Batch: 901 / 7013, Loss: 0.395, Accuracy: 0.917, F1: 0.631\n",
      "TRAIN: Epoch: 2, Batch: 1001 / 7013, Loss: 0.386, Accuracy: 0.897, F1: 0.520\n",
      "TRAIN: Epoch: 2, Batch: 1101 / 7013, Loss: 0.503, Accuracy: 0.891, F1: 0.504\n",
      "TRAIN: Epoch: 2, Batch: 1201 / 7013, Loss: 0.606, Accuracy: 0.861, F1: 0.493\n",
      "TRAIN: Epoch: 2, Batch: 1301 / 7013, Loss: 0.319, Accuracy: 0.914, F1: 0.662\n",
      "TRAIN: Epoch: 2, Batch: 1401 / 7013, Loss: 0.453, Accuracy: 0.879, F1: 0.559\n",
      "TRAIN: Epoch: 2, Batch: 1501 / 7013, Loss: 0.396, Accuracy: 0.896, F1: 0.573\n",
      "TRAIN: Epoch: 2, Batch: 1601 / 7013, Loss: 0.451, Accuracy: 0.884, F1: 0.513\n",
      "TRAIN: Epoch: 2, Batch: 1701 / 7013, Loss: 0.234, Accuracy: 0.930, F1: 0.678\n",
      "TRAIN: Epoch: 2, Batch: 1801 / 7013, Loss: 0.328, Accuracy: 0.914, F1: 0.672\n",
      "TRAIN: Epoch: 2, Batch: 1901 / 7013, Loss: 0.452, Accuracy: 0.887, F1: 0.563\n",
      "TRAIN: Epoch: 2, Batch: 2001 / 7013, Loss: 0.429, Accuracy: 0.889, F1: 0.579\n",
      "TRAIN: Epoch: 2, Batch: 2101 / 7013, Loss: 0.428, Accuracy: 0.880, F1: 0.485\n",
      "TRAIN: Epoch: 2, Batch: 2201 / 7013, Loss: 0.452, Accuracy: 0.884, F1: 0.467\n",
      "TRAIN: Epoch: 2, Batch: 2301 / 7013, Loss: 0.259, Accuracy: 0.925, F1: 0.673\n",
      "TRAIN: Epoch: 2, Batch: 2401 / 7013, Loss: 0.512, Accuracy: 0.860, F1: 0.479\n",
      "TRAIN: Epoch: 2, Batch: 2501 / 7013, Loss: 0.301, Accuracy: 0.908, F1: 0.627\n",
      "TRAIN: Epoch: 2, Batch: 2601 / 7013, Loss: 0.463, Accuracy: 0.880, F1: 0.569\n",
      "TRAIN: Epoch: 2, Batch: 2701 / 7013, Loss: 0.326, Accuracy: 0.904, F1: 0.529\n",
      "TRAIN: Epoch: 2, Batch: 2801 / 7013, Loss: 0.397, Accuracy: 0.885, F1: 0.519\n",
      "TRAIN: Epoch: 2, Batch: 2901 / 7013, Loss: 0.271, Accuracy: 0.922, F1: 0.641\n",
      "TRAIN: Epoch: 2, Batch: 3001 / 7013, Loss: 0.636, Accuracy: 0.842, F1: 0.402\n",
      "TRAIN: Epoch: 2, Batch: 3101 / 7013, Loss: 0.385, Accuracy: 0.917, F1: 0.564\n",
      "TRAIN: Epoch: 2, Batch: 3201 / 7013, Loss: 0.395, Accuracy: 0.901, F1: 0.635\n",
      "TRAIN: Epoch: 2, Batch: 3301 / 7013, Loss: 0.380, Accuracy: 0.888, F1: 0.579\n",
      "TRAIN: Epoch: 2, Batch: 3401 / 7013, Loss: 0.301, Accuracy: 0.909, F1: 0.653\n",
      "TRAIN: Epoch: 2, Batch: 3501 / 7013, Loss: 0.315, Accuracy: 0.918, F1: 0.624\n",
      "TRAIN: Epoch: 2, Batch: 3601 / 7013, Loss: 0.359, Accuracy: 0.899, F1: 0.642\n",
      "TRAIN: Epoch: 2, Batch: 3701 / 7013, Loss: 0.287, Accuracy: 0.909, F1: 0.604\n",
      "TRAIN: Epoch: 2, Batch: 3801 / 7013, Loss: 0.223, Accuracy: 0.939, F1: 0.684\n",
      "TRAIN: Epoch: 2, Batch: 3901 / 7013, Loss: 0.367, Accuracy: 0.909, F1: 0.667\n",
      "TRAIN: Epoch: 2, Batch: 4001 / 7013, Loss: 0.319, Accuracy: 0.917, F1: 0.594\n",
      "TRAIN: Epoch: 2, Batch: 4101 / 7013, Loss: 0.548, Accuracy: 0.876, F1: 0.529\n",
      "TRAIN: Epoch: 2, Batch: 4201 / 7013, Loss: 0.300, Accuracy: 0.911, F1: 0.663\n",
      "TRAIN: Epoch: 2, Batch: 4301 / 7013, Loss: 0.212, Accuracy: 0.934, F1: 0.675\n",
      "TRAIN: Epoch: 2, Batch: 4401 / 7013, Loss: 0.355, Accuracy: 0.892, F1: 0.590\n",
      "TRAIN: Epoch: 2, Batch: 4501 / 7013, Loss: 0.349, Accuracy: 0.901, F1: 0.654\n",
      "TRAIN: Epoch: 2, Batch: 4601 / 7013, Loss: 0.228, Accuracy: 0.931, F1: 0.711\n",
      "TRAIN: Epoch: 2, Batch: 4701 / 7013, Loss: 0.539, Accuracy: 0.865, F1: 0.461\n",
      "TRAIN: Epoch: 2, Batch: 4801 / 7013, Loss: 0.287, Accuracy: 0.918, F1: 0.715\n",
      "TRAIN: Epoch: 2, Batch: 4901 / 7013, Loss: 0.338, Accuracy: 0.909, F1: 0.623\n",
      "TRAIN: Epoch: 2, Batch: 5001 / 7013, Loss: 0.405, Accuracy: 0.894, F1: 0.509\n",
      "TRAIN: Epoch: 2, Batch: 5101 / 7013, Loss: 0.415, Accuracy: 0.888, F1: 0.576\n",
      "TRAIN: Epoch: 2, Batch: 5201 / 7013, Loss: 0.317, Accuracy: 0.907, F1: 0.636\n",
      "TRAIN: Epoch: 2, Batch: 5301 / 7013, Loss: 0.444, Accuracy: 0.888, F1: 0.518\n",
      "TRAIN: Epoch: 2, Batch: 5401 / 7013, Loss: 0.400, Accuracy: 0.903, F1: 0.576\n",
      "TRAIN: Epoch: 2, Batch: 5501 / 7013, Loss: 0.430, Accuracy: 0.903, F1: 0.586\n",
      "TRAIN: Epoch: 2, Batch: 5601 / 7013, Loss: 0.323, Accuracy: 0.912, F1: 0.697\n",
      "TRAIN: Epoch: 2, Batch: 5701 / 7013, Loss: 0.344, Accuracy: 0.905, F1: 0.567\n",
      "TRAIN: Epoch: 2, Batch: 5801 / 7013, Loss: 0.245, Accuracy: 0.930, F1: 0.738\n",
      "TRAIN: Epoch: 2, Batch: 5901 / 7013, Loss: 0.286, Accuracy: 0.918, F1: 0.626\n",
      "TRAIN: Epoch: 2, Batch: 6001 / 7013, Loss: 0.306, Accuracy: 0.907, F1: 0.638\n",
      "TRAIN: Epoch: 2, Batch: 6101 / 7013, Loss: 0.467, Accuracy: 0.872, F1: 0.534\n",
      "TRAIN: Epoch: 2, Batch: 6201 / 7013, Loss: 0.506, Accuracy: 0.866, F1: 0.536\n",
      "TRAIN: Epoch: 2, Batch: 6301 / 7013, Loss: 0.341, Accuracy: 0.906, F1: 0.645\n",
      "TRAIN: Epoch: 2, Batch: 6401 / 7013, Loss: 0.280, Accuracy: 0.917, F1: 0.686\n",
      "TRAIN: Epoch: 2, Batch: 6501 / 7013, Loss: 0.431, Accuracy: 0.875, F1: 0.536\n",
      "TRAIN: Epoch: 2, Batch: 6601 / 7013, Loss: 0.287, Accuracy: 0.917, F1: 0.600\n",
      "TRAIN: Epoch: 2, Batch: 6701 / 7013, Loss: 0.446, Accuracy: 0.884, F1: 0.529\n",
      "TRAIN: Epoch: 2, Batch: 6801 / 7013, Loss: 0.380, Accuracy: 0.898, F1: 0.555\n",
      "TRAIN: Epoch: 2, Batch: 6901 / 7013, Loss: 0.500, Accuracy: 0.890, F1: 0.549\n",
      "TRAIN: Epoch: 2, Batch: 7001 / 7013, Loss: 0.541, Accuracy: 0.870, F1: 0.568\n",
      "VAL: Epoch: 2, Batch: 1 / 367, Loss: 0.315, Accuracy: 0.905, F1: 0.627\n",
      "VAL: Epoch: 2, Batch: 101 / 367, Loss: 0.384, Accuracy: 0.928, F1: 0.696\n",
      "VAL: Epoch: 2, Batch: 201 / 367, Loss: 0.077, Accuracy: 0.975, F1: 0.914\n",
      "VAL: Epoch: 2, Batch: 301 / 367, Loss: 0.446, Accuracy: 0.882, F1: 0.592\n",
      "After epoch #2:\n",
      "Train loss: 0.376, Train Accuracy: 0.899, Train F1: 0.582\n",
      "Dev loss: 0.305, Dev Accuracy: 0.914, Dev F1: 0.664\n",
      "\n",
      "TRAIN: Epoch: 3, Batch: 1 / 7013, Loss: 0.265, Accuracy: 0.920, F1: 0.622\n",
      "TRAIN: Epoch: 3, Batch: 101 / 7013, Loss: 0.237, Accuracy: 0.929, F1: 0.723\n",
      "TRAIN: Epoch: 3, Batch: 201 / 7013, Loss: 0.166, Accuracy: 0.941, F1: 0.780\n",
      "TRAIN: Epoch: 3, Batch: 301 / 7013, Loss: 0.261, Accuracy: 0.919, F1: 0.686\n",
      "TRAIN: Epoch: 3, Batch: 401 / 7013, Loss: 0.371, Accuracy: 0.898, F1: 0.575\n",
      "TRAIN: Epoch: 3, Batch: 501 / 7013, Loss: 0.228, Accuracy: 0.931, F1: 0.725\n",
      "TRAIN: Epoch: 3, Batch: 601 / 7013, Loss: 0.228, Accuracy: 0.930, F1: 0.632\n",
      "TRAIN: Epoch: 3, Batch: 701 / 7013, Loss: 0.332, Accuracy: 0.899, F1: 0.611\n",
      "TRAIN: Epoch: 3, Batch: 801 / 7013, Loss: 0.268, Accuracy: 0.912, F1: 0.649\n",
      "TRAIN: Epoch: 3, Batch: 901 / 7013, Loss: 0.380, Accuracy: 0.895, F1: 0.640\n",
      "TRAIN: Epoch: 3, Batch: 1001 / 7013, Loss: 0.482, Accuracy: 0.895, F1: 0.597\n",
      "TRAIN: Epoch: 3, Batch: 1101 / 7013, Loss: 0.226, Accuracy: 0.925, F1: 0.631\n",
      "TRAIN: Epoch: 3, Batch: 1201 / 7013, Loss: 0.354, Accuracy: 0.916, F1: 0.593\n",
      "TRAIN: Epoch: 3, Batch: 1301 / 7013, Loss: 0.398, Accuracy: 0.911, F1: 0.616\n",
      "TRAIN: Epoch: 3, Batch: 1401 / 7013, Loss: 0.367, Accuracy: 0.913, F1: 0.594\n",
      "TRAIN: Epoch: 3, Batch: 1501 / 7013, Loss: 0.271, Accuracy: 0.924, F1: 0.655\n",
      "TRAIN: Epoch: 3, Batch: 1601 / 7013, Loss: 0.293, Accuracy: 0.927, F1: 0.697\n",
      "TRAIN: Epoch: 3, Batch: 1701 / 7013, Loss: 0.368, Accuracy: 0.917, F1: 0.650\n",
      "TRAIN: Epoch: 3, Batch: 1801 / 7013, Loss: 0.244, Accuracy: 0.932, F1: 0.750\n",
      "TRAIN: Epoch: 3, Batch: 1901 / 7013, Loss: 0.311, Accuracy: 0.898, F1: 0.539\n",
      "TRAIN: Epoch: 3, Batch: 2001 / 7013, Loss: 0.166, Accuracy: 0.943, F1: 0.782\n",
      "TRAIN: Epoch: 3, Batch: 2101 / 7013, Loss: 0.323, Accuracy: 0.907, F1: 0.623\n",
      "TRAIN: Epoch: 3, Batch: 2201 / 7013, Loss: 0.233, Accuracy: 0.929, F1: 0.672\n",
      "TRAIN: Epoch: 3, Batch: 2301 / 7013, Loss: 0.308, Accuracy: 0.916, F1: 0.673\n",
      "TRAIN: Epoch: 3, Batch: 2401 / 7013, Loss: 0.362, Accuracy: 0.901, F1: 0.587\n",
      "TRAIN: Epoch: 3, Batch: 2501 / 7013, Loss: 0.288, Accuracy: 0.921, F1: 0.635\n",
      "TRAIN: Epoch: 3, Batch: 2601 / 7013, Loss: 0.378, Accuracy: 0.904, F1: 0.641\n",
      "TRAIN: Epoch: 3, Batch: 2701 / 7013, Loss: 0.332, Accuracy: 0.905, F1: 0.602\n",
      "TRAIN: Epoch: 3, Batch: 2801 / 7013, Loss: 0.226, Accuracy: 0.937, F1: 0.695\n",
      "TRAIN: Epoch: 3, Batch: 2901 / 7013, Loss: 0.365, Accuracy: 0.914, F1: 0.648\n",
      "TRAIN: Epoch: 3, Batch: 3001 / 7013, Loss: 0.469, Accuracy: 0.895, F1: 0.609\n",
      "TRAIN: Epoch: 3, Batch: 3101 / 7013, Loss: 0.275, Accuracy: 0.930, F1: 0.648\n",
      "TRAIN: Epoch: 3, Batch: 3201 / 7013, Loss: 0.275, Accuracy: 0.935, F1: 0.732\n",
      "TRAIN: Epoch: 3, Batch: 3301 / 7013, Loss: 0.250, Accuracy: 0.919, F1: 0.611\n",
      "TRAIN: Epoch: 3, Batch: 3401 / 7013, Loss: 0.275, Accuracy: 0.918, F1: 0.660\n",
      "TRAIN: Epoch: 3, Batch: 3501 / 7013, Loss: 0.303, Accuracy: 0.912, F1: 0.664\n",
      "TRAIN: Epoch: 3, Batch: 3601 / 7013, Loss: 0.282, Accuracy: 0.931, F1: 0.639\n",
      "TRAIN: Epoch: 3, Batch: 3701 / 7013, Loss: 0.376, Accuracy: 0.896, F1: 0.511\n",
      "TRAIN: Epoch: 3, Batch: 3801 / 7013, Loss: 0.259, Accuracy: 0.925, F1: 0.619\n",
      "TRAIN: Epoch: 3, Batch: 3901 / 7013, Loss: 0.360, Accuracy: 0.921, F1: 0.616\n",
      "TRAIN: Epoch: 3, Batch: 4001 / 7013, Loss: 0.167, Accuracy: 0.943, F1: 0.776\n",
      "TRAIN: Epoch: 3, Batch: 4101 / 7013, Loss: 0.297, Accuracy: 0.924, F1: 0.716\n",
      "TRAIN: Epoch: 3, Batch: 4201 / 7013, Loss: 0.410, Accuracy: 0.900, F1: 0.543\n",
      "TRAIN: Epoch: 3, Batch: 4301 / 7013, Loss: 0.284, Accuracy: 0.929, F1: 0.647\n",
      "TRAIN: Epoch: 3, Batch: 4401 / 7013, Loss: 0.282, Accuracy: 0.922, F1: 0.624\n",
      "TRAIN: Epoch: 3, Batch: 4501 / 7013, Loss: 0.311, Accuracy: 0.906, F1: 0.628\n",
      "TRAIN: Epoch: 3, Batch: 4601 / 7013, Loss: 0.353, Accuracy: 0.892, F1: 0.606\n",
      "TRAIN: Epoch: 3, Batch: 4701 / 7013, Loss: 0.258, Accuracy: 0.931, F1: 0.706\n",
      "TRAIN: Epoch: 3, Batch: 4801 / 7013, Loss: 0.382, Accuracy: 0.890, F1: 0.587\n",
      "TRAIN: Epoch: 3, Batch: 4901 / 7013, Loss: 0.378, Accuracy: 0.891, F1: 0.579\n",
      "TRAIN: Epoch: 3, Batch: 5001 / 7013, Loss: 0.383, Accuracy: 0.901, F1: 0.676\n",
      "TRAIN: Epoch: 3, Batch: 5101 / 7013, Loss: 0.271, Accuracy: 0.943, F1: 0.746\n",
      "TRAIN: Epoch: 3, Batch: 5201 / 7013, Loss: 0.285, Accuracy: 0.915, F1: 0.667\n",
      "TRAIN: Epoch: 3, Batch: 5301 / 7013, Loss: 0.311, Accuracy: 0.913, F1: 0.680\n",
      "TRAIN: Epoch: 3, Batch: 5401 / 7013, Loss: 0.210, Accuracy: 0.941, F1: 0.786\n",
      "TRAIN: Epoch: 3, Batch: 5501 / 7013, Loss: 0.178, Accuracy: 0.944, F1: 0.739\n",
      "TRAIN: Epoch: 3, Batch: 5601 / 7013, Loss: 0.254, Accuracy: 0.932, F1: 0.681\n",
      "TRAIN: Epoch: 3, Batch: 5701 / 7013, Loss: 0.277, Accuracy: 0.920, F1: 0.606\n",
      "TRAIN: Epoch: 3, Batch: 5801 / 7013, Loss: 0.243, Accuracy: 0.924, F1: 0.699\n",
      "TRAIN: Epoch: 3, Batch: 5901 / 7013, Loss: 0.144, Accuracy: 0.946, F1: 0.742\n",
      "TRAIN: Epoch: 3, Batch: 6001 / 7013, Loss: 0.118, Accuracy: 0.967, F1: 0.821\n",
      "TRAIN: Epoch: 3, Batch: 6101 / 7013, Loss: 0.203, Accuracy: 0.946, F1: 0.754\n",
      "TRAIN: Epoch: 3, Batch: 6201 / 7013, Loss: 0.420, Accuracy: 0.882, F1: 0.566\n",
      "TRAIN: Epoch: 3, Batch: 6301 / 7013, Loss: 0.260, Accuracy: 0.923, F1: 0.713\n",
      "TRAIN: Epoch: 3, Batch: 6401 / 7013, Loss: 0.481, Accuracy: 0.873, F1: 0.569\n",
      "TRAIN: Epoch: 3, Batch: 6501 / 7013, Loss: 0.202, Accuracy: 0.942, F1: 0.721\n",
      "TRAIN: Epoch: 3, Batch: 6601 / 7013, Loss: 0.174, Accuracy: 0.954, F1: 0.764\n",
      "TRAIN: Epoch: 3, Batch: 6701 / 7013, Loss: 0.301, Accuracy: 0.906, F1: 0.613\n",
      "TRAIN: Epoch: 3, Batch: 6801 / 7013, Loss: 0.250, Accuracy: 0.922, F1: 0.661\n",
      "TRAIN: Epoch: 3, Batch: 6901 / 7013, Loss: 0.186, Accuracy: 0.948, F1: 0.755\n",
      "TRAIN: Epoch: 3, Batch: 7001 / 7013, Loss: 0.338, Accuracy: 0.912, F1: 0.641\n",
      "VAL: Epoch: 3, Batch: 1 / 367, Loss: 0.263, Accuracy: 0.920, F1: 0.714\n",
      "VAL: Epoch: 3, Batch: 101 / 367, Loss: 0.339, Accuracy: 0.947, F1: 0.775\n",
      "VAL: Epoch: 3, Batch: 201 / 367, Loss: 0.050, Accuracy: 0.984, F1: 0.937\n",
      "VAL: Epoch: 3, Batch: 301 / 367, Loss: 0.435, Accuracy: 0.900, F1: 0.659\n",
      "After epoch #3:\n",
      "Train loss: 0.286, Train Accuracy: 0.920, Train F1: 0.663\n",
      "Dev loss: 0.274, Dev Accuracy: 0.930, Dev F1: 0.729\n",
      "\n",
      "TRAIN: Epoch: 4, Batch: 1 / 7013, Loss: 0.414, Accuracy: 0.876, F1: 0.522\n",
      "TRAIN: Epoch: 4, Batch: 101 / 7013, Loss: 0.226, Accuracy: 0.928, F1: 0.689\n",
      "TRAIN: Epoch: 4, Batch: 201 / 7013, Loss: 0.270, Accuracy: 0.917, F1: 0.654\n",
      "TRAIN: Epoch: 4, Batch: 301 / 7013, Loss: 0.201, Accuracy: 0.936, F1: 0.774\n",
      "TRAIN: Epoch: 4, Batch: 401 / 7013, Loss: 0.194, Accuracy: 0.937, F1: 0.695\n",
      "TRAIN: Epoch: 4, Batch: 501 / 7013, Loss: 0.321, Accuracy: 0.924, F1: 0.677\n",
      "TRAIN: Epoch: 4, Batch: 601 / 7013, Loss: 0.267, Accuracy: 0.922, F1: 0.664\n",
      "TRAIN: Epoch: 4, Batch: 701 / 7013, Loss: 0.239, Accuracy: 0.923, F1: 0.663\n",
      "TRAIN: Epoch: 4, Batch: 801 / 7013, Loss: 0.157, Accuracy: 0.943, F1: 0.747\n",
      "TRAIN: Epoch: 4, Batch: 901 / 7013, Loss: 0.203, Accuracy: 0.941, F1: 0.759\n",
      "TRAIN: Epoch: 4, Batch: 1001 / 7013, Loss: 0.250, Accuracy: 0.916, F1: 0.665\n",
      "TRAIN: Epoch: 4, Batch: 1101 / 7013, Loss: 0.148, Accuracy: 0.950, F1: 0.796\n",
      "TRAIN: Epoch: 4, Batch: 1201 / 7013, Loss: 0.204, Accuracy: 0.939, F1: 0.765\n",
      "TRAIN: Epoch: 4, Batch: 1301 / 7013, Loss: 0.115, Accuracy: 0.966, F1: 0.837\n",
      "TRAIN: Epoch: 4, Batch: 1401 / 7013, Loss: 0.238, Accuracy: 0.929, F1: 0.664\n",
      "TRAIN: Epoch: 4, Batch: 1501 / 7013, Loss: 0.370, Accuracy: 0.906, F1: 0.638\n",
      "TRAIN: Epoch: 4, Batch: 1601 / 7013, Loss: 0.357, Accuracy: 0.917, F1: 0.660\n",
      "TRAIN: Epoch: 4, Batch: 1701 / 7013, Loss: 0.240, Accuracy: 0.935, F1: 0.768\n",
      "TRAIN: Epoch: 4, Batch: 1801 / 7013, Loss: 0.326, Accuracy: 0.923, F1: 0.646\n",
      "TRAIN: Epoch: 4, Batch: 1901 / 7013, Loss: 0.203, Accuracy: 0.937, F1: 0.721\n",
      "TRAIN: Epoch: 4, Batch: 2001 / 7013, Loss: 0.149, Accuracy: 0.957, F1: 0.787\n",
      "TRAIN: Epoch: 4, Batch: 2101 / 7013, Loss: 0.173, Accuracy: 0.953, F1: 0.810\n",
      "TRAIN: Epoch: 4, Batch: 2201 / 7013, Loss: 0.186, Accuracy: 0.942, F1: 0.720\n",
      "TRAIN: Epoch: 4, Batch: 2301 / 7013, Loss: 0.194, Accuracy: 0.945, F1: 0.748\n",
      "TRAIN: Epoch: 4, Batch: 2401 / 7013, Loss: 0.277, Accuracy: 0.914, F1: 0.700\n",
      "TRAIN: Epoch: 4, Batch: 2501 / 7013, Loss: 0.253, Accuracy: 0.925, F1: 0.695\n",
      "TRAIN: Epoch: 4, Batch: 2601 / 7013, Loss: 0.294, Accuracy: 0.921, F1: 0.686\n",
      "TRAIN: Epoch: 4, Batch: 2701 / 7013, Loss: 0.217, Accuracy: 0.936, F1: 0.706\n",
      "TRAIN: Epoch: 4, Batch: 2801 / 7013, Loss: 0.278, Accuracy: 0.915, F1: 0.659\n",
      "TRAIN: Epoch: 4, Batch: 2901 / 7013, Loss: 0.322, Accuracy: 0.903, F1: 0.688\n",
      "TRAIN: Epoch: 4, Batch: 3001 / 7013, Loss: 0.197, Accuracy: 0.943, F1: 0.749\n",
      "TRAIN: Epoch: 4, Batch: 3101 / 7013, Loss: 0.232, Accuracy: 0.933, F1: 0.717\n",
      "TRAIN: Epoch: 4, Batch: 3201 / 7013, Loss: 0.237, Accuracy: 0.926, F1: 0.652\n",
      "TRAIN: Epoch: 4, Batch: 3301 / 7013, Loss: 0.270, Accuracy: 0.925, F1: 0.679\n",
      "TRAIN: Epoch: 4, Batch: 3401 / 7013, Loss: 0.182, Accuracy: 0.942, F1: 0.774\n",
      "TRAIN: Epoch: 4, Batch: 3501 / 7013, Loss: 0.242, Accuracy: 0.940, F1: 0.734\n",
      "TRAIN: Epoch: 4, Batch: 3601 / 7013, Loss: 0.243, Accuracy: 0.925, F1: 0.687\n",
      "TRAIN: Epoch: 4, Batch: 3701 / 7013, Loss: 0.149, Accuracy: 0.950, F1: 0.767\n",
      "TRAIN: Epoch: 4, Batch: 3801 / 7013, Loss: 0.251, Accuracy: 0.939, F1: 0.752\n",
      "TRAIN: Epoch: 4, Batch: 3901 / 7013, Loss: 0.203, Accuracy: 0.946, F1: 0.798\n",
      "TRAIN: Epoch: 4, Batch: 4001 / 7013, Loss: 0.268, Accuracy: 0.922, F1: 0.644\n",
      "TRAIN: Epoch: 4, Batch: 4101 / 7013, Loss: 0.284, Accuracy: 0.923, F1: 0.723\n",
      "TRAIN: Epoch: 4, Batch: 4201 / 7013, Loss: 0.283, Accuracy: 0.922, F1: 0.710\n",
      "TRAIN: Epoch: 4, Batch: 4301 / 7013, Loss: 0.167, Accuracy: 0.955, F1: 0.826\n",
      "TRAIN: Epoch: 4, Batch: 4401 / 7013, Loss: 0.274, Accuracy: 0.906, F1: 0.589\n",
      "TRAIN: Epoch: 4, Batch: 4501 / 7013, Loss: 0.230, Accuracy: 0.928, F1: 0.703\n",
      "TRAIN: Epoch: 4, Batch: 4601 / 7013, Loss: 0.240, Accuracy: 0.932, F1: 0.692\n",
      "TRAIN: Epoch: 4, Batch: 4701 / 7013, Loss: 0.180, Accuracy: 0.946, F1: 0.730\n",
      "TRAIN: Epoch: 4, Batch: 4801 / 7013, Loss: 0.190, Accuracy: 0.939, F1: 0.737\n",
      "TRAIN: Epoch: 4, Batch: 4901 / 7013, Loss: 0.303, Accuracy: 0.923, F1: 0.703\n",
      "TRAIN: Epoch: 4, Batch: 5001 / 7013, Loss: 0.276, Accuracy: 0.917, F1: 0.659\n",
      "TRAIN: Epoch: 4, Batch: 5101 / 7013, Loss: 0.196, Accuracy: 0.946, F1: 0.750\n",
      "TRAIN: Epoch: 4, Batch: 5201 / 7013, Loss: 0.195, Accuracy: 0.945, F1: 0.770\n",
      "TRAIN: Epoch: 4, Batch: 5301 / 7013, Loss: 0.191, Accuracy: 0.958, F1: 0.833\n",
      "TRAIN: Epoch: 4, Batch: 5401 / 7013, Loss: 0.118, Accuracy: 0.963, F1: 0.815\n",
      "TRAIN: Epoch: 4, Batch: 5501 / 7013, Loss: 0.205, Accuracy: 0.940, F1: 0.691\n",
      "TRAIN: Epoch: 4, Batch: 5601 / 7013, Loss: 0.272, Accuracy: 0.920, F1: 0.703\n",
      "TRAIN: Epoch: 4, Batch: 5701 / 7013, Loss: 0.181, Accuracy: 0.947, F1: 0.780\n",
      "TRAIN: Epoch: 4, Batch: 5801 / 7013, Loss: 0.219, Accuracy: 0.942, F1: 0.747\n",
      "TRAIN: Epoch: 4, Batch: 5901 / 7013, Loss: 0.202, Accuracy: 0.937, F1: 0.714\n",
      "TRAIN: Epoch: 4, Batch: 6001 / 7013, Loss: 0.194, Accuracy: 0.939, F1: 0.748\n",
      "TRAIN: Epoch: 4, Batch: 6101 / 7013, Loss: 0.119, Accuracy: 0.961, F1: 0.865\n",
      "TRAIN: Epoch: 4, Batch: 6201 / 7013, Loss: 0.246, Accuracy: 0.927, F1: 0.734\n",
      "TRAIN: Epoch: 4, Batch: 6301 / 7013, Loss: 0.182, Accuracy: 0.947, F1: 0.768\n",
      "TRAIN: Epoch: 4, Batch: 6401 / 7013, Loss: 0.255, Accuracy: 0.928, F1: 0.691\n",
      "TRAIN: Epoch: 4, Batch: 6501 / 7013, Loss: 0.231, Accuracy: 0.928, F1: 0.734\n",
      "TRAIN: Epoch: 4, Batch: 6601 / 7013, Loss: 0.284, Accuracy: 0.916, F1: 0.613\n",
      "TRAIN: Epoch: 4, Batch: 6701 / 7013, Loss: 0.080, Accuracy: 0.972, F1: 0.851\n",
      "TRAIN: Epoch: 4, Batch: 6801 / 7013, Loss: 0.205, Accuracy: 0.942, F1: 0.797\n",
      "TRAIN: Epoch: 4, Batch: 6901 / 7013, Loss: 0.172, Accuracy: 0.950, F1: 0.774\n",
      "TRAIN: Epoch: 4, Batch: 7001 / 7013, Loss: 0.269, Accuracy: 0.934, F1: 0.739\n",
      "VAL: Epoch: 4, Batch: 1 / 367, Loss: 0.249, Accuracy: 0.929, F1: 0.781\n",
      "VAL: Epoch: 4, Batch: 101 / 367, Loss: 0.407, Accuracy: 0.937, F1: 0.731\n",
      "VAL: Epoch: 4, Batch: 201 / 367, Loss: 0.034, Accuracy: 0.986, F1: 0.966\n",
      "VAL: Epoch: 4, Batch: 301 / 367, Loss: 0.435, Accuracy: 0.909, F1: 0.674\n",
      "After epoch #4:\n",
      "Train loss: 0.230, Train Accuracy: 0.934, Train F1: 0.725\n",
      "Dev loss: 0.268, Dev Accuracy: 0.936, Dev F1: 0.754\n",
      "\n",
      "TRAIN: Epoch: 5, Batch: 1 / 7013, Loss: 0.161, Accuracy: 0.950, F1: 0.790\n",
      "TRAIN: Epoch: 5, Batch: 101 / 7013, Loss: 0.180, Accuracy: 0.951, F1: 0.812\n",
      "TRAIN: Epoch: 5, Batch: 201 / 7013, Loss: 0.140, Accuracy: 0.960, F1: 0.815\n",
      "TRAIN: Epoch: 5, Batch: 301 / 7013, Loss: 0.309, Accuracy: 0.918, F1: 0.683\n",
      "TRAIN: Epoch: 5, Batch: 401 / 7013, Loss: 0.102, Accuracy: 0.965, F1: 0.862\n",
      "TRAIN: Epoch: 5, Batch: 501 / 7013, Loss: 0.161, Accuracy: 0.957, F1: 0.827\n",
      "TRAIN: Epoch: 5, Batch: 601 / 7013, Loss: 0.187, Accuracy: 0.951, F1: 0.785\n",
      "TRAIN: Epoch: 5, Batch: 701 / 7013, Loss: 0.128, Accuracy: 0.954, F1: 0.800\n",
      "TRAIN: Epoch: 5, Batch: 801 / 7013, Loss: 0.243, Accuracy: 0.925, F1: 0.736\n",
      "TRAIN: Epoch: 5, Batch: 901 / 7013, Loss: 0.149, Accuracy: 0.953, F1: 0.783\n",
      "TRAIN: Epoch: 5, Batch: 1001 / 7013, Loss: 0.149, Accuracy: 0.954, F1: 0.787\n",
      "TRAIN: Epoch: 5, Batch: 1101 / 7013, Loss: 0.240, Accuracy: 0.932, F1: 0.741\n",
      "TRAIN: Epoch: 5, Batch: 1201 / 7013, Loss: 0.154, Accuracy: 0.944, F1: 0.805\n",
      "TRAIN: Epoch: 5, Batch: 1301 / 7013, Loss: 0.149, Accuracy: 0.952, F1: 0.814\n",
      "TRAIN: Epoch: 5, Batch: 1401 / 7013, Loss: 0.182, Accuracy: 0.949, F1: 0.801\n",
      "TRAIN: Epoch: 5, Batch: 1501 / 7013, Loss: 0.118, Accuracy: 0.961, F1: 0.824\n",
      "TRAIN: Epoch: 5, Batch: 1601 / 7013, Loss: 0.157, Accuracy: 0.955, F1: 0.816\n",
      "TRAIN: Epoch: 5, Batch: 1701 / 7013, Loss: 0.178, Accuracy: 0.947, F1: 0.767\n",
      "TRAIN: Epoch: 5, Batch: 1801 / 7013, Loss: 0.124, Accuracy: 0.962, F1: 0.828\n",
      "TRAIN: Epoch: 5, Batch: 1901 / 7013, Loss: 0.273, Accuracy: 0.935, F1: 0.743\n",
      "TRAIN: Epoch: 5, Batch: 2001 / 7013, Loss: 0.190, Accuracy: 0.931, F1: 0.769\n",
      "TRAIN: Epoch: 5, Batch: 2101 / 7013, Loss: 0.328, Accuracy: 0.913, F1: 0.691\n",
      "TRAIN: Epoch: 5, Batch: 2201 / 7013, Loss: 0.203, Accuracy: 0.943, F1: 0.760\n",
      "TRAIN: Epoch: 5, Batch: 2301 / 7013, Loss: 0.138, Accuracy: 0.952, F1: 0.791\n",
      "TRAIN: Epoch: 5, Batch: 2401 / 7013, Loss: 0.174, Accuracy: 0.943, F1: 0.785\n",
      "TRAIN: Epoch: 5, Batch: 2501 / 7013, Loss: 0.171, Accuracy: 0.937, F1: 0.750\n",
      "TRAIN: Epoch: 5, Batch: 2601 / 7013, Loss: 0.187, Accuracy: 0.949, F1: 0.775\n",
      "TRAIN: Epoch: 5, Batch: 2701 / 7013, Loss: 0.146, Accuracy: 0.955, F1: 0.798\n",
      "TRAIN: Epoch: 5, Batch: 2801 / 7013, Loss: 0.170, Accuracy: 0.947, F1: 0.777\n",
      "TRAIN: Epoch: 5, Batch: 2901 / 7013, Loss: 0.122, Accuracy: 0.963, F1: 0.831\n",
      "TRAIN: Epoch: 5, Batch: 3001 / 7013, Loss: 0.200, Accuracy: 0.943, F1: 0.742\n",
      "TRAIN: Epoch: 5, Batch: 3101 / 7013, Loss: 0.220, Accuracy: 0.942, F1: 0.755\n",
      "TRAIN: Epoch: 5, Batch: 3201 / 7013, Loss: 0.143, Accuracy: 0.956, F1: 0.790\n",
      "TRAIN: Epoch: 5, Batch: 3301 / 7013, Loss: 0.194, Accuracy: 0.937, F1: 0.726\n",
      "TRAIN: Epoch: 5, Batch: 3401 / 7013, Loss: 0.223, Accuracy: 0.923, F1: 0.710\n",
      "TRAIN: Epoch: 5, Batch: 3501 / 7013, Loss: 0.160, Accuracy: 0.949, F1: 0.795\n",
      "TRAIN: Epoch: 5, Batch: 3601 / 7013, Loss: 0.179, Accuracy: 0.952, F1: 0.766\n",
      "TRAIN: Epoch: 5, Batch: 3701 / 7013, Loss: 0.208, Accuracy: 0.938, F1: 0.750\n",
      "TRAIN: Epoch: 5, Batch: 3801 / 7013, Loss: 0.200, Accuracy: 0.947, F1: 0.765\n",
      "TRAIN: Epoch: 5, Batch: 3901 / 7013, Loss: 0.241, Accuracy: 0.928, F1: 0.698\n",
      "TRAIN: Epoch: 5, Batch: 4001 / 7013, Loss: 0.179, Accuracy: 0.948, F1: 0.767\n",
      "TRAIN: Epoch: 5, Batch: 4101 / 7013, Loss: 0.124, Accuracy: 0.956, F1: 0.814\n",
      "TRAIN: Epoch: 5, Batch: 4201 / 7013, Loss: 0.130, Accuracy: 0.961, F1: 0.831\n",
      "TRAIN: Epoch: 5, Batch: 4301 / 7013, Loss: 0.170, Accuracy: 0.949, F1: 0.782\n",
      "TRAIN: Epoch: 5, Batch: 4401 / 7013, Loss: 0.192, Accuracy: 0.942, F1: 0.739\n",
      "TRAIN: Epoch: 5, Batch: 4501 / 7013, Loss: 0.179, Accuracy: 0.952, F1: 0.751\n",
      "TRAIN: Epoch: 5, Batch: 4601 / 7013, Loss: 0.301, Accuracy: 0.917, F1: 0.705\n",
      "TRAIN: Epoch: 5, Batch: 4701 / 7013, Loss: 0.162, Accuracy: 0.941, F1: 0.721\n",
      "TRAIN: Epoch: 5, Batch: 4801 / 7013, Loss: 0.222, Accuracy: 0.929, F1: 0.705\n",
      "TRAIN: Epoch: 5, Batch: 4901 / 7013, Loss: 0.231, Accuracy: 0.943, F1: 0.743\n",
      "TRAIN: Epoch: 5, Batch: 5001 / 7013, Loss: 0.090, Accuracy: 0.971, F1: 0.865\n",
      "TRAIN: Epoch: 5, Batch: 5101 / 7013, Loss: 0.094, Accuracy: 0.967, F1: 0.844\n",
      "TRAIN: Epoch: 5, Batch: 5201 / 7013, Loss: 0.236, Accuracy: 0.936, F1: 0.734\n",
      "TRAIN: Epoch: 5, Batch: 5301 / 7013, Loss: 0.285, Accuracy: 0.928, F1: 0.673\n",
      "TRAIN: Epoch: 5, Batch: 5401 / 7013, Loss: 0.290, Accuracy: 0.924, F1: 0.666\n",
      "TRAIN: Epoch: 5, Batch: 5501 / 7013, Loss: 0.150, Accuracy: 0.961, F1: 0.851\n",
      "TRAIN: Epoch: 5, Batch: 5601 / 7013, Loss: 0.240, Accuracy: 0.939, F1: 0.767\n",
      "TRAIN: Epoch: 5, Batch: 5701 / 7013, Loss: 0.142, Accuracy: 0.956, F1: 0.818\n",
      "TRAIN: Epoch: 5, Batch: 5801 / 7013, Loss: 0.165, Accuracy: 0.951, F1: 0.753\n",
      "TRAIN: Epoch: 5, Batch: 5901 / 7013, Loss: 0.358, Accuracy: 0.911, F1: 0.654\n",
      "TRAIN: Epoch: 5, Batch: 6001 / 7013, Loss: 0.159, Accuracy: 0.953, F1: 0.803\n",
      "TRAIN: Epoch: 5, Batch: 6101 / 7013, Loss: 0.129, Accuracy: 0.955, F1: 0.807\n",
      "TRAIN: Epoch: 5, Batch: 6201 / 7013, Loss: 0.104, Accuracy: 0.964, F1: 0.891\n",
      "TRAIN: Epoch: 5, Batch: 6301 / 7013, Loss: 0.144, Accuracy: 0.962, F1: 0.835\n",
      "TRAIN: Epoch: 5, Batch: 6401 / 7013, Loss: 0.241, Accuracy: 0.931, F1: 0.750\n",
      "TRAIN: Epoch: 5, Batch: 6501 / 7013, Loss: 0.194, Accuracy: 0.943, F1: 0.778\n",
      "TRAIN: Epoch: 5, Batch: 6601 / 7013, Loss: 0.267, Accuracy: 0.923, F1: 0.732\n",
      "TRAIN: Epoch: 5, Batch: 6701 / 7013, Loss: 0.143, Accuracy: 0.954, F1: 0.812\n",
      "TRAIN: Epoch: 5, Batch: 6801 / 7013, Loss: 0.213, Accuracy: 0.932, F1: 0.728\n",
      "TRAIN: Epoch: 5, Batch: 6901 / 7013, Loss: 0.215, Accuracy: 0.935, F1: 0.735\n",
      "TRAIN: Epoch: 5, Batch: 7001 / 7013, Loss: 0.124, Accuracy: 0.968, F1: 0.833\n",
      "VAL: Epoch: 5, Batch: 1 / 367, Loss: 0.203, Accuracy: 0.936, F1: 0.766\n",
      "VAL: Epoch: 5, Batch: 101 / 367, Loss: 0.396, Accuracy: 0.938, F1: 0.738\n",
      "VAL: Epoch: 5, Batch: 201 / 367, Loss: 0.039, Accuracy: 0.987, F1: 0.963\n",
      "VAL: Epoch: 5, Batch: 301 / 367, Loss: 0.315, Accuracy: 0.925, F1: 0.751\n",
      "After epoch #5:\n",
      "Train loss: 0.190, Train Accuracy: 0.946, Train F1: 0.772\n",
      "Dev loss: 0.266, Dev Accuracy: 0.940, Dev F1: 0.774\n",
      "\n",
      "TRAIN: Epoch: 6, Batch: 1 / 7013, Loss: 0.158, Accuracy: 0.958, F1: 0.825\n",
      "TRAIN: Epoch: 6, Batch: 101 / 7013, Loss: 0.229, Accuracy: 0.944, F1: 0.794\n",
      "TRAIN: Epoch: 6, Batch: 201 / 7013, Loss: 0.111, Accuracy: 0.963, F1: 0.864\n",
      "TRAIN: Epoch: 6, Batch: 301 / 7013, Loss: 0.217, Accuracy: 0.941, F1: 0.755\n",
      "TRAIN: Epoch: 6, Batch: 401 / 7013, Loss: 0.197, Accuracy: 0.951, F1: 0.784\n",
      "TRAIN: Epoch: 6, Batch: 501 / 7013, Loss: 0.268, Accuracy: 0.928, F1: 0.709\n",
      "TRAIN: Epoch: 6, Batch: 601 / 7013, Loss: 0.195, Accuracy: 0.941, F1: 0.767\n",
      "TRAIN: Epoch: 6, Batch: 701 / 7013, Loss: 0.127, Accuracy: 0.958, F1: 0.805\n",
      "TRAIN: Epoch: 6, Batch: 801 / 7013, Loss: 0.127, Accuracy: 0.957, F1: 0.796\n",
      "TRAIN: Epoch: 6, Batch: 901 / 7013, Loss: 0.183, Accuracy: 0.954, F1: 0.855\n",
      "TRAIN: Epoch: 6, Batch: 1001 / 7013, Loss: 0.185, Accuracy: 0.947, F1: 0.779\n",
      "TRAIN: Epoch: 6, Batch: 1101 / 7013, Loss: 0.180, Accuracy: 0.956, F1: 0.831\n",
      "TRAIN: Epoch: 6, Batch: 1201 / 7013, Loss: 0.153, Accuracy: 0.952, F1: 0.807\n",
      "TRAIN: Epoch: 6, Batch: 1301 / 7013, Loss: 0.168, Accuracy: 0.954, F1: 0.767\n",
      "TRAIN: Epoch: 6, Batch: 1401 / 7013, Loss: 0.088, Accuracy: 0.973, F1: 0.854\n",
      "TRAIN: Epoch: 6, Batch: 1501 / 7013, Loss: 0.093, Accuracy: 0.972, F1: 0.870\n",
      "TRAIN: Epoch: 6, Batch: 1601 / 7013, Loss: 0.126, Accuracy: 0.964, F1: 0.836\n",
      "TRAIN: Epoch: 6, Batch: 1701 / 7013, Loss: 0.092, Accuracy: 0.966, F1: 0.860\n",
      "TRAIN: Epoch: 6, Batch: 1801 / 7013, Loss: 0.084, Accuracy: 0.972, F1: 0.885\n",
      "TRAIN: Epoch: 6, Batch: 1901 / 7013, Loss: 0.204, Accuracy: 0.941, F1: 0.744\n",
      "TRAIN: Epoch: 6, Batch: 2001 / 7013, Loss: 0.190, Accuracy: 0.950, F1: 0.763\n",
      "TRAIN: Epoch: 6, Batch: 2101 / 7013, Loss: 0.199, Accuracy: 0.961, F1: 0.807\n",
      "TRAIN: Epoch: 6, Batch: 2201 / 7013, Loss: 0.203, Accuracy: 0.947, F1: 0.814\n",
      "TRAIN: Epoch: 6, Batch: 2301 / 7013, Loss: 0.170, Accuracy: 0.946, F1: 0.762\n",
      "TRAIN: Epoch: 6, Batch: 2401 / 7013, Loss: 0.188, Accuracy: 0.947, F1: 0.817\n",
      "TRAIN: Epoch: 6, Batch: 2501 / 7013, Loss: 0.144, Accuracy: 0.954, F1: 0.844\n",
      "TRAIN: Epoch: 6, Batch: 2601 / 7013, Loss: 0.156, Accuracy: 0.968, F1: 0.846\n",
      "TRAIN: Epoch: 6, Batch: 2701 / 7013, Loss: 0.113, Accuracy: 0.961, F1: 0.800\n",
      "TRAIN: Epoch: 6, Batch: 2801 / 7013, Loss: 0.141, Accuracy: 0.955, F1: 0.843\n",
      "TRAIN: Epoch: 6, Batch: 2901 / 7013, Loss: 0.057, Accuracy: 0.980, F1: 0.926\n",
      "TRAIN: Epoch: 6, Batch: 3001 / 7013, Loss: 0.165, Accuracy: 0.953, F1: 0.818\n",
      "TRAIN: Epoch: 6, Batch: 3101 / 7013, Loss: 0.085, Accuracy: 0.976, F1: 0.902\n",
      "TRAIN: Epoch: 6, Batch: 3201 / 7013, Loss: 0.179, Accuracy: 0.942, F1: 0.764\n",
      "TRAIN: Epoch: 6, Batch: 3301 / 7013, Loss: 0.144, Accuracy: 0.952, F1: 0.805\n",
      "TRAIN: Epoch: 6, Batch: 3401 / 7013, Loss: 0.138, Accuracy: 0.964, F1: 0.840\n",
      "TRAIN: Epoch: 6, Batch: 3501 / 7013, Loss: 0.250, Accuracy: 0.944, F1: 0.788\n",
      "TRAIN: Epoch: 6, Batch: 3601 / 7013, Loss: 0.105, Accuracy: 0.964, F1: 0.858\n",
      "TRAIN: Epoch: 6, Batch: 3701 / 7013, Loss: 0.263, Accuracy: 0.923, F1: 0.770\n",
      "TRAIN: Epoch: 6, Batch: 3801 / 7013, Loss: 0.119, Accuracy: 0.967, F1: 0.859\n",
      "TRAIN: Epoch: 6, Batch: 3901 / 7013, Loss: 0.152, Accuracy: 0.953, F1: 0.779\n",
      "TRAIN: Epoch: 6, Batch: 4001 / 7013, Loss: 0.144, Accuracy: 0.961, F1: 0.799\n",
      "TRAIN: Epoch: 6, Batch: 4101 / 7013, Loss: 0.101, Accuracy: 0.968, F1: 0.871\n",
      "TRAIN: Epoch: 6, Batch: 4201 / 7013, Loss: 0.184, Accuracy: 0.954, F1: 0.788\n",
      "TRAIN: Epoch: 6, Batch: 4301 / 7013, Loss: 0.144, Accuracy: 0.957, F1: 0.803\n",
      "TRAIN: Epoch: 6, Batch: 4401 / 7013, Loss: 0.141, Accuracy: 0.958, F1: 0.793\n",
      "TRAIN: Epoch: 6, Batch: 4501 / 7013, Loss: 0.211, Accuracy: 0.947, F1: 0.758\n",
      "TRAIN: Epoch: 6, Batch: 4601 / 7013, Loss: 0.141, Accuracy: 0.949, F1: 0.801\n",
      "TRAIN: Epoch: 6, Batch: 4701 / 7013, Loss: 0.179, Accuracy: 0.940, F1: 0.775\n",
      "TRAIN: Epoch: 6, Batch: 4801 / 7013, Loss: 0.158, Accuracy: 0.952, F1: 0.802\n",
      "TRAIN: Epoch: 6, Batch: 4901 / 7013, Loss: 0.078, Accuracy: 0.975, F1: 0.894\n",
      "TRAIN: Epoch: 6, Batch: 5001 / 7013, Loss: 0.249, Accuracy: 0.938, F1: 0.748\n",
      "TRAIN: Epoch: 6, Batch: 5101 / 7013, Loss: 0.112, Accuracy: 0.971, F1: 0.883\n",
      "TRAIN: Epoch: 6, Batch: 5201 / 7013, Loss: 0.175, Accuracy: 0.942, F1: 0.773\n",
      "TRAIN: Epoch: 6, Batch: 5301 / 7013, Loss: 0.093, Accuracy: 0.974, F1: 0.868\n",
      "TRAIN: Epoch: 6, Batch: 5401 / 7013, Loss: 0.054, Accuracy: 0.983, F1: 0.920\n",
      "TRAIN: Epoch: 6, Batch: 5501 / 7013, Loss: 0.104, Accuracy: 0.968, F1: 0.842\n",
      "TRAIN: Epoch: 6, Batch: 5601 / 7013, Loss: 0.121, Accuracy: 0.964, F1: 0.867\n",
      "TRAIN: Epoch: 6, Batch: 5701 / 7013, Loss: 0.194, Accuracy: 0.944, F1: 0.769\n",
      "TRAIN: Epoch: 6, Batch: 5801 / 7013, Loss: 0.080, Accuracy: 0.980, F1: 0.909\n",
      "TRAIN: Epoch: 6, Batch: 5901 / 7013, Loss: 0.188, Accuracy: 0.948, F1: 0.791\n",
      "TRAIN: Epoch: 6, Batch: 6001 / 7013, Loss: 0.196, Accuracy: 0.955, F1: 0.824\n",
      "TRAIN: Epoch: 6, Batch: 6101 / 7013, Loss: 0.251, Accuracy: 0.923, F1: 0.678\n",
      "TRAIN: Epoch: 6, Batch: 6201 / 7013, Loss: 0.224, Accuracy: 0.952, F1: 0.796\n",
      "TRAIN: Epoch: 6, Batch: 6301 / 7013, Loss: 0.280, Accuracy: 0.938, F1: 0.765\n",
      "TRAIN: Epoch: 6, Batch: 6401 / 7013, Loss: 0.088, Accuracy: 0.972, F1: 0.878\n",
      "TRAIN: Epoch: 6, Batch: 6501 / 7013, Loss: 0.126, Accuracy: 0.960, F1: 0.831\n",
      "TRAIN: Epoch: 6, Batch: 6601 / 7013, Loss: 0.163, Accuracy: 0.956, F1: 0.798\n",
      "TRAIN: Epoch: 6, Batch: 6701 / 7013, Loss: 0.226, Accuracy: 0.938, F1: 0.767\n",
      "TRAIN: Epoch: 6, Batch: 6801 / 7013, Loss: 0.083, Accuracy: 0.974, F1: 0.885\n",
      "TRAIN: Epoch: 6, Batch: 6901 / 7013, Loss: 0.199, Accuracy: 0.951, F1: 0.730\n",
      "TRAIN: Epoch: 6, Batch: 7001 / 7013, Loss: 0.107, Accuracy: 0.966, F1: 0.854\n",
      "VAL: Epoch: 6, Batch: 1 / 367, Loss: 0.215, Accuracy: 0.941, F1: 0.785\n",
      "VAL: Epoch: 6, Batch: 101 / 367, Loss: 0.334, Accuracy: 0.948, F1: 0.763\n",
      "VAL: Epoch: 6, Batch: 201 / 367, Loss: 0.034, Accuracy: 0.991, F1: 0.966\n",
      "VAL: Epoch: 6, Batch: 301 / 367, Loss: 0.426, Accuracy: 0.926, F1: 0.754\n",
      "After epoch #6:\n",
      "Train loss: 0.161, Train Accuracy: 0.954, Train F1: 0.807\n",
      "Dev loss: 0.259, Dev Accuracy: 0.943, Dev F1: 0.782\n",
      "\n",
      "TRAIN: Epoch: 7, Batch: 1 / 7013, Loss: 0.272, Accuracy: 0.937, F1: 0.768\n",
      "TRAIN: Epoch: 7, Batch: 101 / 7013, Loss: 0.160, Accuracy: 0.955, F1: 0.797\n",
      "TRAIN: Epoch: 7, Batch: 201 / 7013, Loss: 0.145, Accuracy: 0.965, F1: 0.861\n",
      "TRAIN: Epoch: 7, Batch: 301 / 7013, Loss: 0.126, Accuracy: 0.968, F1: 0.857\n",
      "TRAIN: Epoch: 7, Batch: 401 / 7013, Loss: 0.096, Accuracy: 0.975, F1: 0.892\n",
      "TRAIN: Epoch: 7, Batch: 501 / 7013, Loss: 0.138, Accuracy: 0.958, F1: 0.861\n",
      "TRAIN: Epoch: 7, Batch: 601 / 7013, Loss: 0.080, Accuracy: 0.976, F1: 0.903\n",
      "TRAIN: Epoch: 7, Batch: 701 / 7013, Loss: 0.224, Accuracy: 0.940, F1: 0.781\n",
      "TRAIN: Epoch: 7, Batch: 801 / 7013, Loss: 0.171, Accuracy: 0.969, F1: 0.882\n",
      "TRAIN: Epoch: 7, Batch: 901 / 7013, Loss: 0.118, Accuracy: 0.965, F1: 0.887\n",
      "TRAIN: Epoch: 7, Batch: 1001 / 7013, Loss: 0.122, Accuracy: 0.965, F1: 0.808\n",
      "TRAIN: Epoch: 7, Batch: 1101 / 7013, Loss: 0.226, Accuracy: 0.934, F1: 0.733\n",
      "TRAIN: Epoch: 7, Batch: 1201 / 7013, Loss: 0.093, Accuracy: 0.969, F1: 0.870\n",
      "TRAIN: Epoch: 7, Batch: 1301 / 7013, Loss: 0.180, Accuracy: 0.969, F1: 0.879\n",
      "TRAIN: Epoch: 7, Batch: 1401 / 7013, Loss: 0.122, Accuracy: 0.962, F1: 0.859\n",
      "TRAIN: Epoch: 7, Batch: 1501 / 7013, Loss: 0.088, Accuracy: 0.968, F1: 0.897\n",
      "TRAIN: Epoch: 7, Batch: 1601 / 7013, Loss: 0.139, Accuracy: 0.959, F1: 0.833\n",
      "TRAIN: Epoch: 7, Batch: 1701 / 7013, Loss: 0.201, Accuracy: 0.949, F1: 0.763\n",
      "TRAIN: Epoch: 7, Batch: 1801 / 7013, Loss: 0.136, Accuracy: 0.954, F1: 0.858\n",
      "TRAIN: Epoch: 7, Batch: 1901 / 7013, Loss: 0.250, Accuracy: 0.951, F1: 0.797\n",
      "TRAIN: Epoch: 7, Batch: 2001 / 7013, Loss: 0.205, Accuracy: 0.943, F1: 0.786\n",
      "TRAIN: Epoch: 7, Batch: 2101 / 7013, Loss: 0.170, Accuracy: 0.951, F1: 0.828\n",
      "TRAIN: Epoch: 7, Batch: 2201 / 7013, Loss: 0.103, Accuracy: 0.967, F1: 0.861\n",
      "TRAIN: Epoch: 7, Batch: 2301 / 7013, Loss: 0.251, Accuracy: 0.911, F1: 0.710\n",
      "TRAIN: Epoch: 7, Batch: 2401 / 7013, Loss: 0.183, Accuracy: 0.951, F1: 0.792\n",
      "TRAIN: Epoch: 7, Batch: 2501 / 7013, Loss: 0.149, Accuracy: 0.963, F1: 0.829\n",
      "TRAIN: Epoch: 7, Batch: 2601 / 7013, Loss: 0.190, Accuracy: 0.951, F1: 0.796\n",
      "TRAIN: Epoch: 7, Batch: 2701 / 7013, Loss: 0.062, Accuracy: 0.982, F1: 0.927\n",
      "TRAIN: Epoch: 7, Batch: 2801 / 7013, Loss: 0.123, Accuracy: 0.963, F1: 0.842\n",
      "TRAIN: Epoch: 7, Batch: 2901 / 7013, Loss: 0.117, Accuracy: 0.964, F1: 0.849\n",
      "TRAIN: Epoch: 7, Batch: 3001 / 7013, Loss: 0.151, Accuracy: 0.958, F1: 0.814\n",
      "TRAIN: Epoch: 7, Batch: 3101 / 7013, Loss: 0.245, Accuracy: 0.929, F1: 0.720\n",
      "TRAIN: Epoch: 7, Batch: 3201 / 7013, Loss: 0.098, Accuracy: 0.966, F1: 0.837\n",
      "TRAIN: Epoch: 7, Batch: 3301 / 7013, Loss: 0.146, Accuracy: 0.958, F1: 0.817\n",
      "TRAIN: Epoch: 7, Batch: 3401 / 7013, Loss: 0.156, Accuracy: 0.954, F1: 0.786\n",
      "TRAIN: Epoch: 7, Batch: 3501 / 7013, Loss: 0.065, Accuracy: 0.981, F1: 0.925\n",
      "TRAIN: Epoch: 7, Batch: 3601 / 7013, Loss: 0.131, Accuracy: 0.965, F1: 0.834\n",
      "TRAIN: Epoch: 7, Batch: 3701 / 7013, Loss: 0.141, Accuracy: 0.962, F1: 0.791\n",
      "TRAIN: Epoch: 7, Batch: 3801 / 7013, Loss: 0.142, Accuracy: 0.966, F1: 0.820\n",
      "TRAIN: Epoch: 7, Batch: 3901 / 7013, Loss: 0.093, Accuracy: 0.973, F1: 0.847\n",
      "TRAIN: Epoch: 7, Batch: 4001 / 7013, Loss: 0.161, Accuracy: 0.957, F1: 0.794\n",
      "TRAIN: Epoch: 7, Batch: 4101 / 7013, Loss: 0.121, Accuracy: 0.967, F1: 0.894\n",
      "TRAIN: Epoch: 7, Batch: 4201 / 7013, Loss: 0.192, Accuracy: 0.960, F1: 0.871\n",
      "TRAIN: Epoch: 7, Batch: 4301 / 7013, Loss: 0.155, Accuracy: 0.952, F1: 0.826\n",
      "TRAIN: Epoch: 7, Batch: 4401 / 7013, Loss: 0.082, Accuracy: 0.970, F1: 0.891\n",
      "TRAIN: Epoch: 7, Batch: 4501 / 7013, Loss: 0.099, Accuracy: 0.964, F1: 0.846\n",
      "TRAIN: Epoch: 7, Batch: 4601 / 7013, Loss: 0.119, Accuracy: 0.960, F1: 0.808\n",
      "TRAIN: Epoch: 7, Batch: 4701 / 7013, Loss: 0.217, Accuracy: 0.944, F1: 0.740\n",
      "TRAIN: Epoch: 7, Batch: 4801 / 7013, Loss: 0.132, Accuracy: 0.959, F1: 0.854\n",
      "TRAIN: Epoch: 7, Batch: 4901 / 7013, Loss: 0.089, Accuracy: 0.970, F1: 0.859\n",
      "TRAIN: Epoch: 7, Batch: 5001 / 7013, Loss: 0.065, Accuracy: 0.979, F1: 0.900\n",
      "TRAIN: Epoch: 7, Batch: 5101 / 7013, Loss: 0.085, Accuracy: 0.973, F1: 0.897\n",
      "TRAIN: Epoch: 7, Batch: 5201 / 7013, Loss: 0.168, Accuracy: 0.962, F1: 0.819\n",
      "TRAIN: Epoch: 7, Batch: 5301 / 7013, Loss: 0.175, Accuracy: 0.954, F1: 0.835\n",
      "TRAIN: Epoch: 7, Batch: 5401 / 7013, Loss: 0.172, Accuracy: 0.947, F1: 0.793\n",
      "TRAIN: Epoch: 7, Batch: 5501 / 7013, Loss: 0.161, Accuracy: 0.957, F1: 0.836\n",
      "TRAIN: Epoch: 7, Batch: 5601 / 7013, Loss: 0.108, Accuracy: 0.967, F1: 0.879\n",
      "TRAIN: Epoch: 7, Batch: 5701 / 7013, Loss: 0.128, Accuracy: 0.975, F1: 0.896\n",
      "TRAIN: Epoch: 7, Batch: 5801 / 7013, Loss: 0.094, Accuracy: 0.973, F1: 0.864\n",
      "TRAIN: Epoch: 7, Batch: 5901 / 7013, Loss: 0.048, Accuracy: 0.985, F1: 0.945\n",
      "TRAIN: Epoch: 7, Batch: 6001 / 7013, Loss: 0.146, Accuracy: 0.954, F1: 0.797\n",
      "TRAIN: Epoch: 7, Batch: 6101 / 7013, Loss: 0.090, Accuracy: 0.967, F1: 0.893\n",
      "TRAIN: Epoch: 7, Batch: 6201 / 7013, Loss: 0.202, Accuracy: 0.941, F1: 0.789\n",
      "TRAIN: Epoch: 7, Batch: 6301 / 7013, Loss: 0.173, Accuracy: 0.962, F1: 0.814\n",
      "TRAIN: Epoch: 7, Batch: 6401 / 7013, Loss: 0.121, Accuracy: 0.962, F1: 0.846\n",
      "TRAIN: Epoch: 7, Batch: 6501 / 7013, Loss: 0.082, Accuracy: 0.970, F1: 0.919\n",
      "TRAIN: Epoch: 7, Batch: 6601 / 7013, Loss: 0.159, Accuracy: 0.950, F1: 0.799\n",
      "TRAIN: Epoch: 7, Batch: 6701 / 7013, Loss: 0.144, Accuracy: 0.959, F1: 0.849\n",
      "TRAIN: Epoch: 7, Batch: 6801 / 7013, Loss: 0.151, Accuracy: 0.966, F1: 0.884\n",
      "TRAIN: Epoch: 7, Batch: 6901 / 7013, Loss: 0.083, Accuracy: 0.971, F1: 0.879\n",
      "TRAIN: Epoch: 7, Batch: 7001 / 7013, Loss: 0.080, Accuracy: 0.973, F1: 0.899\n",
      "VAL: Epoch: 7, Batch: 1 / 367, Loss: 0.251, Accuracy: 0.941, F1: 0.798\n",
      "VAL: Epoch: 7, Batch: 101 / 367, Loss: 0.344, Accuracy: 0.953, F1: 0.789\n",
      "VAL: Epoch: 7, Batch: 201 / 367, Loss: 0.034, Accuracy: 0.990, F1: 0.962\n",
      "VAL: Epoch: 7, Batch: 301 / 367, Loss: 0.448, Accuracy: 0.935, F1: 0.793\n",
      "After epoch #7:\n",
      "Train loss: 0.141, Train Accuracy: 0.960, Train F1: 0.833\n",
      "Dev loss: 0.271, Dev Accuracy: 0.947, Dev F1: 0.795\n",
      "\n",
      "TRAIN: Epoch: 8, Batch: 1 / 7013, Loss: 0.131, Accuracy: 0.963, F1: 0.830\n",
      "TRAIN: Epoch: 8, Batch: 101 / 7013, Loss: 0.042, Accuracy: 0.985, F1: 0.944\n",
      "TRAIN: Epoch: 8, Batch: 201 / 7013, Loss: 0.071, Accuracy: 0.975, F1: 0.887\n",
      "TRAIN: Epoch: 8, Batch: 301 / 7013, Loss: 0.184, Accuracy: 0.942, F1: 0.826\n",
      "TRAIN: Epoch: 8, Batch: 401 / 7013, Loss: 0.120, Accuracy: 0.957, F1: 0.810\n",
      "TRAIN: Epoch: 8, Batch: 501 / 7013, Loss: 0.083, Accuracy: 0.976, F1: 0.913\n",
      "TRAIN: Epoch: 8, Batch: 601 / 7013, Loss: 0.090, Accuracy: 0.968, F1: 0.884\n",
      "TRAIN: Epoch: 8, Batch: 701 / 7013, Loss: 0.079, Accuracy: 0.977, F1: 0.876\n",
      "TRAIN: Epoch: 8, Batch: 801 / 7013, Loss: 0.136, Accuracy: 0.955, F1: 0.822\n",
      "TRAIN: Epoch: 8, Batch: 901 / 7013, Loss: 0.119, Accuracy: 0.965, F1: 0.875\n",
      "TRAIN: Epoch: 8, Batch: 1001 / 7013, Loss: 0.073, Accuracy: 0.977, F1: 0.897\n",
      "TRAIN: Epoch: 8, Batch: 1101 / 7013, Loss: 0.134, Accuracy: 0.960, F1: 0.811\n",
      "TRAIN: Epoch: 8, Batch: 1201 / 7013, Loss: 0.219, Accuracy: 0.939, F1: 0.757\n",
      "TRAIN: Epoch: 8, Batch: 1301 / 7013, Loss: 0.090, Accuracy: 0.975, F1: 0.887\n",
      "TRAIN: Epoch: 8, Batch: 1401 / 7013, Loss: 0.088, Accuracy: 0.976, F1: 0.893\n",
      "TRAIN: Epoch: 8, Batch: 1501 / 7013, Loss: 0.165, Accuracy: 0.955, F1: 0.876\n",
      "TRAIN: Epoch: 8, Batch: 1601 / 7013, Loss: 0.164, Accuracy: 0.948, F1: 0.755\n",
      "TRAIN: Epoch: 8, Batch: 1701 / 7013, Loss: 0.100, Accuracy: 0.973, F1: 0.892\n",
      "TRAIN: Epoch: 8, Batch: 1801 / 7013, Loss: 0.292, Accuracy: 0.935, F1: 0.754\n",
      "TRAIN: Epoch: 8, Batch: 1901 / 7013, Loss: 0.121, Accuracy: 0.963, F1: 0.861\n",
      "TRAIN: Epoch: 8, Batch: 2001 / 7013, Loss: 0.085, Accuracy: 0.974, F1: 0.890\n",
      "TRAIN: Epoch: 8, Batch: 2101 / 7013, Loss: 0.134, Accuracy: 0.961, F1: 0.852\n",
      "TRAIN: Epoch: 8, Batch: 2201 / 7013, Loss: 0.174, Accuracy: 0.952, F1: 0.771\n",
      "TRAIN: Epoch: 8, Batch: 2301 / 7013, Loss: 0.101, Accuracy: 0.969, F1: 0.864\n",
      "TRAIN: Epoch: 8, Batch: 2401 / 7013, Loss: 0.228, Accuracy: 0.929, F1: 0.730\n",
      "TRAIN: Epoch: 8, Batch: 2501 / 7013, Loss: 0.157, Accuracy: 0.960, F1: 0.781\n",
      "TRAIN: Epoch: 8, Batch: 2601 / 7013, Loss: 0.090, Accuracy: 0.971, F1: 0.883\n",
      "TRAIN: Epoch: 8, Batch: 2701 / 7013, Loss: 0.204, Accuracy: 0.939, F1: 0.803\n",
      "TRAIN: Epoch: 8, Batch: 2801 / 7013, Loss: 0.134, Accuracy: 0.957, F1: 0.815\n",
      "TRAIN: Epoch: 8, Batch: 2901 / 7013, Loss: 0.182, Accuracy: 0.957, F1: 0.821\n",
      "TRAIN: Epoch: 8, Batch: 3001 / 7013, Loss: 0.049, Accuracy: 0.985, F1: 0.932\n",
      "TRAIN: Epoch: 8, Batch: 3101 / 7013, Loss: 0.272, Accuracy: 0.943, F1: 0.757\n",
      "TRAIN: Epoch: 8, Batch: 3201 / 7013, Loss: 0.109, Accuracy: 0.968, F1: 0.850\n",
      "TRAIN: Epoch: 8, Batch: 3301 / 7013, Loss: 0.104, Accuracy: 0.964, F1: 0.864\n",
      "TRAIN: Epoch: 8, Batch: 3401 / 7013, Loss: 0.139, Accuracy: 0.950, F1: 0.828\n",
      "TRAIN: Epoch: 8, Batch: 3501 / 7013, Loss: 0.049, Accuracy: 0.985, F1: 0.910\n",
      "TRAIN: Epoch: 8, Batch: 3601 / 7013, Loss: 0.109, Accuracy: 0.963, F1: 0.858\n",
      "TRAIN: Epoch: 8, Batch: 3701 / 7013, Loss: 0.039, Accuracy: 0.986, F1: 0.926\n",
      "TRAIN: Epoch: 8, Batch: 3801 / 7013, Loss: 0.135, Accuracy: 0.963, F1: 0.848\n",
      "TRAIN: Epoch: 8, Batch: 3901 / 7013, Loss: 0.081, Accuracy: 0.977, F1: 0.886\n",
      "TRAIN: Epoch: 8, Batch: 4001 / 7013, Loss: 0.143, Accuracy: 0.974, F1: 0.881\n",
      "TRAIN: Epoch: 8, Batch: 4101 / 7013, Loss: 0.067, Accuracy: 0.976, F1: 0.894\n",
      "TRAIN: Epoch: 8, Batch: 4201 / 7013, Loss: 0.054, Accuracy: 0.983, F1: 0.911\n",
      "TRAIN: Epoch: 8, Batch: 4301 / 7013, Loss: 0.141, Accuracy: 0.971, F1: 0.852\n",
      "TRAIN: Epoch: 8, Batch: 4401 / 7013, Loss: 0.088, Accuracy: 0.972, F1: 0.882\n",
      "TRAIN: Epoch: 8, Batch: 4501 / 7013, Loss: 0.165, Accuracy: 0.949, F1: 0.812\n",
      "TRAIN: Epoch: 8, Batch: 4601 / 7013, Loss: 0.102, Accuracy: 0.977, F1: 0.912\n",
      "TRAIN: Epoch: 8, Batch: 4701 / 7013, Loss: 0.150, Accuracy: 0.951, F1: 0.794\n",
      "TRAIN: Epoch: 8, Batch: 4801 / 7013, Loss: 0.205, Accuracy: 0.959, F1: 0.829\n",
      "TRAIN: Epoch: 8, Batch: 4901 / 7013, Loss: 0.120, Accuracy: 0.969, F1: 0.865\n",
      "TRAIN: Epoch: 8, Batch: 5001 / 7013, Loss: 0.094, Accuracy: 0.972, F1: 0.865\n",
      "TRAIN: Epoch: 8, Batch: 5101 / 7013, Loss: 0.110, Accuracy: 0.967, F1: 0.874\n",
      "TRAIN: Epoch: 8, Batch: 5201 / 7013, Loss: 0.152, Accuracy: 0.966, F1: 0.841\n",
      "TRAIN: Epoch: 8, Batch: 5301 / 7013, Loss: 0.127, Accuracy: 0.967, F1: 0.873\n",
      "TRAIN: Epoch: 8, Batch: 5401 / 7013, Loss: 0.114, Accuracy: 0.968, F1: 0.872\n",
      "TRAIN: Epoch: 8, Batch: 5501 / 7013, Loss: 0.172, Accuracy: 0.949, F1: 0.784\n",
      "TRAIN: Epoch: 8, Batch: 5601 / 7013, Loss: 0.090, Accuracy: 0.970, F1: 0.889\n",
      "TRAIN: Epoch: 8, Batch: 5701 / 7013, Loss: 0.385, Accuracy: 0.928, F1: 0.748\n",
      "TRAIN: Epoch: 8, Batch: 5801 / 7013, Loss: 0.131, Accuracy: 0.961, F1: 0.855\n",
      "TRAIN: Epoch: 8, Batch: 5901 / 7013, Loss: 0.130, Accuracy: 0.962, F1: 0.834\n",
      "TRAIN: Epoch: 8, Batch: 6001 / 7013, Loss: 0.112, Accuracy: 0.969, F1: 0.828\n",
      "TRAIN: Epoch: 8, Batch: 6101 / 7013, Loss: 0.140, Accuracy: 0.954, F1: 0.831\n",
      "TRAIN: Epoch: 8, Batch: 6201 / 7013, Loss: 0.129, Accuracy: 0.965, F1: 0.858\n",
      "TRAIN: Epoch: 8, Batch: 6301 / 7013, Loss: 0.081, Accuracy: 0.975, F1: 0.885\n",
      "TRAIN: Epoch: 8, Batch: 6401 / 7013, Loss: 0.106, Accuracy: 0.975, F1: 0.866\n",
      "TRAIN: Epoch: 8, Batch: 6501 / 7013, Loss: 0.097, Accuracy: 0.974, F1: 0.905\n",
      "TRAIN: Epoch: 8, Batch: 6601 / 7013, Loss: 0.062, Accuracy: 0.982, F1: 0.924\n",
      "TRAIN: Epoch: 8, Batch: 6701 / 7013, Loss: 0.087, Accuracy: 0.972, F1: 0.887\n",
      "TRAIN: Epoch: 8, Batch: 6801 / 7013, Loss: 0.089, Accuracy: 0.975, F1: 0.886\n",
      "TRAIN: Epoch: 8, Batch: 6901 / 7013, Loss: 0.104, Accuracy: 0.972, F1: 0.852\n",
      "TRAIN: Epoch: 8, Batch: 7001 / 7013, Loss: 0.168, Accuracy: 0.951, F1: 0.803\n",
      "VAL: Epoch: 8, Batch: 1 / 367, Loss: 0.241, Accuracy: 0.952, F1: 0.841\n",
      "VAL: Epoch: 8, Batch: 101 / 367, Loss: 0.409, Accuracy: 0.953, F1: 0.790\n",
      "VAL: Epoch: 8, Batch: 201 / 367, Loss: 0.033, Accuracy: 0.992, F1: 0.970\n",
      "VAL: Epoch: 8, Batch: 301 / 367, Loss: 0.350, Accuracy: 0.931, F1: 0.780\n",
      "After epoch #8:\n",
      "Train loss: 0.122, Train Accuracy: 0.965, Train F1: 0.855\n",
      "Dev loss: 0.277, Dev Accuracy: 0.949, Dev F1: 0.805\n",
      "\n",
      "TRAIN: Epoch: 9, Batch: 1 / 7013, Loss: 0.124, Accuracy: 0.964, F1: 0.833\n",
      "TRAIN: Epoch: 9, Batch: 101 / 7013, Loss: 0.107, Accuracy: 0.970, F1: 0.925\n",
      "TRAIN: Epoch: 9, Batch: 201 / 7013, Loss: 0.113, Accuracy: 0.969, F1: 0.878\n",
      "TRAIN: Epoch: 9, Batch: 301 / 7013, Loss: 0.135, Accuracy: 0.957, F1: 0.848\n",
      "TRAIN: Epoch: 9, Batch: 401 / 7013, Loss: 0.077, Accuracy: 0.972, F1: 0.921\n",
      "TRAIN: Epoch: 9, Batch: 501 / 7013, Loss: 0.098, Accuracy: 0.970, F1: 0.888\n",
      "TRAIN: Epoch: 9, Batch: 601 / 7013, Loss: 0.112, Accuracy: 0.967, F1: 0.830\n",
      "TRAIN: Epoch: 9, Batch: 701 / 7013, Loss: 0.145, Accuracy: 0.962, F1: 0.848\n",
      "TRAIN: Epoch: 9, Batch: 801 / 7013, Loss: 0.121, Accuracy: 0.964, F1: 0.854\n",
      "TRAIN: Epoch: 9, Batch: 901 / 7013, Loss: 0.159, Accuracy: 0.960, F1: 0.815\n",
      "TRAIN: Epoch: 9, Batch: 1001 / 7013, Loss: 0.063, Accuracy: 0.980, F1: 0.932\n",
      "TRAIN: Epoch: 9, Batch: 1101 / 7013, Loss: 0.107, Accuracy: 0.966, F1: 0.834\n",
      "TRAIN: Epoch: 9, Batch: 1201 / 7013, Loss: 0.172, Accuracy: 0.950, F1: 0.754\n",
      "TRAIN: Epoch: 9, Batch: 1301 / 7013, Loss: 0.086, Accuracy: 0.970, F1: 0.897\n",
      "TRAIN: Epoch: 9, Batch: 1401 / 7013, Loss: 0.129, Accuracy: 0.969, F1: 0.851\n",
      "TRAIN: Epoch: 9, Batch: 1501 / 7013, Loss: 0.128, Accuracy: 0.961, F1: 0.842\n",
      "TRAIN: Epoch: 9, Batch: 1601 / 7013, Loss: 0.049, Accuracy: 0.986, F1: 0.931\n",
      "TRAIN: Epoch: 9, Batch: 1701 / 7013, Loss: 0.119, Accuracy: 0.971, F1: 0.842\n",
      "TRAIN: Epoch: 9, Batch: 1801 / 7013, Loss: 0.086, Accuracy: 0.972, F1: 0.863\n",
      "TRAIN: Epoch: 9, Batch: 1901 / 7013, Loss: 0.177, Accuracy: 0.950, F1: 0.813\n",
      "TRAIN: Epoch: 9, Batch: 2001 / 7013, Loss: 0.153, Accuracy: 0.951, F1: 0.810\n",
      "TRAIN: Epoch: 9, Batch: 2101 / 7013, Loss: 0.147, Accuracy: 0.962, F1: 0.866\n",
      "TRAIN: Epoch: 9, Batch: 2201 / 7013, Loss: 0.099, Accuracy: 0.971, F1: 0.869\n",
      "TRAIN: Epoch: 9, Batch: 2301 / 7013, Loss: 0.098, Accuracy: 0.962, F1: 0.829\n",
      "TRAIN: Epoch: 9, Batch: 2401 / 7013, Loss: 0.094, Accuracy: 0.967, F1: 0.885\n",
      "TRAIN: Epoch: 9, Batch: 2501 / 7013, Loss: 0.169, Accuracy: 0.960, F1: 0.829\n",
      "TRAIN: Epoch: 9, Batch: 2601 / 7013, Loss: 0.112, Accuracy: 0.967, F1: 0.869\n",
      "TRAIN: Epoch: 9, Batch: 2701 / 7013, Loss: 0.124, Accuracy: 0.967, F1: 0.844\n",
      "TRAIN: Epoch: 9, Batch: 2801 / 7013, Loss: 0.147, Accuracy: 0.953, F1: 0.795\n",
      "TRAIN: Epoch: 9, Batch: 2901 / 7013, Loss: 0.184, Accuracy: 0.957, F1: 0.843\n",
      "TRAIN: Epoch: 9, Batch: 3001 / 7013, Loss: 0.084, Accuracy: 0.975, F1: 0.897\n",
      "TRAIN: Epoch: 9, Batch: 3101 / 7013, Loss: 0.039, Accuracy: 0.987, F1: 0.921\n",
      "TRAIN: Epoch: 9, Batch: 3201 / 7013, Loss: 0.058, Accuracy: 0.986, F1: 0.937\n",
      "TRAIN: Epoch: 9, Batch: 3301 / 7013, Loss: 0.174, Accuracy: 0.950, F1: 0.774\n",
      "TRAIN: Epoch: 9, Batch: 3401 / 7013, Loss: 0.313, Accuracy: 0.952, F1: 0.816\n",
      "TRAIN: Epoch: 9, Batch: 3501 / 7013, Loss: 0.115, Accuracy: 0.968, F1: 0.844\n",
      "TRAIN: Epoch: 9, Batch: 3601 / 7013, Loss: 0.116, Accuracy: 0.971, F1: 0.906\n",
      "TRAIN: Epoch: 9, Batch: 3701 / 7013, Loss: 0.076, Accuracy: 0.980, F1: 0.919\n",
      "TRAIN: Epoch: 9, Batch: 3801 / 7013, Loss: 0.128, Accuracy: 0.955, F1: 0.859\n",
      "TRAIN: Epoch: 9, Batch: 3901 / 7013, Loss: 0.164, Accuracy: 0.961, F1: 0.816\n",
      "TRAIN: Epoch: 9, Batch: 4001 / 7013, Loss: 0.065, Accuracy: 0.980, F1: 0.915\n",
      "TRAIN: Epoch: 9, Batch: 4101 / 7013, Loss: 0.082, Accuracy: 0.971, F1: 0.870\n",
      "TRAIN: Epoch: 9, Batch: 4201 / 7013, Loss: 0.114, Accuracy: 0.965, F1: 0.830\n",
      "TRAIN: Epoch: 9, Batch: 4301 / 7013, Loss: 0.119, Accuracy: 0.958, F1: 0.824\n",
      "TRAIN: Epoch: 9, Batch: 4401 / 7013, Loss: 0.083, Accuracy: 0.972, F1: 0.884\n",
      "TRAIN: Epoch: 9, Batch: 4501 / 7013, Loss: 0.093, Accuracy: 0.970, F1: 0.896\n",
      "TRAIN: Epoch: 9, Batch: 4601 / 7013, Loss: 0.184, Accuracy: 0.945, F1: 0.800\n",
      "TRAIN: Epoch: 9, Batch: 4701 / 7013, Loss: 0.179, Accuracy: 0.948, F1: 0.818\n",
      "TRAIN: Epoch: 9, Batch: 4801 / 7013, Loss: 0.099, Accuracy: 0.967, F1: 0.848\n",
      "TRAIN: Epoch: 9, Batch: 4901 / 7013, Loss: 0.114, Accuracy: 0.973, F1: 0.871\n",
      "TRAIN: Epoch: 9, Batch: 5001 / 7013, Loss: 0.114, Accuracy: 0.966, F1: 0.844\n",
      "TRAIN: Epoch: 9, Batch: 5101 / 7013, Loss: 0.113, Accuracy: 0.966, F1: 0.854\n",
      "TRAIN: Epoch: 9, Batch: 5201 / 7013, Loss: 0.146, Accuracy: 0.952, F1: 0.808\n",
      "TRAIN: Epoch: 9, Batch: 5301 / 7013, Loss: 0.054, Accuracy: 0.984, F1: 0.954\n",
      "TRAIN: Epoch: 9, Batch: 5401 / 7013, Loss: 0.149, Accuracy: 0.945, F1: 0.853\n",
      "TRAIN: Epoch: 9, Batch: 5501 / 7013, Loss: 0.123, Accuracy: 0.959, F1: 0.833\n",
      "TRAIN: Epoch: 9, Batch: 5601 / 7013, Loss: 0.115, Accuracy: 0.967, F1: 0.831\n",
      "TRAIN: Epoch: 9, Batch: 5701 / 7013, Loss: 0.076, Accuracy: 0.976, F1: 0.857\n",
      "TRAIN: Epoch: 9, Batch: 5801 / 7013, Loss: 0.106, Accuracy: 0.974, F1: 0.878\n",
      "TRAIN: Epoch: 9, Batch: 5901 / 7013, Loss: 0.153, Accuracy: 0.956, F1: 0.822\n",
      "TRAIN: Epoch: 9, Batch: 6001 / 7013, Loss: 0.139, Accuracy: 0.977, F1: 0.869\n",
      "TRAIN: Epoch: 9, Batch: 6101 / 7013, Loss: 0.106, Accuracy: 0.968, F1: 0.852\n",
      "TRAIN: Epoch: 9, Batch: 6201 / 7013, Loss: 0.037, Accuracy: 0.989, F1: 0.949\n",
      "TRAIN: Epoch: 9, Batch: 6301 / 7013, Loss: 0.081, Accuracy: 0.974, F1: 0.879\n",
      "TRAIN: Epoch: 9, Batch: 6401 / 7013, Loss: 0.087, Accuracy: 0.975, F1: 0.895\n",
      "TRAIN: Epoch: 9, Batch: 6501 / 7013, Loss: 0.153, Accuracy: 0.967, F1: 0.818\n",
      "TRAIN: Epoch: 9, Batch: 6601 / 7013, Loss: 0.058, Accuracy: 0.983, F1: 0.932\n",
      "TRAIN: Epoch: 9, Batch: 6701 / 7013, Loss: 0.182, Accuracy: 0.953, F1: 0.816\n",
      "TRAIN: Epoch: 9, Batch: 6801 / 7013, Loss: 0.145, Accuracy: 0.957, F1: 0.834\n",
      "TRAIN: Epoch: 9, Batch: 6901 / 7013, Loss: 0.102, Accuracy: 0.977, F1: 0.892\n",
      "TRAIN: Epoch: 9, Batch: 7001 / 7013, Loss: 0.159, Accuracy: 0.955, F1: 0.823\n",
      "VAL: Epoch: 9, Batch: 1 / 367, Loss: 0.206, Accuracy: 0.948, F1: 0.829\n",
      "VAL: Epoch: 9, Batch: 101 / 367, Loss: 0.449, Accuracy: 0.945, F1: 0.765\n",
      "VAL: Epoch: 9, Batch: 201 / 367, Loss: 0.013, Accuracy: 0.996, F1: 0.991\n",
      "VAL: Epoch: 9, Batch: 301 / 367, Loss: 0.466, Accuracy: 0.924, F1: 0.762\n",
      "After epoch #9:\n",
      "Train loss: 0.109, Train Accuracy: 0.969, Train F1: 0.870\n",
      "Dev loss: 0.259, Dev Accuracy: 0.950, Dev F1: 0.808\n",
      "\n",
      "TRAIN: Epoch: 10, Batch: 1 / 7013, Loss: 0.203, Accuracy: 0.950, F1: 0.826\n",
      "TRAIN: Epoch: 10, Batch: 101 / 7013, Loss: 0.123, Accuracy: 0.960, F1: 0.877\n",
      "TRAIN: Epoch: 10, Batch: 201 / 7013, Loss: 0.093, Accuracy: 0.971, F1: 0.876\n",
      "TRAIN: Epoch: 10, Batch: 301 / 7013, Loss: 0.038, Accuracy: 0.990, F1: 0.954\n",
      "TRAIN: Epoch: 10, Batch: 401 / 7013, Loss: 0.094, Accuracy: 0.981, F1: 0.877\n",
      "TRAIN: Epoch: 10, Batch: 501 / 7013, Loss: 0.087, Accuracy: 0.981, F1: 0.933\n",
      "TRAIN: Epoch: 10, Batch: 601 / 7013, Loss: 0.283, Accuracy: 0.918, F1: 0.689\n",
      "TRAIN: Epoch: 10, Batch: 701 / 7013, Loss: 0.081, Accuracy: 0.970, F1: 0.901\n",
      "TRAIN: Epoch: 10, Batch: 801 / 7013, Loss: 0.056, Accuracy: 0.983, F1: 0.939\n",
      "TRAIN: Epoch: 10, Batch: 901 / 7013, Loss: 0.163, Accuracy: 0.965, F1: 0.865\n",
      "TRAIN: Epoch: 10, Batch: 1001 / 7013, Loss: 0.079, Accuracy: 0.975, F1: 0.914\n",
      "TRAIN: Epoch: 10, Batch: 1101 / 7013, Loss: 0.151, Accuracy: 0.963, F1: 0.857\n",
      "TRAIN: Epoch: 10, Batch: 1201 / 7013, Loss: 0.088, Accuracy: 0.974, F1: 0.910\n",
      "TRAIN: Epoch: 10, Batch: 1301 / 7013, Loss: 0.114, Accuracy: 0.965, F1: 0.868\n",
      "TRAIN: Epoch: 10, Batch: 1401 / 7013, Loss: 0.191, Accuracy: 0.947, F1: 0.774\n",
      "TRAIN: Epoch: 10, Batch: 1501 / 7013, Loss: 0.110, Accuracy: 0.966, F1: 0.839\n",
      "TRAIN: Epoch: 10, Batch: 1601 / 7013, Loss: 0.070, Accuracy: 0.979, F1: 0.943\n",
      "TRAIN: Epoch: 10, Batch: 1701 / 7013, Loss: 0.163, Accuracy: 0.955, F1: 0.795\n",
      "TRAIN: Epoch: 10, Batch: 1801 / 7013, Loss: 0.132, Accuracy: 0.967, F1: 0.876\n",
      "TRAIN: Epoch: 10, Batch: 1901 / 7013, Loss: 0.078, Accuracy: 0.980, F1: 0.908\n",
      "TRAIN: Epoch: 10, Batch: 2001 / 7013, Loss: 0.041, Accuracy: 0.986, F1: 0.925\n",
      "TRAIN: Epoch: 10, Batch: 2101 / 7013, Loss: 0.085, Accuracy: 0.978, F1: 0.893\n",
      "TRAIN: Epoch: 10, Batch: 2201 / 7013, Loss: 0.070, Accuracy: 0.979, F1: 0.943\n",
      "TRAIN: Epoch: 10, Batch: 2301 / 7013, Loss: 0.077, Accuracy: 0.971, F1: 0.880\n",
      "TRAIN: Epoch: 10, Batch: 2401 / 7013, Loss: 0.165, Accuracy: 0.960, F1: 0.836\n",
      "TRAIN: Epoch: 10, Batch: 2501 / 7013, Loss: 0.129, Accuracy: 0.975, F1: 0.906\n",
      "TRAIN: Epoch: 10, Batch: 2601 / 7013, Loss: 0.076, Accuracy: 0.976, F1: 0.907\n",
      "TRAIN: Epoch: 10, Batch: 2701 / 7013, Loss: 0.086, Accuracy: 0.969, F1: 0.904\n",
      "TRAIN: Epoch: 10, Batch: 2801 / 7013, Loss: 0.125, Accuracy: 0.969, F1: 0.863\n",
      "TRAIN: Epoch: 10, Batch: 2901 / 7013, Loss: 0.096, Accuracy: 0.965, F1: 0.847\n",
      "TRAIN: Epoch: 10, Batch: 3001 / 7013, Loss: 0.079, Accuracy: 0.980, F1: 0.924\n",
      "TRAIN: Epoch: 10, Batch: 3101 / 7013, Loss: 0.123, Accuracy: 0.974, F1: 0.901\n",
      "TRAIN: Epoch: 10, Batch: 3201 / 7013, Loss: 0.060, Accuracy: 0.980, F1: 0.915\n",
      "TRAIN: Epoch: 10, Batch: 3301 / 7013, Loss: 0.116, Accuracy: 0.962, F1: 0.875\n",
      "TRAIN: Epoch: 10, Batch: 3401 / 7013, Loss: 0.088, Accuracy: 0.970, F1: 0.842\n",
      "TRAIN: Epoch: 10, Batch: 3501 / 7013, Loss: 0.118, Accuracy: 0.962, F1: 0.857\n",
      "TRAIN: Epoch: 10, Batch: 3601 / 7013, Loss: 0.117, Accuracy: 0.978, F1: 0.897\n",
      "TRAIN: Epoch: 10, Batch: 3701 / 7013, Loss: 0.064, Accuracy: 0.978, F1: 0.884\n",
      "TRAIN: Epoch: 10, Batch: 3801 / 7013, Loss: 0.044, Accuracy: 0.983, F1: 0.943\n",
      "TRAIN: Epoch: 10, Batch: 3901 / 7013, Loss: 0.056, Accuracy: 0.987, F1: 0.951\n",
      "TRAIN: Epoch: 10, Batch: 4001 / 7013, Loss: 0.061, Accuracy: 0.985, F1: 0.952\n",
      "TRAIN: Epoch: 10, Batch: 4101 / 7013, Loss: 0.117, Accuracy: 0.969, F1: 0.872\n",
      "TRAIN: Epoch: 10, Batch: 4201 / 7013, Loss: 0.045, Accuracy: 0.987, F1: 0.944\n",
      "TRAIN: Epoch: 10, Batch: 4301 / 7013, Loss: 0.049, Accuracy: 0.982, F1: 0.905\n",
      "TRAIN: Epoch: 10, Batch: 4401 / 7013, Loss: 0.077, Accuracy: 0.973, F1: 0.898\n",
      "TRAIN: Epoch: 10, Batch: 4501 / 7013, Loss: 0.141, Accuracy: 0.957, F1: 0.823\n",
      "TRAIN: Epoch: 10, Batch: 4601 / 7013, Loss: 0.117, Accuracy: 0.966, F1: 0.877\n",
      "TRAIN: Epoch: 10, Batch: 4701 / 7013, Loss: 0.125, Accuracy: 0.964, F1: 0.819\n",
      "TRAIN: Epoch: 10, Batch: 4801 / 7013, Loss: 0.047, Accuracy: 0.985, F1: 0.922\n",
      "TRAIN: Epoch: 10, Batch: 4901 / 7013, Loss: 0.136, Accuracy: 0.963, F1: 0.844\n",
      "TRAIN: Epoch: 10, Batch: 5001 / 7013, Loss: 0.095, Accuracy: 0.969, F1: 0.864\n",
      "TRAIN: Epoch: 10, Batch: 5101 / 7013, Loss: 0.028, Accuracy: 0.991, F1: 0.953\n",
      "TRAIN: Epoch: 10, Batch: 5201 / 7013, Loss: 0.095, Accuracy: 0.968, F1: 0.892\n",
      "TRAIN: Epoch: 10, Batch: 5301 / 7013, Loss: 0.147, Accuracy: 0.954, F1: 0.793\n",
      "TRAIN: Epoch: 10, Batch: 5401 / 7013, Loss: 0.099, Accuracy: 0.970, F1: 0.878\n",
      "TRAIN: Epoch: 10, Batch: 5501 / 7013, Loss: 0.130, Accuracy: 0.954, F1: 0.846\n",
      "TRAIN: Epoch: 10, Batch: 5601 / 7013, Loss: 0.210, Accuracy: 0.951, F1: 0.785\n",
      "TRAIN: Epoch: 10, Batch: 5701 / 7013, Loss: 0.185, Accuracy: 0.960, F1: 0.821\n",
      "TRAIN: Epoch: 10, Batch: 5801 / 7013, Loss: 0.118, Accuracy: 0.968, F1: 0.859\n",
      "TRAIN: Epoch: 10, Batch: 5901 / 7013, Loss: 0.114, Accuracy: 0.971, F1: 0.848\n",
      "TRAIN: Epoch: 10, Batch: 6001 / 7013, Loss: 0.028, Accuracy: 0.994, F1: 0.968\n",
      "TRAIN: Epoch: 10, Batch: 6101 / 7013, Loss: 0.050, Accuracy: 0.982, F1: 0.924\n",
      "TRAIN: Epoch: 10, Batch: 6201 / 7013, Loss: 0.079, Accuracy: 0.977, F1: 0.891\n",
      "TRAIN: Epoch: 10, Batch: 6301 / 7013, Loss: 0.183, Accuracy: 0.969, F1: 0.874\n",
      "TRAIN: Epoch: 10, Batch: 6401 / 7013, Loss: 0.049, Accuracy: 0.987, F1: 0.935\n",
      "TRAIN: Epoch: 10, Batch: 6501 / 7013, Loss: 0.100, Accuracy: 0.967, F1: 0.857\n",
      "TRAIN: Epoch: 10, Batch: 6601 / 7013, Loss: 0.052, Accuracy: 0.987, F1: 0.925\n",
      "TRAIN: Epoch: 10, Batch: 6701 / 7013, Loss: 0.078, Accuracy: 0.977, F1: 0.889\n",
      "TRAIN: Epoch: 10, Batch: 6801 / 7013, Loss: 0.148, Accuracy: 0.955, F1: 0.846\n",
      "TRAIN: Epoch: 10, Batch: 6901 / 7013, Loss: 0.074, Accuracy: 0.974, F1: 0.873\n",
      "TRAIN: Epoch: 10, Batch: 7001 / 7013, Loss: 0.054, Accuracy: 0.985, F1: 0.923\n",
      "VAL: Epoch: 10, Batch: 1 / 367, Loss: 0.248, Accuracy: 0.944, F1: 0.813\n",
      "VAL: Epoch: 10, Batch: 101 / 367, Loss: 0.320, Accuracy: 0.964, F1: 0.828\n",
      "VAL: Epoch: 10, Batch: 201 / 367, Loss: 0.017, Accuracy: 0.995, F1: 0.988\n",
      "VAL: Epoch: 10, Batch: 301 / 367, Loss: 0.458, Accuracy: 0.922, F1: 0.759\n",
      "After epoch #10:\n",
      "Train loss: 0.097, Train Accuracy: 0.972, Train F1: 0.886\n",
      "Dev loss: 0.288, Dev Accuracy: 0.952, Dev F1: 0.816\n",
      "\n",
      "TRAIN: Epoch: 11, Batch: 1 / 7013, Loss: 0.104, Accuracy: 0.977, F1: 0.881\n",
      "TRAIN: Epoch: 11, Batch: 101 / 7013, Loss: 0.063, Accuracy: 0.978, F1: 0.925\n",
      "TRAIN: Epoch: 11, Batch: 201 / 7013, Loss: 0.035, Accuracy: 0.987, F1: 0.929\n",
      "TRAIN: Epoch: 11, Batch: 301 / 7013, Loss: 0.105, Accuracy: 0.965, F1: 0.863\n",
      "TRAIN: Epoch: 11, Batch: 401 / 7013, Loss: 0.096, Accuracy: 0.970, F1: 0.880\n",
      "TRAIN: Epoch: 11, Batch: 501 / 7013, Loss: 0.073, Accuracy: 0.979, F1: 0.923\n",
      "TRAIN: Epoch: 11, Batch: 601 / 7013, Loss: 0.079, Accuracy: 0.975, F1: 0.894\n",
      "TRAIN: Epoch: 11, Batch: 701 / 7013, Loss: 0.082, Accuracy: 0.970, F1: 0.879\n",
      "TRAIN: Epoch: 11, Batch: 801 / 7013, Loss: 0.024, Accuracy: 0.993, F1: 0.975\n",
      "TRAIN: Epoch: 11, Batch: 901 / 7013, Loss: 0.125, Accuracy: 0.963, F1: 0.844\n",
      "TRAIN: Epoch: 11, Batch: 1001 / 7013, Loss: 0.063, Accuracy: 0.983, F1: 0.920\n",
      "TRAIN: Epoch: 11, Batch: 1101 / 7013, Loss: 0.065, Accuracy: 0.980, F1: 0.933\n",
      "TRAIN: Epoch: 11, Batch: 1201 / 7013, Loss: 0.062, Accuracy: 0.980, F1: 0.933\n",
      "TRAIN: Epoch: 11, Batch: 1301 / 7013, Loss: 0.107, Accuracy: 0.969, F1: 0.877\n",
      "TRAIN: Epoch: 11, Batch: 1401 / 7013, Loss: 0.097, Accuracy: 0.976, F1: 0.889\n",
      "TRAIN: Epoch: 11, Batch: 1501 / 7013, Loss: 0.143, Accuracy: 0.958, F1: 0.869\n",
      "TRAIN: Epoch: 11, Batch: 1601 / 7013, Loss: 0.075, Accuracy: 0.979, F1: 0.913\n",
      "TRAIN: Epoch: 11, Batch: 1701 / 7013, Loss: 0.096, Accuracy: 0.968, F1: 0.858\n",
      "TRAIN: Epoch: 11, Batch: 1801 / 7013, Loss: 0.169, Accuracy: 0.954, F1: 0.839\n",
      "TRAIN: Epoch: 11, Batch: 1901 / 7013, Loss: 0.039, Accuracy: 0.989, F1: 0.953\n",
      "TRAIN: Epoch: 11, Batch: 2001 / 7013, Loss: 0.102, Accuracy: 0.969, F1: 0.908\n",
      "TRAIN: Epoch: 11, Batch: 2101 / 7013, Loss: 0.083, Accuracy: 0.975, F1: 0.900\n",
      "TRAIN: Epoch: 11, Batch: 2201 / 7013, Loss: 0.095, Accuracy: 0.969, F1: 0.866\n",
      "TRAIN: Epoch: 11, Batch: 2301 / 7013, Loss: 0.059, Accuracy: 0.986, F1: 0.938\n",
      "TRAIN: Epoch: 11, Batch: 2401 / 7013, Loss: 0.060, Accuracy: 0.984, F1: 0.944\n",
      "TRAIN: Epoch: 11, Batch: 2501 / 7013, Loss: 0.066, Accuracy: 0.980, F1: 0.912\n",
      "TRAIN: Epoch: 11, Batch: 2601 / 7013, Loss: 0.032, Accuracy: 0.991, F1: 0.951\n",
      "TRAIN: Epoch: 11, Batch: 2701 / 7013, Loss: 0.057, Accuracy: 0.985, F1: 0.934\n",
      "TRAIN: Epoch: 11, Batch: 2801 / 7013, Loss: 0.054, Accuracy: 0.978, F1: 0.939\n",
      "TRAIN: Epoch: 11, Batch: 2901 / 7013, Loss: 0.041, Accuracy: 0.984, F1: 0.956\n",
      "TRAIN: Epoch: 11, Batch: 3001 / 7013, Loss: 0.132, Accuracy: 0.968, F1: 0.855\n",
      "TRAIN: Epoch: 11, Batch: 3101 / 7013, Loss: 0.052, Accuracy: 0.988, F1: 0.942\n",
      "TRAIN: Epoch: 11, Batch: 3201 / 7013, Loss: 0.063, Accuracy: 0.979, F1: 0.905\n",
      "TRAIN: Epoch: 11, Batch: 3301 / 7013, Loss: 0.044, Accuracy: 0.986, F1: 0.946\n",
      "TRAIN: Epoch: 11, Batch: 3401 / 7013, Loss: 0.098, Accuracy: 0.972, F1: 0.894\n",
      "TRAIN: Epoch: 11, Batch: 3501 / 7013, Loss: 0.104, Accuracy: 0.973, F1: 0.885\n",
      "TRAIN: Epoch: 11, Batch: 3601 / 7013, Loss: 0.059, Accuracy: 0.979, F1: 0.934\n",
      "TRAIN: Epoch: 11, Batch: 3701 / 7013, Loss: 0.065, Accuracy: 0.979, F1: 0.914\n",
      "TRAIN: Epoch: 11, Batch: 3801 / 7013, Loss: 0.112, Accuracy: 0.975, F1: 0.906\n",
      "TRAIN: Epoch: 11, Batch: 3901 / 7013, Loss: 0.085, Accuracy: 0.971, F1: 0.903\n",
      "TRAIN: Epoch: 11, Batch: 4001 / 7013, Loss: 0.068, Accuracy: 0.978, F1: 0.904\n",
      "TRAIN: Epoch: 11, Batch: 4101 / 7013, Loss: 0.078, Accuracy: 0.975, F1: 0.919\n",
      "TRAIN: Epoch: 11, Batch: 4201 / 7013, Loss: 0.108, Accuracy: 0.979, F1: 0.916\n",
      "TRAIN: Epoch: 11, Batch: 4301 / 7013, Loss: 0.154, Accuracy: 0.961, F1: 0.876\n",
      "TRAIN: Epoch: 11, Batch: 4401 / 7013, Loss: 0.063, Accuracy: 0.981, F1: 0.923\n",
      "TRAIN: Epoch: 11, Batch: 4501 / 7013, Loss: 0.081, Accuracy: 0.978, F1: 0.888\n",
      "TRAIN: Epoch: 11, Batch: 4601 / 7013, Loss: 0.082, Accuracy: 0.977, F1: 0.885\n",
      "TRAIN: Epoch: 11, Batch: 4701 / 7013, Loss: 0.091, Accuracy: 0.976, F1: 0.905\n",
      "TRAIN: Epoch: 11, Batch: 4801 / 7013, Loss: 0.078, Accuracy: 0.980, F1: 0.904\n",
      "TRAIN: Epoch: 11, Batch: 4901 / 7013, Loss: 0.098, Accuracy: 0.968, F1: 0.874\n",
      "TRAIN: Epoch: 11, Batch: 5001 / 7013, Loss: 0.046, Accuracy: 0.986, F1: 0.922\n",
      "TRAIN: Epoch: 11, Batch: 5101 / 7013, Loss: 0.094, Accuracy: 0.969, F1: 0.885\n",
      "TRAIN: Epoch: 11, Batch: 5201 / 7013, Loss: 0.068, Accuracy: 0.980, F1: 0.955\n",
      "TRAIN: Epoch: 11, Batch: 5301 / 7013, Loss: 0.098, Accuracy: 0.967, F1: 0.846\n",
      "TRAIN: Epoch: 11, Batch: 5401 / 7013, Loss: 0.151, Accuracy: 0.957, F1: 0.836\n",
      "TRAIN: Epoch: 11, Batch: 5501 / 7013, Loss: 0.075, Accuracy: 0.973, F1: 0.882\n",
      "TRAIN: Epoch: 11, Batch: 5601 / 7013, Loss: 0.125, Accuracy: 0.969, F1: 0.843\n",
      "TRAIN: Epoch: 11, Batch: 5701 / 7013, Loss: 0.040, Accuracy: 0.987, F1: 0.954\n",
      "TRAIN: Epoch: 11, Batch: 5801 / 7013, Loss: 0.115, Accuracy: 0.974, F1: 0.871\n",
      "TRAIN: Epoch: 11, Batch: 5901 / 7013, Loss: 0.133, Accuracy: 0.968, F1: 0.883\n",
      "TRAIN: Epoch: 11, Batch: 6001 / 7013, Loss: 0.107, Accuracy: 0.971, F1: 0.900\n",
      "TRAIN: Epoch: 11, Batch: 6101 / 7013, Loss: 0.065, Accuracy: 0.983, F1: 0.915\n",
      "TRAIN: Epoch: 11, Batch: 6201 / 7013, Loss: 0.110, Accuracy: 0.961, F1: 0.880\n",
      "TRAIN: Epoch: 11, Batch: 6301 / 7013, Loss: 0.065, Accuracy: 0.983, F1: 0.930\n",
      "TRAIN: Epoch: 11, Batch: 6401 / 7013, Loss: 0.163, Accuracy: 0.965, F1: 0.866\n",
      "TRAIN: Epoch: 11, Batch: 6501 / 7013, Loss: 0.030, Accuracy: 0.992, F1: 0.982\n",
      "TRAIN: Epoch: 11, Batch: 6601 / 7013, Loss: 0.108, Accuracy: 0.969, F1: 0.876\n",
      "TRAIN: Epoch: 11, Batch: 6701 / 7013, Loss: 0.058, Accuracy: 0.984, F1: 0.936\n",
      "TRAIN: Epoch: 11, Batch: 6801 / 7013, Loss: 0.132, Accuracy: 0.963, F1: 0.839\n",
      "TRAIN: Epoch: 11, Batch: 6901 / 7013, Loss: 0.044, Accuracy: 0.989, F1: 0.954\n",
      "TRAIN: Epoch: 11, Batch: 7001 / 7013, Loss: 0.089, Accuracy: 0.973, F1: 0.872\n",
      "VAL: Epoch: 11, Batch: 1 / 367, Loss: 0.290, Accuracy: 0.953, F1: 0.845\n",
      "VAL: Epoch: 11, Batch: 101 / 367, Loss: 0.482, Accuracy: 0.951, F1: 0.789\n",
      "VAL: Epoch: 11, Batch: 201 / 367, Loss: 0.030, Accuracy: 0.991, F1: 0.983\n",
      "VAL: Epoch: 11, Batch: 301 / 367, Loss: 0.455, Accuracy: 0.921, F1: 0.755\n",
      "After epoch #11:\n",
      "Train loss: 0.087, Train Accuracy: 0.975, Train F1: 0.897\n",
      "Dev loss: 0.313, Dev Accuracy: 0.949, Dev F1: 0.809\n",
      "\n",
      "TRAIN: Epoch: 12, Batch: 1 / 7013, Loss: 0.061, Accuracy: 0.977, F1: 0.930\n",
      "TRAIN: Epoch: 12, Batch: 101 / 7013, Loss: 0.045, Accuracy: 0.985, F1: 0.942\n",
      "TRAIN: Epoch: 12, Batch: 201 / 7013, Loss: 0.030, Accuracy: 0.991, F1: 0.950\n",
      "TRAIN: Epoch: 12, Batch: 301 / 7013, Loss: 0.035, Accuracy: 0.990, F1: 0.967\n",
      "TRAIN: Epoch: 12, Batch: 401 / 7013, Loss: 0.035, Accuracy: 0.991, F1: 0.964\n",
      "TRAIN: Epoch: 12, Batch: 501 / 7013, Loss: 0.216, Accuracy: 0.956, F1: 0.823\n",
      "TRAIN: Epoch: 12, Batch: 601 / 7013, Loss: 0.064, Accuracy: 0.976, F1: 0.900\n",
      "TRAIN: Epoch: 12, Batch: 701 / 7013, Loss: 0.110, Accuracy: 0.957, F1: 0.821\n",
      "TRAIN: Epoch: 12, Batch: 801 / 7013, Loss: 0.045, Accuracy: 0.986, F1: 0.946\n",
      "TRAIN: Epoch: 12, Batch: 901 / 7013, Loss: 0.039, Accuracy: 0.987, F1: 0.953\n",
      "TRAIN: Epoch: 12, Batch: 1001 / 7013, Loss: 0.062, Accuracy: 0.980, F1: 0.922\n",
      "TRAIN: Epoch: 12, Batch: 1101 / 7013, Loss: 0.049, Accuracy: 0.986, F1: 0.940\n",
      "TRAIN: Epoch: 12, Batch: 1201 / 7013, Loss: 0.033, Accuracy: 0.993, F1: 0.965\n",
      "TRAIN: Epoch: 12, Batch: 1301 / 7013, Loss: 0.140, Accuracy: 0.970, F1: 0.859\n",
      "TRAIN: Epoch: 12, Batch: 1401 / 7013, Loss: 0.071, Accuracy: 0.979, F1: 0.905\n",
      "TRAIN: Epoch: 12, Batch: 1501 / 7013, Loss: 0.040, Accuracy: 0.988, F1: 0.964\n",
      "TRAIN: Epoch: 12, Batch: 1601 / 7013, Loss: 0.068, Accuracy: 0.980, F1: 0.925\n",
      "TRAIN: Epoch: 12, Batch: 1701 / 7013, Loss: 0.100, Accuracy: 0.966, F1: 0.858\n",
      "TRAIN: Epoch: 12, Batch: 1801 / 7013, Loss: 0.054, Accuracy: 0.987, F1: 0.937\n",
      "TRAIN: Epoch: 12, Batch: 1901 / 7013, Loss: 0.023, Accuracy: 0.993, F1: 0.966\n",
      "TRAIN: Epoch: 12, Batch: 2001 / 7013, Loss: 0.046, Accuracy: 0.984, F1: 0.926\n",
      "TRAIN: Epoch: 12, Batch: 2101 / 7013, Loss: 0.077, Accuracy: 0.974, F1: 0.886\n",
      "TRAIN: Epoch: 12, Batch: 2201 / 7013, Loss: 0.107, Accuracy: 0.965, F1: 0.834\n",
      "TRAIN: Epoch: 12, Batch: 2301 / 7013, Loss: 0.092, Accuracy: 0.966, F1: 0.902\n",
      "TRAIN: Epoch: 12, Batch: 2401 / 7013, Loss: 0.073, Accuracy: 0.973, F1: 0.905\n",
      "TRAIN: Epoch: 12, Batch: 2501 / 7013, Loss: 0.070, Accuracy: 0.978, F1: 0.925\n",
      "TRAIN: Epoch: 12, Batch: 2601 / 7013, Loss: 0.073, Accuracy: 0.974, F1: 0.907\n",
      "TRAIN: Epoch: 12, Batch: 2701 / 7013, Loss: 0.193, Accuracy: 0.956, F1: 0.832\n",
      "TRAIN: Epoch: 12, Batch: 2801 / 7013, Loss: 0.062, Accuracy: 0.981, F1: 0.940\n",
      "TRAIN: Epoch: 12, Batch: 2901 / 7013, Loss: 0.097, Accuracy: 0.969, F1: 0.889\n",
      "TRAIN: Epoch: 12, Batch: 3001 / 7013, Loss: 0.131, Accuracy: 0.960, F1: 0.854\n",
      "TRAIN: Epoch: 12, Batch: 3101 / 7013, Loss: 0.072, Accuracy: 0.978, F1: 0.926\n",
      "TRAIN: Epoch: 12, Batch: 3201 / 7013, Loss: 0.067, Accuracy: 0.977, F1: 0.903\n",
      "TRAIN: Epoch: 12, Batch: 3301 / 7013, Loss: 0.085, Accuracy: 0.977, F1: 0.913\n",
      "TRAIN: Epoch: 12, Batch: 3401 / 7013, Loss: 0.041, Accuracy: 0.988, F1: 0.947\n",
      "TRAIN: Epoch: 12, Batch: 3501 / 7013, Loss: 0.082, Accuracy: 0.971, F1: 0.882\n",
      "TRAIN: Epoch: 12, Batch: 3601 / 7013, Loss: 0.036, Accuracy: 0.986, F1: 0.934\n",
      "TRAIN: Epoch: 12, Batch: 3701 / 7013, Loss: 0.111, Accuracy: 0.973, F1: 0.905\n",
      "TRAIN: Epoch: 12, Batch: 3801 / 7013, Loss: 0.093, Accuracy: 0.975, F1: 0.887\n",
      "TRAIN: Epoch: 12, Batch: 3901 / 7013, Loss: 0.041, Accuracy: 0.988, F1: 0.922\n",
      "TRAIN: Epoch: 12, Batch: 4001 / 7013, Loss: 0.124, Accuracy: 0.964, F1: 0.858\n",
      "TRAIN: Epoch: 12, Batch: 4101 / 7013, Loss: 0.130, Accuracy: 0.976, F1: 0.886\n",
      "TRAIN: Epoch: 12, Batch: 4201 / 7013, Loss: 0.034, Accuracy: 0.990, F1: 0.962\n",
      "TRAIN: Epoch: 12, Batch: 4301 / 7013, Loss: 0.049, Accuracy: 0.985, F1: 0.924\n",
      "TRAIN: Epoch: 12, Batch: 4401 / 7013, Loss: 0.058, Accuracy: 0.983, F1: 0.955\n",
      "TRAIN: Epoch: 12, Batch: 4501 / 7013, Loss: 0.057, Accuracy: 0.980, F1: 0.941\n",
      "TRAIN: Epoch: 12, Batch: 4601 / 7013, Loss: 0.063, Accuracy: 0.979, F1: 0.931\n",
      "TRAIN: Epoch: 12, Batch: 4701 / 7013, Loss: 0.069, Accuracy: 0.983, F1: 0.932\n",
      "TRAIN: Epoch: 12, Batch: 4801 / 7013, Loss: 0.039, Accuracy: 0.985, F1: 0.940\n",
      "TRAIN: Epoch: 12, Batch: 4901 / 7013, Loss: 0.113, Accuracy: 0.976, F1: 0.868\n",
      "TRAIN: Epoch: 12, Batch: 5001 / 7013, Loss: 0.066, Accuracy: 0.977, F1: 0.910\n",
      "TRAIN: Epoch: 12, Batch: 5101 / 7013, Loss: 0.067, Accuracy: 0.980, F1: 0.896\n",
      "TRAIN: Epoch: 12, Batch: 5201 / 7013, Loss: 0.075, Accuracy: 0.971, F1: 0.902\n",
      "TRAIN: Epoch: 12, Batch: 5301 / 7013, Loss: 0.102, Accuracy: 0.966, F1: 0.876\n",
      "TRAIN: Epoch: 12, Batch: 5401 / 7013, Loss: 0.081, Accuracy: 0.978, F1: 0.925\n",
      "TRAIN: Epoch: 12, Batch: 5501 / 7013, Loss: 0.047, Accuracy: 0.985, F1: 0.934\n",
      "TRAIN: Epoch: 12, Batch: 5601 / 7013, Loss: 0.114, Accuracy: 0.975, F1: 0.896\n",
      "TRAIN: Epoch: 12, Batch: 5701 / 7013, Loss: 0.036, Accuracy: 0.990, F1: 0.954\n",
      "TRAIN: Epoch: 12, Batch: 5801 / 7013, Loss: 0.058, Accuracy: 0.978, F1: 0.906\n",
      "TRAIN: Epoch: 12, Batch: 5901 / 7013, Loss: 0.026, Accuracy: 0.993, F1: 0.974\n",
      "TRAIN: Epoch: 12, Batch: 6001 / 7013, Loss: 0.159, Accuracy: 0.961, F1: 0.877\n",
      "TRAIN: Epoch: 12, Batch: 6101 / 7013, Loss: 0.074, Accuracy: 0.979, F1: 0.918\n",
      "TRAIN: Epoch: 12, Batch: 6201 / 7013, Loss: 0.091, Accuracy: 0.970, F1: 0.891\n",
      "TRAIN: Epoch: 12, Batch: 6301 / 7013, Loss: 0.040, Accuracy: 0.990, F1: 0.942\n",
      "TRAIN: Epoch: 12, Batch: 6401 / 7013, Loss: 0.046, Accuracy: 0.983, F1: 0.939\n",
      "TRAIN: Epoch: 12, Batch: 6501 / 7013, Loss: 0.032, Accuracy: 0.991, F1: 0.965\n",
      "TRAIN: Epoch: 12, Batch: 6601 / 7013, Loss: 0.041, Accuracy: 0.987, F1: 0.942\n",
      "TRAIN: Epoch: 12, Batch: 6701 / 7013, Loss: 0.025, Accuracy: 0.994, F1: 0.976\n",
      "TRAIN: Epoch: 12, Batch: 6801 / 7013, Loss: 0.076, Accuracy: 0.977, F1: 0.892\n",
      "TRAIN: Epoch: 12, Batch: 6901 / 7013, Loss: 0.131, Accuracy: 0.974, F1: 0.872\n",
      "TRAIN: Epoch: 12, Batch: 7001 / 7013, Loss: 0.054, Accuracy: 0.981, F1: 0.922\n",
      "VAL: Epoch: 12, Batch: 1 / 367, Loss: 0.311, Accuracy: 0.949, F1: 0.822\n",
      "VAL: Epoch: 12, Batch: 101 / 367, Loss: 0.320, Accuracy: 0.968, F1: 0.850\n",
      "VAL: Epoch: 12, Batch: 201 / 367, Loss: 0.022, Accuracy: 0.994, F1: 0.978\n",
      "VAL: Epoch: 12, Batch: 301 / 367, Loss: 0.613, Accuracy: 0.925, F1: 0.773\n",
      "After epoch #12:\n",
      "Train loss: 0.079, Train Accuracy: 0.977, Train F1: 0.908\n",
      "Dev loss: 0.297, Dev Accuracy: 0.952, Dev F1: 0.816\n",
      "\n",
      "TRAIN: Epoch: 13, Batch: 1 / 7013, Loss: 0.035, Accuracy: 0.990, F1: 0.955\n",
      "TRAIN: Epoch: 13, Batch: 101 / 7013, Loss: 0.101, Accuracy: 0.974, F1: 0.915\n",
      "TRAIN: Epoch: 13, Batch: 201 / 7013, Loss: 0.087, Accuracy: 0.969, F1: 0.880\n",
      "TRAIN: Epoch: 13, Batch: 301 / 7013, Loss: 0.057, Accuracy: 0.981, F1: 0.917\n",
      "TRAIN: Epoch: 13, Batch: 401 / 7013, Loss: 0.062, Accuracy: 0.981, F1: 0.917\n",
      "TRAIN: Epoch: 13, Batch: 501 / 7013, Loss: 0.034, Accuracy: 0.990, F1: 0.959\n",
      "TRAIN: Epoch: 13, Batch: 601 / 7013, Loss: 0.073, Accuracy: 0.977, F1: 0.878\n",
      "TRAIN: Epoch: 13, Batch: 701 / 7013, Loss: 0.032, Accuracy: 0.988, F1: 0.956\n",
      "TRAIN: Epoch: 13, Batch: 801 / 7013, Loss: 0.049, Accuracy: 0.985, F1: 0.919\n",
      "TRAIN: Epoch: 13, Batch: 901 / 7013, Loss: 0.059, Accuracy: 0.982, F1: 0.936\n",
      "TRAIN: Epoch: 13, Batch: 1001 / 7013, Loss: 0.025, Accuracy: 0.993, F1: 0.972\n",
      "TRAIN: Epoch: 13, Batch: 1101 / 7013, Loss: 0.094, Accuracy: 0.970, F1: 0.886\n",
      "TRAIN: Epoch: 13, Batch: 1201 / 7013, Loss: 0.047, Accuracy: 0.991, F1: 0.937\n",
      "TRAIN: Epoch: 13, Batch: 1301 / 7013, Loss: 0.123, Accuracy: 0.958, F1: 0.825\n",
      "TRAIN: Epoch: 13, Batch: 1401 / 7013, Loss: 0.037, Accuracy: 0.987, F1: 0.940\n",
      "TRAIN: Epoch: 13, Batch: 1501 / 7013, Loss: 0.049, Accuracy: 0.986, F1: 0.945\n",
      "TRAIN: Epoch: 13, Batch: 1601 / 7013, Loss: 0.038, Accuracy: 0.987, F1: 0.925\n",
      "TRAIN: Epoch: 13, Batch: 1701 / 7013, Loss: 0.056, Accuracy: 0.984, F1: 0.926\n",
      "TRAIN: Epoch: 13, Batch: 1801 / 7013, Loss: 0.045, Accuracy: 0.985, F1: 0.928\n",
      "TRAIN: Epoch: 13, Batch: 1901 / 7013, Loss: 0.097, Accuracy: 0.968, F1: 0.879\n",
      "TRAIN: Epoch: 13, Batch: 2001 / 7013, Loss: 0.046, Accuracy: 0.988, F1: 0.937\n",
      "TRAIN: Epoch: 13, Batch: 2101 / 7013, Loss: 0.075, Accuracy: 0.980, F1: 0.899\n",
      "TRAIN: Epoch: 13, Batch: 2201 / 7013, Loss: 0.075, Accuracy: 0.973, F1: 0.913\n",
      "TRAIN: Epoch: 13, Batch: 2301 / 7013, Loss: 0.081, Accuracy: 0.983, F1: 0.919\n",
      "TRAIN: Epoch: 13, Batch: 2401 / 7013, Loss: 0.048, Accuracy: 0.988, F1: 0.947\n",
      "TRAIN: Epoch: 13, Batch: 2501 / 7013, Loss: 0.042, Accuracy: 0.986, F1: 0.930\n",
      "TRAIN: Epoch: 13, Batch: 2601 / 7013, Loss: 0.114, Accuracy: 0.962, F1: 0.888\n",
      "TRAIN: Epoch: 13, Batch: 2701 / 7013, Loss: 0.131, Accuracy: 0.962, F1: 0.830\n",
      "TRAIN: Epoch: 13, Batch: 2801 / 7013, Loss: 0.079, Accuracy: 0.977, F1: 0.937\n",
      "TRAIN: Epoch: 13, Batch: 2901 / 7013, Loss: 0.057, Accuracy: 0.985, F1: 0.960\n",
      "TRAIN: Epoch: 13, Batch: 3001 / 7013, Loss: 0.123, Accuracy: 0.959, F1: 0.796\n",
      "TRAIN: Epoch: 13, Batch: 3101 / 7013, Loss: 0.058, Accuracy: 0.983, F1: 0.947\n",
      "TRAIN: Epoch: 13, Batch: 3201 / 7013, Loss: 0.058, Accuracy: 0.986, F1: 0.929\n",
      "TRAIN: Epoch: 13, Batch: 3301 / 7013, Loss: 0.086, Accuracy: 0.976, F1: 0.868\n",
      "TRAIN: Epoch: 13, Batch: 3401 / 7013, Loss: 0.078, Accuracy: 0.979, F1: 0.909\n",
      "TRAIN: Epoch: 13, Batch: 3501 / 7013, Loss: 0.036, Accuracy: 0.987, F1: 0.958\n",
      "TRAIN: Epoch: 13, Batch: 3601 / 7013, Loss: 0.083, Accuracy: 0.979, F1: 0.915\n",
      "TRAIN: Epoch: 13, Batch: 3701 / 7013, Loss: 0.156, Accuracy: 0.964, F1: 0.854\n",
      "TRAIN: Epoch: 13, Batch: 3801 / 7013, Loss: 0.074, Accuracy: 0.976, F1: 0.906\n",
      "TRAIN: Epoch: 13, Batch: 3901 / 7013, Loss: 0.077, Accuracy: 0.980, F1: 0.896\n",
      "TRAIN: Epoch: 13, Batch: 4001 / 7013, Loss: 0.188, Accuracy: 0.952, F1: 0.803\n",
      "TRAIN: Epoch: 13, Batch: 4101 / 7013, Loss: 0.045, Accuracy: 0.986, F1: 0.934\n",
      "TRAIN: Epoch: 13, Batch: 4201 / 7013, Loss: 0.150, Accuracy: 0.962, F1: 0.849\n",
      "TRAIN: Epoch: 13, Batch: 4301 / 7013, Loss: 0.095, Accuracy: 0.967, F1: 0.883\n",
      "TRAIN: Epoch: 13, Batch: 4401 / 7013, Loss: 0.079, Accuracy: 0.978, F1: 0.916\n",
      "TRAIN: Epoch: 13, Batch: 4501 / 7013, Loss: 0.084, Accuracy: 0.970, F1: 0.913\n",
      "TRAIN: Epoch: 13, Batch: 4601 / 7013, Loss: 0.153, Accuracy: 0.960, F1: 0.861\n",
      "TRAIN: Epoch: 13, Batch: 4701 / 7013, Loss: 0.022, Accuracy: 0.994, F1: 0.977\n",
      "TRAIN: Epoch: 13, Batch: 4801 / 7013, Loss: 0.080, Accuracy: 0.975, F1: 0.904\n",
      "TRAIN: Epoch: 13, Batch: 4901 / 7013, Loss: 0.067, Accuracy: 0.977, F1: 0.893\n",
      "TRAIN: Epoch: 13, Batch: 5001 / 7013, Loss: 0.087, Accuracy: 0.976, F1: 0.902\n",
      "TRAIN: Epoch: 13, Batch: 5101 / 7013, Loss: 0.113, Accuracy: 0.978, F1: 0.904\n",
      "TRAIN: Epoch: 13, Batch: 5201 / 7013, Loss: 0.110, Accuracy: 0.982, F1: 0.908\n",
      "TRAIN: Epoch: 13, Batch: 5301 / 7013, Loss: 0.032, Accuracy: 0.989, F1: 0.965\n",
      "TRAIN: Epoch: 13, Batch: 5401 / 7013, Loss: 0.034, Accuracy: 0.988, F1: 0.965\n",
      "TRAIN: Epoch: 13, Batch: 5501 / 7013, Loss: 0.088, Accuracy: 0.969, F1: 0.861\n",
      "TRAIN: Epoch: 13, Batch: 5601 / 7013, Loss: 0.043, Accuracy: 0.987, F1: 0.945\n",
      "TRAIN: Epoch: 13, Batch: 5701 / 7013, Loss: 0.040, Accuracy: 0.987, F1: 0.935\n",
      "TRAIN: Epoch: 13, Batch: 5801 / 7013, Loss: 0.094, Accuracy: 0.980, F1: 0.924\n",
      "TRAIN: Epoch: 13, Batch: 5901 / 7013, Loss: 0.111, Accuracy: 0.967, F1: 0.860\n",
      "TRAIN: Epoch: 13, Batch: 6001 / 7013, Loss: 0.015, Accuracy: 0.998, F1: 0.987\n",
      "TRAIN: Epoch: 13, Batch: 6101 / 7013, Loss: 0.047, Accuracy: 0.986, F1: 0.934\n",
      "TRAIN: Epoch: 13, Batch: 6201 / 7013, Loss: 0.041, Accuracy: 0.988, F1: 0.944\n",
      "TRAIN: Epoch: 13, Batch: 6301 / 7013, Loss: 0.300, Accuracy: 0.948, F1: 0.798\n",
      "TRAIN: Epoch: 13, Batch: 6401 / 7013, Loss: 0.050, Accuracy: 0.987, F1: 0.936\n",
      "TRAIN: Epoch: 13, Batch: 6501 / 7013, Loss: 0.102, Accuracy: 0.964, F1: 0.872\n",
      "TRAIN: Epoch: 13, Batch: 6601 / 7013, Loss: 0.055, Accuracy: 0.978, F1: 0.901\n",
      "TRAIN: Epoch: 13, Batch: 6701 / 7013, Loss: 0.050, Accuracy: 0.988, F1: 0.937\n",
      "TRAIN: Epoch: 13, Batch: 6801 / 7013, Loss: 0.059, Accuracy: 0.978, F1: 0.913\n",
      "TRAIN: Epoch: 13, Batch: 6901 / 7013, Loss: 0.098, Accuracy: 0.964, F1: 0.844\n",
      "TRAIN: Epoch: 13, Batch: 7001 / 7013, Loss: 0.048, Accuracy: 0.983, F1: 0.921\n",
      "VAL: Epoch: 13, Batch: 1 / 367, Loss: 0.352, Accuracy: 0.946, F1: 0.815\n",
      "VAL: Epoch: 13, Batch: 101 / 367, Loss: 0.280, Accuracy: 0.970, F1: 0.865\n",
      "VAL: Epoch: 13, Batch: 201 / 367, Loss: 0.008, Accuracy: 0.997, F1: 0.990\n",
      "VAL: Epoch: 13, Batch: 301 / 367, Loss: 0.551, Accuracy: 0.918, F1: 0.740\n",
      "After epoch #13:\n",
      "Train loss: 0.073, Train Accuracy: 0.979, Train F1: 0.915\n",
      "Dev loss: 0.298, Dev Accuracy: 0.952, Dev F1: 0.815\n",
      "\n"
     ]
    }
   ],
   "source": [
    "last_epoch = 0\n",
    "dev_losses = []\n",
    "dev_f1s = []\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss, train_acc, train_f1 = _train(model, train_batches, optimizer, epoch)\n",
    "    dev_loss, dev_acc, dev_f1 = _val(model, dev_batches, epoch)\n",
    "\n",
    "    if len(dev_f1s) == 0 or dev_f1 >= dev_f1s[-1]:\n",
    "        model.save_pretrained(SAVE_PATH)\n",
    "\n",
    "    elif last_epoch == 0:\n",
    "        last_epoch = epoch + patience\n",
    "\n",
    "    print(f\"After epoch #{epoch}:\")\n",
    "    print(f\"Train loss: {train_loss:.3f}, Train Accuracy: {train_acc:.3f}, Train F1: {train_f1:.3f}\")\n",
    "    print(f\"Dev loss: {dev_loss:.3f}, Dev Accuracy: {dev_acc:.3f}, Dev F1: {dev_f1:.3f}\\n\")\n",
    "\n",
    "    dev_losses.append(dev_loss)\n",
    "    dev_f1s.append(dev_f1)\n",
    "    if epoch == last_epoch:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb55a65",
   "metadata": {},
   "source": [
    "# Decode Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "56296579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "866"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "723e0db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = XLMRobertaForMaskedLM.from_pretrained(SAVE_PATH, local_files_only=True)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8afadb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(trained_model=model, batches=dev_batches, tokenizer=tokenizer):\n",
    "    pred_labels = []\n",
    "\n",
    "    for item in pb(batches):\n",
    "        item = item[:, 0, :]\n",
    "        out = trained_model(item.to(device))\n",
    "        logits = out.logits\n",
    "        tokens_encoded = logits.argmax(axis=-1).tolist()\n",
    "        for enc in tokens_encoded:\n",
    "            decoded = tokenizer.decode(enc, skip_special_tokens=True)\n",
    "            pred_labels.append(decoded)\n",
    "        \n",
    "    return pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0cb4e625",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (367 of 367) |######################| Elapsed Time: 0:01:31 Time:  0:01:31\n"
     ]
    }
   ],
   "source": [
    "dev_x = [item[0] for item in dev_data]\n",
    "dev_y_true = [item[1] for item in dev_data]\n",
    "dev_y_pred = get_predictions(model, dev_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "957e339f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'настоящие лемуры почти исключительно травоядны : они питаются цветами, dvd, листьями, однако в неволе известны примеры питания насекомыми.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idd = 800*4 + 70 # 800*2 + 80    -- *495\n",
    "dev_y_pred[idd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ae4a358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'настоящие лемуры почти исключительно травоядны : они питаются цветами , dvd , листьями , однако в неволе известны примеры питания насекомыми .'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_y_true[idd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "551f4392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'настоящие лемуры почти исключительно травоядны : они питаются цветами , фрукт , листьями , однако в неволе известны примеры питания насекомыми .'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_x[idd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "35eb18d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2lang = {}\n",
    "\n",
    "for key, items in dev_texts.items():\n",
    "    for item in items:\n",
    "        sentence2lang[\" \".join(item)] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "efb43674",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_texts2labels = defaultdict(dict)\n",
    "\n",
    "for lang in dev_texts:\n",
    "    for text, labels in zip(dev_texts[lang], dev_labels[lang]):\n",
    "        dev_texts2labels[lang][\" \".join(text)] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5d9abf3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jesus': 'PER',\n",
       " 'nato': 'GRP',\n",
       " 'single': 'CW',\n",
       " 'france': 'LOC',\n",
       " 'bbc': 'CORP',\n",
       " 'stucco': 'PROD'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_label2label[\"EN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a055f656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pred_labels(input_sent, \n",
    "                      pred_sent, \n",
    "                      word_label2label=word_label2label, \n",
    "                      sentence2lang=sentence2lang,\n",
    "                      nltk_tokenizer=WhitespaceTokenizer()\n",
    "                     ):\n",
    "    \n",
    "    lang = sentence2lang[input_sent]\n",
    "    res, matched_spans = [], []\n",
    "\n",
    "    pred_tokens = word_tokenize(pred_sent)\n",
    "    input_tokens = input_sent.split()\n",
    "#     print(pred_tokens)\n",
    "#     print(input_tokens)\n",
    "#     print(len(pred_tokens), len(input_tokens))\n",
    "\n",
    "    i, j = 0, 0  \n",
    "    res = []\n",
    "    \n",
    "    while i < len(pred_tokens) and j < len(input_tokens):\n",
    "        \n",
    "        pred = pred_tokens[i]\n",
    "        inp = input_tokens[j]\n",
    "#         print(pred, i)\n",
    "#         print(inp, j)\n",
    "#         print(res)\n",
    "#         print()\n",
    "        if pred == inp and pred not in word_label2label[lang]:\n",
    "            res.append(\"O\")\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif pred in word_label2label[lang]:\n",
    "            res.append(word_label2label[lang][pred])\n",
    "            j += 1\n",
    "            while j < len(input_tokens) and i+1 < len(pred_tokens) and input_tokens[j] != pred_tokens[i+1]:\n",
    "#                 print(j, i)\n",
    "                res.append(word_label2label[lang][pred])\n",
    "                j += 1\n",
    "                \n",
    "            i += 1\n",
    "        elif pred != inp:\n",
    "            break\n",
    "#         input()\n",
    "\n",
    "    if len(res) < len(input_tokens):\n",
    "        res.extend([\"O\"] * (len(input_tokens) - len(res)))\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e819930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_iob(labels):\n",
    "    iob_labels = []\n",
    "    \n",
    "    if labels[0] != \"O\":\n",
    "        iob_labels.append(\"B-\" + labels[0])\n",
    "    else:\n",
    "        iob_labels.append(\"O\")\n",
    "    \n",
    "    for i, label in enumerate(labels[1:], 1):\n",
    "\n",
    "        if label == \"O\":\n",
    "            iob_labels.append(\"O\")\n",
    "            \n",
    "        elif labels[i-1] == \"O\" or (label != labels[i-1] and labels[i-1] != \"O\"):\n",
    "            iob_labels.append(\"B-\" + label)\n",
    "            \n",
    "        elif not labels[i-1].startswith(\"O\") and label != \"O\":\n",
    "            iob_labels.append(\"I-\" + label)\n",
    "        \n",
    "        \n",
    "    return iob_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6a74fdfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'LOC', 'O']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = \"파일 : nyk maritime museum01s3200 . jpg | 일본우선 요코하마지점 ( 나카구 )\"\n",
    "y = \"파일 : nyk maritime museum01s3200. jpg | 일본우선 요코하마지점 ( 대한민국\"\n",
    "\n",
    "build_pred_labels(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "40206cf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CORP', 'O', 'O', 'O', 'O']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_texts2labels['KO'][x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3d1e71d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predd = add_iob(build_pred_labels(dev_x[idd], dev_y_pred[idd]))\n",
    "truee = dev_texts2labels['RU'][dev_x[idd]]\n",
    "predd == truee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d8765845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PROD', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PROD', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(predd)\n",
    "print(truee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1f58968c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broken = 0\n",
    "dev_labels_pred = defaultdict(list)\n",
    "\n",
    "for x, y_pred in zip(dev_x, dev_y_pred):\n",
    "    lang = sentence2lang[x]\n",
    "    \n",
    "    true_labels = dev_texts2labels[lang][x]\n",
    "    try:\n",
    "        pred_labels = build_pred_labels(x, y_pred)\n",
    "    except:\n",
    "        broken += 1\n",
    "        pred_labels = [\"O\"] * len(true_labels)\n",
    "        \n",
    "    pred_labels = add_iob(pred_labels)\n",
    "    assert len(true_labels) == len(pred_labels)\n",
    "    \n",
    "    dev_labels_pred[lang].append((true_labels, pred_labels))\n",
    "broken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733f5679",
   "metadata": {},
   "source": [
    "# Calc metrics by span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "66c450f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Set\n",
    "from overrides import overrides\n",
    "\n",
    "from allennlp.training.metrics.metric import Metric\n",
    "\n",
    "\n",
    "class SpanF1(Metric):\n",
    "    def __init__(self, non_entity_labels=['O']) -> None:\n",
    "        self._num_gold_mentions = 0\n",
    "        self._num_recalled_mentions = 0\n",
    "        self._num_predicted_mentions = 0\n",
    "        self._TP, self._FP, self._GT = defaultdict(int), defaultdict(int), defaultdict(int)\n",
    "        self.non_entity_labels = set(non_entity_labels)\n",
    "\n",
    "    @overrides\n",
    "    def __call__(self, batched_predicted_spans, batched_gold_spans, sentences=None):\n",
    "        non_entity_labels = self.non_entity_labels\n",
    "        for predicted_spans, gold_spans in zip(batched_predicted_spans, batched_gold_spans):\n",
    "            gold_spans_set = set([x for x, y in gold_spans.items() if y not in non_entity_labels])\n",
    "            pred_spans_set = set([x for x, y in predicted_spans.items() if y not in non_entity_labels])\n",
    "\n",
    "            self._num_gold_mentions += len(gold_spans_set)\n",
    "            self._num_recalled_mentions += len(gold_spans_set & pred_spans_set)\n",
    "            self._num_predicted_mentions += len(pred_spans_set)\n",
    "\n",
    "            for ky, val in gold_spans.items():\n",
    "                if val not in non_entity_labels:\n",
    "                    self._GT[val] += 1\n",
    "\n",
    "            for ky, val in predicted_spans.items():\n",
    "                if val in non_entity_labels:\n",
    "                    continue\n",
    "                if ky in gold_spans and val == gold_spans[ky]:\n",
    "                    self._TP[val] += 1\n",
    "                else:\n",
    "                    self._FP[val] += 1\n",
    "\n",
    "    @overrides\n",
    "    def get_metric(self, reset: bool = False) -> float:\n",
    "        all_tags: Set[str] = set()\n",
    "        all_tags.update(self._TP.keys())\n",
    "        all_tags.update(self._FP.keys())\n",
    "        all_tags.update(self._GT.keys())\n",
    "        all_metrics = {}\n",
    "\n",
    "        for tag in all_tags:\n",
    "            precision, recall, f1_measure = self.compute_prf_metrics(true_positives=self._TP[tag],\n",
    "                                                                     false_negatives=self._GT[tag] - self._TP[tag],\n",
    "                                                                     false_positives=self._FP[tag])\n",
    "            all_metrics['P@{}'.format(tag)] = precision\n",
    "            all_metrics['R@{}'.format(tag)] = recall\n",
    "            all_metrics['F1@{}'.format(tag)] = f1_measure\n",
    "\n",
    "        # Compute the precision, recall and f1 for all spans jointly.\n",
    "        precision, recall, f1_measure = self.compute_prf_metrics(true_positives=sum(self._TP.values()),\n",
    "                                                                 false_positives=sum(self._FP.values()),\n",
    "                                                                 false_negatives=sum(self._GT.values())-sum(self._TP.values()))\n",
    "        all_metrics[\"micro@P\"] = precision\n",
    "        all_metrics[\"micro@R\"] = recall\n",
    "        all_metrics[\"micro@F1\"] = f1_measure\n",
    "\n",
    "        if self._num_gold_mentions == 0:\n",
    "            entity_recall = 0.0\n",
    "        else:\n",
    "            entity_recall = self._num_recalled_mentions / float(self._num_gold_mentions)\n",
    "\n",
    "        if self._num_predicted_mentions == 0:\n",
    "            entity_precision = 0.0\n",
    "        else:\n",
    "            entity_precision = self._num_recalled_mentions / float(self._num_predicted_mentions)\n",
    "\n",
    "        all_metrics['MD@R'] = entity_recall\n",
    "        all_metrics['MD@P'] = entity_precision\n",
    "        all_metrics['MD@F1'] = 2. * ((entity_precision * entity_recall) / (entity_precision + entity_recall + 1e-13))\n",
    "        all_metrics['ALLTRUE'] = self._num_gold_mentions\n",
    "        all_metrics['ALLRECALLED'] = self._num_recalled_mentions\n",
    "        all_metrics['ALLPRED'] = self._num_predicted_mentions\n",
    "        if reset:\n",
    "            self.reset()\n",
    "        return all_metrics\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_prf_metrics(true_positives: int, false_positives: int, false_negatives: int):\n",
    "        precision = float(true_positives) / float(true_positives + false_positives + 1e-13)\n",
    "        recall = float(true_positives) / float(true_positives + false_negatives + 1e-13)\n",
    "        f1_measure = 2. * ((precision * recall) / (precision + recall + 1e-13))\n",
    "        return precision, recall, f1_measure\n",
    "\n",
    "    @overrides\n",
    "    def reset(self):\n",
    "        self._num_gold_mentions = 0\n",
    "        self._num_recalled_mentions = 0\n",
    "        self._num_predicted_mentions = 0\n",
    "        self._TP.clear()\n",
    "        self._FP.clear()\n",
    "        self._GT.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "786432dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spans(labels):\n",
    "    fin_spans = []\n",
    "    for item_ in labels:\n",
    "\n",
    "        item = deepcopy(item_)\n",
    "        item.insert(0, \"O\")\n",
    "        item.append(\"O\")\n",
    "\n",
    "        new_spans = {}\n",
    "        for i, label in enumerate(item[1:-1], 1):\n",
    "\n",
    "            if item[i] == \"O\":\n",
    "                new_spans[(i-1, i-1)] = \"O\"\n",
    "            else:\n",
    "                if item[i-1] == 'O':\n",
    "                    start_i = i\n",
    "                if item[i+1] == 'O':\n",
    "                    new_spans[(start_i-1, i-1)] = item[i].split('-')[1]\n",
    "                    \n",
    "        fin_spans.append(new_spans)\n",
    "                \n",
    "    return fin_spans    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2fe25a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}\n",
    "\n",
    "for lang in [\"BN\", \"DE\", \"ES\", \"TR\", \"FA\", \"RU\", \"ZH\", \"NL\", \"KO\", \"EN\", \"HI\"]:\n",
    "    true_spans = get_spans([true for true, pred in dev_labels_pred[lang]])\n",
    "    pred_spans = get_spans([pred for true, pred in dev_labels_pred[lang]])\n",
    "    \n",
    "    span_f1 = SpanF1()\n",
    "    span_f1(pred_spans, true_spans)\n",
    "    cur_metric = span_f1.get_metric()\n",
    "    metrics[lang] = cur_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9a603785",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "df = pd.DataFrame(index=list(metrics[\"RU\"].keys()))\n",
    "\n",
    "for lang, metric in metrics.items():\n",
    "    df[lang] = list(metric.values())\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5705d470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BN</th>\n",
       "      <th>DE</th>\n",
       "      <th>ES</th>\n",
       "      <th>TR</th>\n",
       "      <th>FA</th>\n",
       "      <th>RU</th>\n",
       "      <th>ZH</th>\n",
       "      <th>NL</th>\n",
       "      <th>KO</th>\n",
       "      <th>EN</th>\n",
       "      <th>HI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P@CW</th>\n",
       "      <td>0.797</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.743</td>\n",
       "      <td>0.587</td>\n",
       "      <td>0.752</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.884</td>\n",
       "      <td>0.632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R@CW</th>\n",
       "      <td>0.653</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.709</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.693</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1@CW</th>\n",
       "      <td>0.718</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.763</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P@PER</th>\n",
       "      <td>0.802</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.647</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R@PER</th>\n",
       "      <td>0.619</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.742</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1@PER</th>\n",
       "      <td>0.699</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.734</td>\n",
       "      <td>0.694</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.798</td>\n",
       "      <td>0.681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P@GRP</th>\n",
       "      <td>0.738</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R@GRP</th>\n",
       "      <td>0.474</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.641</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.646</td>\n",
       "      <td>0.623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1@GRP</th>\n",
       "      <td>0.577</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.723</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.659</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P@PROD</th>\n",
       "      <td>0.741</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.877</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.523</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R@PROD</th>\n",
       "      <td>0.624</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.534</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.723</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.791</td>\n",
       "      <td>0.408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1@PROD</th>\n",
       "      <td>0.677</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.801</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.843</td>\n",
       "      <td>0.519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P@LOC</th>\n",
       "      <td>0.667</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.811</td>\n",
       "      <td>0.906</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R@LOC</th>\n",
       "      <td>0.367</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.801</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1@LOC</th>\n",
       "      <td>0.473</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.673</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P@CORP</th>\n",
       "      <td>0.815</td>\n",
       "      <td>0.720</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.858</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.899</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.867</td>\n",
       "      <td>0.779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R@CORP</th>\n",
       "      <td>0.591</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1@CORP</th>\n",
       "      <td>0.685</td>\n",
       "      <td>0.568</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.743</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro@P</th>\n",
       "      <td>0.765</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.836</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.711</td>\n",
       "      <td>0.854</td>\n",
       "      <td>0.739</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro@R</th>\n",
       "      <td>0.549</td>\n",
       "      <td>0.506</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.768</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro@F1</th>\n",
       "      <td>0.639</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.723</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MD@R</th>\n",
       "      <td>0.581</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.711</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MD@P</th>\n",
       "      <td>0.810</td>\n",
       "      <td>0.814</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.716</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.742</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MD@F1</th>\n",
       "      <td>0.677</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALLTRUE</th>\n",
       "      <td>800.000</td>\n",
       "      <td>1195.000</td>\n",
       "      <td>1166.000</td>\n",
       "      <td>1223.000</td>\n",
       "      <td>1142.000</td>\n",
       "      <td>1033.000</td>\n",
       "      <td>1251.000</td>\n",
       "      <td>1136.000</td>\n",
       "      <td>1170.000</td>\n",
       "      <td>1218.000</td>\n",
       "      <td>818.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALLRECALLED</th>\n",
       "      <td>465.000</td>\n",
       "      <td>637.000</td>\n",
       "      <td>829.000</td>\n",
       "      <td>826.000</td>\n",
       "      <td>581.000</td>\n",
       "      <td>679.000</td>\n",
       "      <td>509.000</td>\n",
       "      <td>890.000</td>\n",
       "      <td>703.000</td>\n",
       "      <td>937.000</td>\n",
       "      <td>468.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALLPRED</th>\n",
       "      <td>574.000</td>\n",
       "      <td>783.000</td>\n",
       "      <td>947.000</td>\n",
       "      <td>972.000</td>\n",
       "      <td>812.000</td>\n",
       "      <td>792.000</td>\n",
       "      <td>686.000</td>\n",
       "      <td>1021.000</td>\n",
       "      <td>927.000</td>\n",
       "      <td>1062.000</td>\n",
       "      <td>573.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 BN       DE       ES       TR       FA       RU       ZH  \\\n",
       "P@CW          0.797    0.776    0.885    0.827    0.787    0.743    0.587   \n",
       "R@CW          0.653    0.593    0.687    0.709    0.610    0.627    0.344   \n",
       "F1@CW         0.718    0.672    0.773    0.763    0.687    0.680    0.434   \n",
       "P@PER         0.802    0.802    0.779    0.903    0.769    0.860    0.647   \n",
       "R@PER         0.619    0.519    0.695    0.564    0.613    0.672    0.087   \n",
       "F1@PER        0.699    0.630    0.734    0.694    0.683    0.754    0.154   \n",
       "P@GRP         0.738    0.859    0.831    0.719    0.758    0.802    0.455   \n",
       "R@GRP         0.474    0.508    0.641    0.590    0.583    0.593    0.208   \n",
       "F1@GRP        0.577    0.638    0.723    0.648    0.659    0.682    0.286   \n",
       "P@PROD        0.741    0.820    0.922    0.853    0.686    0.877    0.704   \n",
       "R@PROD        0.624    0.477    0.699    0.754    0.534    0.667    0.365   \n",
       "F1@PROD       0.677    0.603    0.795    0.801    0.600    0.758    0.481   \n",
       "P@LOC         0.667    0.672    0.748    0.855    0.570    0.829    0.811   \n",
       "R@LOC         0.367    0.441    0.635    0.602    0.564    0.601    0.575   \n",
       "F1@LOC        0.473    0.532    0.687    0.707    0.567    0.697    0.673   \n",
       "P@CORP        0.815    0.720    0.885    0.858    0.000    0.899    0.701   \n",
       "R@CORP        0.591    0.470    0.777    0.660    0.000    0.682    0.360   \n",
       "F1@CORP       0.685    0.568    0.828    0.746    0.000    0.775    0.476   \n",
       "micro@P       0.765    0.773    0.846    0.836    0.701    0.833    0.711   \n",
       "micro@R       0.549    0.506    0.687    0.665    0.498    0.639    0.390   \n",
       "micro@F1      0.639    0.612    0.758    0.741    0.582    0.723    0.504   \n",
       "MD@R          0.581    0.533    0.711    0.675    0.509    0.657    0.407   \n",
       "MD@P          0.810    0.814    0.875    0.850    0.716    0.857    0.742   \n",
       "MD@F1         0.677    0.644    0.785    0.753    0.595    0.744    0.526   \n",
       "ALLTRUE     800.000 1195.000 1166.000 1223.000 1142.000 1033.000 1251.000   \n",
       "ALLRECALLED 465.000  637.000  829.000  826.000  581.000  679.000  509.000   \n",
       "ALLPRED     574.000  783.000  947.000  972.000  812.000  792.000  686.000   \n",
       "\n",
       "                  NL       KO       EN      HI  \n",
       "P@CW           0.752    0.805    0.884   0.632  \n",
       "R@CW           0.693    0.606    0.782   0.429  \n",
       "F1@CW          0.721    0.692    0.830   0.511  \n",
       "P@PER          0.923    0.805    0.864   0.788  \n",
       "R@PER          0.742    0.549    0.741   0.600  \n",
       "F1@PER         0.822    0.653    0.798   0.681  \n",
       "P@GRP          0.744    0.797    0.766   0.791  \n",
       "R@GRP          0.827    0.624    0.646   0.623  \n",
       "F1@GRP         0.784    0.700    0.701   0.697  \n",
       "P@PROD         0.868    0.523    0.901   0.711  \n",
       "R@PROD         0.723    0.443    0.791   0.408  \n",
       "F1@PROD        0.789    0.480    0.843   0.519  \n",
       "P@LOC          0.906    0.762    0.760   0.798  \n",
       "R@LOC          0.801    0.654    0.648   0.669  \n",
       "F1@LOC         0.850    0.704    0.699   0.728  \n",
       "P@CORP         0.929    0.820    0.867   0.779  \n",
       "R@CORP         0.802    0.680    0.781   0.458  \n",
       "F1@CORP        0.861    0.743    0.822   0.577  \n",
       "micro@P        0.854    0.739    0.850   0.756  \n",
       "micro@R        0.768    0.585    0.741   0.529  \n",
       "micro@F1       0.809    0.653    0.792   0.623  \n",
       "MD@R           0.783    0.601    0.769   0.572  \n",
       "MD@P           0.872    0.758    0.882   0.817  \n",
       "MD@F1          0.825    0.670    0.822   0.673  \n",
       "ALLTRUE     1136.000 1170.000 1218.000 818.000  \n",
       "ALLRECALLED  890.000  703.000  937.000 468.000  \n",
       "ALLPRED     1021.000  927.000 1062.000 573.000  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b899ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
