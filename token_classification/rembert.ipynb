{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b06cdca6",
   "metadata": {
    "cellId": "j9xqz31vrzi7dfo1t9rxmk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentencepiece in /home/jupyter/.local/lib/python3.8/site-packages (0.1.96)\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "%pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "978347e7",
   "metadata": {
    "cellId": "r4zmnb9g4t8krvc6iivxh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/jupyter/.local/lib/python3.8/site-packages (4.15.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: filelock in /home/jupyter/.local/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/jupyter/.local/lib/python3.8/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/jupyter/.local/lib/python3.8/site-packages (from transformers) (4.49.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /kernel/lib/python3.8/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: requests in /kernel/lib/python3.8/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /kernel/lib/python3.8/site-packages (from transformers) (1.19.4)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /kernel/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /kernel/lib/python3.8/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /kernel/lib/python3.8/site-packages (from requests->transformers) (1.26.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /kernel/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /kernel/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: six in /kernel/lib/python3.8/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (8.0.3)\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "%pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "e401d679",
   "metadata": {
    "cellId": "aqgm2hk3vw6xhtz55dav0l"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from transformers import RemBertTokenizer, RemBertForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "2195109a",
   "metadata": {
    "cellId": "k3sfxobfvtdewn3zixdkol"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "languages = ['bn', 'de', 'en', 'es', 'fa', 'hi', 'ko', 'nl', 'ru', 'tr', 'zh']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf66ce8",
   "metadata": {
    "cellId": "41whpaduzzqycfhtl50ven"
   },
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "77b4473c",
   "metadata": {
    "cellId": "efx3qbqwmnmgu6fgwhg58s"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "data_path = \"data/\"\n",
    "SAVE_PATH = \"model/\"\n",
    "\n",
    "max_len = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "1620fd94",
   "metadata": {
    "cellId": "u0vbp7le2iv553ifrmze"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def parse_conll(file):\n",
    "    texts, labels = [], []\n",
    "    new_texts, new_labels = [], []\n",
    "    \n",
    "    with open(file, 'r') as f:\n",
    "        for row in f:\n",
    "            row = row.replace('\\n', '')\n",
    "            if row.startswith(\"#\"):\n",
    "                new_texts, new_labels = [], []\n",
    "                continue\n",
    "\n",
    "            if row == \"\":\n",
    "                texts.append(new_texts)\n",
    "                labels.append(new_labels)\n",
    "\n",
    "            else:\n",
    "                parts = row.split()\n",
    "                new_texts.append(parts[0])\n",
    "                new_labels.append(parts[-1])\n",
    "\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "36e8ce4c",
   "metadata": {
    "cellId": "fo64w0tqavpqnaqvo8y0r8"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "train_texts, train_labels = [], []\n",
    "dev_texts, dev_labels = [], []\n",
    "\n",
    "for lang in languages:\n",
    "    train_path = data_path + lang.upper() + '/' + lang + '_train.conll'\n",
    "    dev_path = data_path + lang.upper() + '/' + lang + '_dev.conll'\n",
    "    train_t, train_l = parse_conll(train_path)\n",
    "    dev_t, dev_l = parse_conll(dev_path)\n",
    "    train_texts += train_t\n",
    "    train_labels += train_l\n",
    "    dev_texts += dev_t\n",
    "    dev_labels += dev_l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff621f6",
   "metadata": {
    "cellId": "8i36cbapbt4m7dcgtolxj"
   },
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "cf3a32ba",
   "metadata": {
    "cellId": "g3o9mkv16hokmrqakxv"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "506f8324",
   "metadata": {
    "cellId": "uaevpdgfl5qm053zv56an9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-CORP': 1,\n",
       " 'B-CW': 2,\n",
       " 'B-GRP': 3,\n",
       " 'B-LOC': 4,\n",
       " 'B-PER': 5,\n",
       " 'B-PROD': 6,\n",
       " 'I-CORP': 7,\n",
       " 'I-CW': 8,\n",
       " 'I-GRP': 9,\n",
       " 'I-LOC': 10,\n",
       " 'I-PER': 11,\n",
       " 'I-PROD': 12}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "O_TAG = 'O'\n",
    "B_TAG = 'B-'\n",
    "I_TAG = 'I-'\n",
    "\n",
    "uniq_labels = set()\n",
    "\n",
    "for item in train_labels:\n",
    "    uniq_labels.update(item)\n",
    "    \n",
    "uniq_labels = sorted(list(uniq_labels))\n",
    "uniq_labels.remove(O_TAG)\n",
    "uniq_labels.insert(0, O_TAG)\n",
    "    \n",
    "GLOBAL_LABEL2ID = {label: idx for idx, label in enumerate(uniq_labels)}\n",
    "GLOBAL_ID2LABEL = {idx: label for label, idx in GLOBAL_LABEL2ID.items()}\n",
    "GLOBAL_LABEL2ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "04529756",
   "metadata": {
    "cellId": "d81r2swjwjro7fyxhjwgq"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af69c1ad897d4e33ae9763b9f22eb7d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=4697711.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c4cfe660ca747c0977e3bff794a12e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=156.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad583d32a7e45e8ae682cf8c7dfff53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=263.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89ad0dd588d41cf978e44ba7efc595a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=8714136.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "tokenizer = RemBertTokenizer.from_pretrained('google/rembert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "2b79d060",
   "metadata": {
    "cellId": "08fcbo2fcynsa395zctmoch"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def pad_sequence(seq, max_len, pad_token):\n",
    "    if len(seq) >= max_len:\n",
    "        seq = seq[:max_len]\n",
    "    else:\n",
    "        seq = seq + [pad_token] * (max_len - len(seq))\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "dc04970a",
   "metadata": {
    "cellId": "aqg5iivvkiobx320v6et1t"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def process_tokens(words, labels):\n",
    "    bert_tokens, bio_labels = [tokenizer.cls_token], [O_TAG]\n",
    "    \n",
    "    for word, label in zip(words, labels):\n",
    "        tokens = tokenizer.tokenize(word)\n",
    "        bert_tokens.extend(tokens)\n",
    "        new_labels = [label] * len(tokens)\n",
    "        bio_labels.extend(new_labels)\n",
    "        \n",
    "    bert_tokens.append(tokenizer.sep_token)\n",
    "    bio_labels.append(O_TAG)\n",
    "    \n",
    "    for i, (token, label) in enumerate(zip(bert_tokens, bio_labels)):\n",
    "        if token.startswith(\"##\") and label.startswith(B_TAG):\n",
    "            bio_labels[i] = I_TAG + label[2:]\n",
    "\n",
    "    encoded_tokens = tokenizer.encode(bert_tokens, add_special_tokens=False)\n",
    "    \n",
    "    if len(bio_labels) >= max_len:\n",
    "        bio_labels[max_len-1] = O_TAG\n",
    "    \n",
    "    bio_labels = pad_sequence(bio_labels, max_len, O_TAG)\n",
    "    encoded_tokens = pad_sequence(encoded_tokens, max_len, tokenizer.pad_token_id)\n",
    "\n",
    "    return encoded_tokens, bio_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "fac4fc61",
   "metadata": {
    "cellId": "fq3m0sl5o3vx8kxhdqh88o"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def prepare_data_for_ner(texts, labels):\n",
    "    result = np.zeros((len(texts), max_len), dtype=np.int32)\n",
    "    fin_labels, fin_labels_encoded = [], []\n",
    "\n",
    "    for i, (text, label) in tqdm(enumerate(zip(texts, labels))):\n",
    "        \n",
    "        c_words, c_labels = process_tokens(text, label) \n",
    "        assert len(c_words) == len(c_labels)\n",
    "        \n",
    "        result[i] = c_words\n",
    "        fin_labels.append(c_labels)\n",
    "    \n",
    "    words_ids, labels_ids = [], []\n",
    "    \n",
    "    for sentence in fin_labels:\n",
    "        new_labels = []\n",
    "        for label in sentence:\n",
    "            new_labels.append(GLOBAL_LABEL2ID[label])\n",
    "        fin_labels_encoded.append(new_labels)\n",
    "    \n",
    "    return result, fin_labels_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "050ab8f0",
   "metadata": {
    "cellId": "nvo1ql877a8v2zu8zkrwti"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84484a9dfb1e49a8bfa7e5c2bc46f77c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022ff408301147aca94541c1ad8d856d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "train_words_enc, train_labels_enc = prepare_data_for_ner(train_texts, train_labels)\n",
    "dev_words_enc, dev_labels_enc = prepare_data_for_ner(dev_texts, dev_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1199bc",
   "metadata": {
    "cellId": "8e2layizte58ex0irzg1v"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "c265591a",
   "metadata": {
    "cellId": "63vm4mw6ldh9nk8097he3m"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d4becebf",
   "metadata": {
    "cellId": "mkt0guz3dgiisqvzxe75oe"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f35ee60ac954ecbb63b03096e732910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=2303882157.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/rembert were not used when initializing RemBertForTokenClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing RemBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RemBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RemBertForTokenClassification were not initialized from the model checkpoint at google/rembert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "model = RemBertForTokenClassification.from_pretrained('google/rembert', num_labels=len(GLOBAL_ID2LABEL))\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "b389e3d5",
   "metadata": {
    "cellId": "rfc75cyzold824mowiqqy9"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "\n",
    "batch_size = 32\n",
    "patience = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3e2a4c4e",
   "metadata": {
    "cellId": "g02rgvepdlhdyjgjb0jnkt"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "train_data = np.stack((train_words_enc, train_labels_enc), axis=1)\n",
    "dev_data = np.stack((dev_words_enc, dev_labels_enc), axis=1)\n",
    "\n",
    "train_batches = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "dev_batches = DataLoader(dev_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc70e58",
   "metadata": {
    "cellId": "swbhwgc66k6uv8nyoak1d"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d68d3f5b",
   "metadata": {
    "cellId": "eyo2ydbibxcxvpwnmmxcjp"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.2, patience=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7dbef709",
   "metadata": {
    "cellId": "kjdvlnm6pgei20pnzze9ps"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def _train(model, train_loader, optimizer, epoch_num):\n",
    "    train_loss, train_acc, train_f1 = [], [], []\n",
    "    model.train()\n",
    "\n",
    "    for batch_num, batch in enumerate(train_loader):\n",
    "        X_batch, y_batch = batch[:, 0, :], batch[:, 1, :]\n",
    "        X_batch = X_batch.type(torch.LongTensor).to(device)\n",
    "        y_batch = y_batch.type(torch.LongTensor).to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(input_ids=X_batch, labels=y_batch.contiguous(), return_dict=True)\n",
    "        loss = out.loss\n",
    "        y_pred = out.logits\n",
    "        y_pred = torch.argmax(y_pred, dim=2)\n",
    "\n",
    "        y_pred_flatten = torch.flatten(y_pred).cpu().numpy()\n",
    "        y_batch_flatten = torch.flatten(y_batch).cpu().numpy()\n",
    "        f1 = f1_score(y_batch_flatten, y_pred_flatten, average=\"micro\")\n",
    "        accuracy = accuracy_score(y_batch_flatten, y_pred_flatten)\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "        train_acc.append(accuracy)\n",
    "        train_f1.append(f1)\n",
    "        \n",
    "        X_batch = X_batch.to('cpu')\n",
    "        y_batch = y_batch.to('cpu')\n",
    "        \n",
    "        if batch_num % 50 == 0:\n",
    "            print(f\"TRAIN: Epoch: {epoch_num}, Batch: {batch_num + 1} / {len(train_loader)}, \"\n",
    "                          f\"Loss: {loss.item():.3f}, Accuracy: {accuracy:.3f}, F1: {f1:.3f}\")\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return np.mean(train_loss), np.mean(train_acc), np.mean(train_f1)\n",
    "\n",
    "def _val(model, val_loader, epoch_num):\n",
    "    val_loss, val_acc, val_f1 = [], [], []\n",
    "    model.eval()\n",
    "\n",
    "    for batch_num, batch in enumerate(val_loader):\n",
    "        X_batch, y_batch = batch[:, 0, :], batch[:, 1, :]\n",
    "        X_batch = X_batch.type(torch.LongTensor).to(device)\n",
    "        y_batch = y_batch.type(torch.LongTensor).to(device)\n",
    "\n",
    "        out = model(input_ids=X_batch, labels=y_batch.contiguous())\n",
    "        loss = out.loss\n",
    "        y_pred = out.logits\n",
    "        y_pred = torch.argmax(y_pred, dim=2)\n",
    "\n",
    "        y_pred_flatten = torch.flatten(y_pred).cpu().numpy()\n",
    "        y_batch_flatten = torch.flatten(y_batch).cpu().numpy()\n",
    "        f1 = f1_score(y_batch_flatten, y_pred_flatten, average=\"micro\")\n",
    "        accuracy = accuracy_score(y_batch_flatten, y_pred_flatten)\n",
    "\n",
    "        val_loss.append(loss.item())\n",
    "        val_acc.append(accuracy)\n",
    "        val_f1.append(f1)\n",
    "        \n",
    "        X_batch = X_batch.to('cpu')\n",
    "        y_batch = y_batch.to('cpu')\n",
    "        \n",
    "        if batch_num % 50 == 0:\n",
    "            print(f\"VAL: Epoch: {epoch_num}, Batch: {batch_num + 1} / {len(val_loader)}, \"\n",
    "                          f\"Loss: {loss.item():.3f}, Accuracy: {accuracy:.3f}, F1: {f1:.3f}\")\n",
    "\n",
    "    return np.mean(val_loss), np.mean(val_acc), np.mean(val_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5451dd1b",
   "metadata": {
    "cellId": "808y1idl06kfvvdtwvg09",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Epoch: 1, Batch: 1 / 5260, Loss: 2.723, Accuracy: 0.035, F1: 0.035\n",
      "TRAIN: Epoch: 1, Batch: 51 / 5260, Loss: 0.309, Accuracy: 0.912, F1: 0.912\n",
      "TRAIN: Epoch: 1, Batch: 101 / 5260, Loss: 0.220, Accuracy: 0.935, F1: 0.935\n",
      "TRAIN: Epoch: 1, Batch: 151 / 5260, Loss: 0.205, Accuracy: 0.930, F1: 0.930\n",
      "TRAIN: Epoch: 1, Batch: 201 / 5260, Loss: 0.123, Accuracy: 0.964, F1: 0.964\n",
      "TRAIN: Epoch: 1, Batch: 251 / 5260, Loss: 0.091, Accuracy: 0.969, F1: 0.969\n",
      "TRAIN: Epoch: 1, Batch: 301 / 5260, Loss: 0.154, Accuracy: 0.949, F1: 0.949\n",
      "TRAIN: Epoch: 1, Batch: 351 / 5260, Loss: 0.104, Accuracy: 0.969, F1: 0.969\n",
      "TRAIN: Epoch: 1, Batch: 401 / 5260, Loss: 0.141, Accuracy: 0.963, F1: 0.963\n",
      "TRAIN: Epoch: 1, Batch: 451 / 5260, Loss: 0.084, Accuracy: 0.974, F1: 0.974\n",
      "TRAIN: Epoch: 1, Batch: 501 / 5260, Loss: 0.118, Accuracy: 0.966, F1: 0.966\n",
      "TRAIN: Epoch: 1, Batch: 551 / 5260, Loss: 0.106, Accuracy: 0.960, F1: 0.960\n",
      "TRAIN: Epoch: 1, Batch: 601 / 5260, Loss: 0.116, Accuracy: 0.962, F1: 0.962\n",
      "TRAIN: Epoch: 1, Batch: 651 / 5260, Loss: 0.075, Accuracy: 0.977, F1: 0.977\n",
      "TRAIN: Epoch: 1, Batch: 701 / 5260, Loss: 0.156, Accuracy: 0.949, F1: 0.949\n",
      "TRAIN: Epoch: 1, Batch: 751 / 5260, Loss: 0.078, Accuracy: 0.972, F1: 0.972\n",
      "TRAIN: Epoch: 1, Batch: 801 / 5260, Loss: 0.180, Accuracy: 0.946, F1: 0.946\n",
      "TRAIN: Epoch: 1, Batch: 851 / 5260, Loss: 0.095, Accuracy: 0.965, F1: 0.965\n",
      "TRAIN: Epoch: 1, Batch: 901 / 5260, Loss: 0.061, Accuracy: 0.978, F1: 0.978\n",
      "TRAIN: Epoch: 1, Batch: 951 / 5260, Loss: 0.081, Accuracy: 0.970, F1: 0.970\n",
      "TRAIN: Epoch: 1, Batch: 1001 / 5260, Loss: 0.068, Accuracy: 0.980, F1: 0.980\n",
      "TRAIN: Epoch: 1, Batch: 1051 / 5260, Loss: 0.073, Accuracy: 0.976, F1: 0.976\n",
      "TRAIN: Epoch: 1, Batch: 1101 / 5260, Loss: 0.085, Accuracy: 0.974, F1: 0.974\n",
      "TRAIN: Epoch: 1, Batch: 1151 / 5260, Loss: 0.076, Accuracy: 0.975, F1: 0.975\n",
      "TRAIN: Epoch: 1, Batch: 1201 / 5260, Loss: 0.100, Accuracy: 0.973, F1: 0.973\n",
      "TRAIN: Epoch: 1, Batch: 1251 / 5260, Loss: 0.111, Accuracy: 0.971, F1: 0.971\n",
      "TRAIN: Epoch: 1, Batch: 1301 / 5260, Loss: 0.108, Accuracy: 0.961, F1: 0.961\n",
      "TRAIN: Epoch: 1, Batch: 1351 / 5260, Loss: 0.100, Accuracy: 0.971, F1: 0.971\n",
      "TRAIN: Epoch: 1, Batch: 1401 / 5260, Loss: 0.108, Accuracy: 0.973, F1: 0.973\n",
      "TRAIN: Epoch: 1, Batch: 1451 / 5260, Loss: 0.057, Accuracy: 0.982, F1: 0.982\n",
      "TRAIN: Epoch: 1, Batch: 1501 / 5260, Loss: 0.091, Accuracy: 0.976, F1: 0.976\n",
      "TRAIN: Epoch: 1, Batch: 1551 / 5260, Loss: 0.086, Accuracy: 0.975, F1: 0.975\n",
      "TRAIN: Epoch: 1, Batch: 1601 / 5260, Loss: 0.061, Accuracy: 0.981, F1: 0.981\n",
      "TRAIN: Epoch: 1, Batch: 1651 / 5260, Loss: 0.090, Accuracy: 0.973, F1: 0.973\n",
      "TRAIN: Epoch: 1, Batch: 1701 / 5260, Loss: 0.061, Accuracy: 0.978, F1: 0.978\n",
      "TRAIN: Epoch: 1, Batch: 1751 / 5260, Loss: 0.067, Accuracy: 0.975, F1: 0.975\n",
      "TRAIN: Epoch: 1, Batch: 1801 / 5260, Loss: 0.073, Accuracy: 0.979, F1: 0.979\n",
      "TRAIN: Epoch: 1, Batch: 1851 / 5260, Loss: 0.065, Accuracy: 0.976, F1: 0.976\n",
      "TRAIN: Epoch: 1, Batch: 1901 / 5260, Loss: 0.085, Accuracy: 0.965, F1: 0.965\n",
      "TRAIN: Epoch: 1, Batch: 1951 / 5260, Loss: 0.080, Accuracy: 0.975, F1: 0.975\n",
      "TRAIN: Epoch: 1, Batch: 2001 / 5260, Loss: 0.085, Accuracy: 0.974, F1: 0.974\n",
      "TRAIN: Epoch: 1, Batch: 2051 / 5260, Loss: 0.064, Accuracy: 0.983, F1: 0.983\n",
      "TRAIN: Epoch: 1, Batch: 2101 / 5260, Loss: 0.095, Accuracy: 0.973, F1: 0.973\n",
      "TRAIN: Epoch: 1, Batch: 2151 / 5260, Loss: 0.085, Accuracy: 0.974, F1: 0.974\n",
      "TRAIN: Epoch: 1, Batch: 2201 / 5260, Loss: 0.104, Accuracy: 0.972, F1: 0.972\n",
      "TRAIN: Epoch: 1, Batch: 2251 / 5260, Loss: 0.106, Accuracy: 0.960, F1: 0.960\n",
      "TRAIN: Epoch: 1, Batch: 2301 / 5260, Loss: 0.072, Accuracy: 0.978, F1: 0.978\n",
      "TRAIN: Epoch: 1, Batch: 2351 / 5260, Loss: 0.068, Accuracy: 0.983, F1: 0.983\n",
      "TRAIN: Epoch: 1, Batch: 2401 / 5260, Loss: 0.055, Accuracy: 0.980, F1: 0.980\n",
      "TRAIN: Epoch: 1, Batch: 2451 / 5260, Loss: 0.078, Accuracy: 0.970, F1: 0.970\n",
      "TRAIN: Epoch: 1, Batch: 2501 / 5260, Loss: 0.072, Accuracy: 0.980, F1: 0.980\n",
      "TRAIN: Epoch: 1, Batch: 2551 / 5260, Loss: 0.097, Accuracy: 0.963, F1: 0.963\n",
      "TRAIN: Epoch: 1, Batch: 2601 / 5260, Loss: 0.056, Accuracy: 0.983, F1: 0.983\n",
      "TRAIN: Epoch: 1, Batch: 2651 / 5260, Loss: 0.047, Accuracy: 0.987, F1: 0.987\n",
      "TRAIN: Epoch: 1, Batch: 2701 / 5260, Loss: 0.063, Accuracy: 0.976, F1: 0.976\n",
      "TRAIN: Epoch: 1, Batch: 2751 / 5260, Loss: 0.108, Accuracy: 0.973, F1: 0.973\n",
      "TRAIN: Epoch: 1, Batch: 2801 / 5260, Loss: 0.088, Accuracy: 0.973, F1: 0.973\n",
      "TRAIN: Epoch: 1, Batch: 2851 / 5260, Loss: 0.078, Accuracy: 0.976, F1: 0.976\n",
      "TRAIN: Epoch: 1, Batch: 2901 / 5260, Loss: 0.067, Accuracy: 0.978, F1: 0.978\n",
      "TRAIN: Epoch: 1, Batch: 2951 / 5260, Loss: 0.033, Accuracy: 0.985, F1: 0.985\n",
      "TRAIN: Epoch: 1, Batch: 3001 / 5260, Loss: 0.097, Accuracy: 0.967, F1: 0.967\n",
      "TRAIN: Epoch: 1, Batch: 3051 / 5260, Loss: 0.079, Accuracy: 0.967, F1: 0.967\n",
      "TRAIN: Epoch: 1, Batch: 3101 / 5260, Loss: 0.111, Accuracy: 0.962, F1: 0.962\n",
      "TRAIN: Epoch: 1, Batch: 3151 / 5260, Loss: 0.087, Accuracy: 0.976, F1: 0.976\n",
      "TRAIN: Epoch: 1, Batch: 3201 / 5260, Loss: 0.100, Accuracy: 0.971, F1: 0.971\n",
      "TRAIN: Epoch: 1, Batch: 3251 / 5260, Loss: 0.093, Accuracy: 0.973, F1: 0.973\n",
      "TRAIN: Epoch: 1, Batch: 3301 / 5260, Loss: 0.061, Accuracy: 0.979, F1: 0.979\n",
      "TRAIN: Epoch: 1, Batch: 3351 / 5260, Loss: 0.060, Accuracy: 0.982, F1: 0.982\n",
      "TRAIN: Epoch: 1, Batch: 3401 / 5260, Loss: 0.080, Accuracy: 0.976, F1: 0.976\n",
      "TRAIN: Epoch: 1, Batch: 3451 / 5260, Loss: 0.123, Accuracy: 0.959, F1: 0.959\n",
      "TRAIN: Epoch: 1, Batch: 3501 / 5260, Loss: 0.089, Accuracy: 0.969, F1: 0.969\n",
      "TRAIN: Epoch: 1, Batch: 3551 / 5260, Loss: 0.074, Accuracy: 0.977, F1: 0.977\n",
      "TRAIN: Epoch: 1, Batch: 3601 / 5260, Loss: 0.069, Accuracy: 0.975, F1: 0.975\n",
      "TRAIN: Epoch: 1, Batch: 3651 / 5260, Loss: 0.072, Accuracy: 0.974, F1: 0.974\n",
      "TRAIN: Epoch: 1, Batch: 3701 / 5260, Loss: 0.060, Accuracy: 0.987, F1: 0.987\n",
      "TRAIN: Epoch: 1, Batch: 3751 / 5260, Loss: 0.042, Accuracy: 0.983, F1: 0.983\n",
      "TRAIN: Epoch: 1, Batch: 3801 / 5260, Loss: 0.055, Accuracy: 0.982, F1: 0.982\n",
      "TRAIN: Epoch: 1, Batch: 3851 / 5260, Loss: 0.050, Accuracy: 0.983, F1: 0.983\n",
      "TRAIN: Epoch: 1, Batch: 3901 / 5260, Loss: 0.098, Accuracy: 0.970, F1: 0.970\n",
      "TRAIN: Epoch: 1, Batch: 3951 / 5260, Loss: 0.074, Accuracy: 0.979, F1: 0.979\n",
      "TRAIN: Epoch: 1, Batch: 4001 / 5260, Loss: 0.091, Accuracy: 0.973, F1: 0.973\n",
      "TRAIN: Epoch: 1, Batch: 4051 / 5260, Loss: 0.054, Accuracy: 0.978, F1: 0.978\n",
      "TRAIN: Epoch: 1, Batch: 4101 / 5260, Loss: 0.079, Accuracy: 0.981, F1: 0.981\n",
      "TRAIN: Epoch: 1, Batch: 4151 / 5260, Loss: 0.061, Accuracy: 0.985, F1: 0.985\n",
      "TRAIN: Epoch: 1, Batch: 4201 / 5260, Loss: 0.048, Accuracy: 0.987, F1: 0.987\n",
      "TRAIN: Epoch: 1, Batch: 4251 / 5260, Loss: 0.045, Accuracy: 0.989, F1: 0.989\n",
      "TRAIN: Epoch: 1, Batch: 4301 / 5260, Loss: 0.073, Accuracy: 0.978, F1: 0.978\n",
      "TRAIN: Epoch: 1, Batch: 4351 / 5260, Loss: 0.140, Accuracy: 0.964, F1: 0.964\n",
      "TRAIN: Epoch: 1, Batch: 4401 / 5260, Loss: 0.050, Accuracy: 0.982, F1: 0.982\n",
      "TRAIN: Epoch: 1, Batch: 4451 / 5260, Loss: 0.125, Accuracy: 0.964, F1: 0.964\n",
      "TRAIN: Epoch: 1, Batch: 4501 / 5260, Loss: 0.069, Accuracy: 0.976, F1: 0.976\n",
      "TRAIN: Epoch: 1, Batch: 4551 / 5260, Loss: 0.089, Accuracy: 0.970, F1: 0.970\n",
      "TRAIN: Epoch: 1, Batch: 4601 / 5260, Loss: 0.065, Accuracy: 0.978, F1: 0.978\n",
      "TRAIN: Epoch: 1, Batch: 4651 / 5260, Loss: 0.064, Accuracy: 0.978, F1: 0.978\n",
      "TRAIN: Epoch: 1, Batch: 4701 / 5260, Loss: 0.088, Accuracy: 0.971, F1: 0.971\n",
      "TRAIN: Epoch: 1, Batch: 4751 / 5260, Loss: 0.054, Accuracy: 0.981, F1: 0.981\n",
      "TRAIN: Epoch: 1, Batch: 4801 / 5260, Loss: 0.046, Accuracy: 0.982, F1: 0.982\n",
      "TRAIN: Epoch: 1, Batch: 4851 / 5260, Loss: 0.059, Accuracy: 0.983, F1: 0.983\n",
      "TRAIN: Epoch: 1, Batch: 4901 / 5260, Loss: 0.046, Accuracy: 0.990, F1: 0.990\n",
      "TRAIN: Epoch: 1, Batch: 4951 / 5260, Loss: 0.074, Accuracy: 0.972, F1: 0.972\n",
      "TRAIN: Epoch: 1, Batch: 5001 / 5260, Loss: 0.102, Accuracy: 0.972, F1: 0.972\n",
      "TRAIN: Epoch: 1, Batch: 5051 / 5260, Loss: 0.035, Accuracy: 0.987, F1: 0.987\n",
      "TRAIN: Epoch: 1, Batch: 5101 / 5260, Loss: 0.058, Accuracy: 0.979, F1: 0.979\n",
      "TRAIN: Epoch: 1, Batch: 5151 / 5260, Loss: 0.061, Accuracy: 0.980, F1: 0.980\n",
      "TRAIN: Epoch: 1, Batch: 5201 / 5260, Loss: 0.023, Accuracy: 0.992, F1: 0.992\n",
      "TRAIN: Epoch: 1, Batch: 5251 / 5260, Loss: 0.071, Accuracy: 0.974, F1: 0.974\n",
      "VAL: Epoch: 1, Batch: 1 / 275, Loss: 0.107, Accuracy: 0.966, F1: 0.966\n",
      "VAL: Epoch: 1, Batch: 51 / 275, Loss: 0.030, Accuracy: 0.989, F1: 0.989\n",
      "VAL: Epoch: 1, Batch: 101 / 275, Loss: 0.136, Accuracy: 0.953, F1: 0.953\n",
      "VAL: Epoch: 1, Batch: 151 / 275, Loss: 0.098, Accuracy: 0.967, F1: 0.967\n",
      "VAL: Epoch: 1, Batch: 201 / 275, Loss: 0.049, Accuracy: 0.985, F1: 0.985\n",
      "VAL: Epoch: 1, Batch: 251 / 275, Loss: 0.142, Accuracy: 0.959, F1: 0.959\n",
      "After epoch #1:\n",
      "Train loss: 0.088, Train Accuracy: 0.972, Train F1: 0.972\n",
      "Dev loss: 0.065, Dev Accuracy: 0.978, Dev F1: 0.978\n",
      "\n",
      "TRAIN: Epoch: 2, Batch: 1 / 5260, Loss: 0.088, Accuracy: 0.969, F1: 0.969\n",
      "TRAIN: Epoch: 2, Batch: 51 / 5260, Loss: 0.022, Accuracy: 0.991, F1: 0.991\n",
      "TRAIN: Epoch: 2, Batch: 101 / 5260, Loss: 0.054, Accuracy: 0.985, F1: 0.985\n",
      "TRAIN: Epoch: 2, Batch: 151 / 5260, Loss: 0.057, Accuracy: 0.984, F1: 0.984\n",
      "TRAIN: Epoch: 2, Batch: 201 / 5260, Loss: 0.028, Accuracy: 0.993, F1: 0.993\n",
      "TRAIN: Epoch: 2, Batch: 251 / 5260, Loss: 0.047, Accuracy: 0.990, F1: 0.990\n",
      "TRAIN: Epoch: 2, Batch: 301 / 5260, Loss: 0.023, Accuracy: 0.991, F1: 0.991\n",
      "TRAIN: Epoch: 2, Batch: 351 / 5260, Loss: 0.033, Accuracy: 0.986, F1: 0.986\n",
      "TRAIN: Epoch: 2, Batch: 401 / 5260, Loss: 0.064, Accuracy: 0.982, F1: 0.982\n",
      "TRAIN: Epoch: 2, Batch: 451 / 5260, Loss: 0.073, Accuracy: 0.982, F1: 0.982\n",
      "TRAIN: Epoch: 2, Batch: 501 / 5260, Loss: 0.040, Accuracy: 0.989, F1: 0.989\n",
      "TRAIN: Epoch: 2, Batch: 551 / 5260, Loss: 0.048, Accuracy: 0.987, F1: 0.987\n",
      "TRAIN: Epoch: 2, Batch: 601 / 5260, Loss: 0.055, Accuracy: 0.974, F1: 0.974\n",
      "TRAIN: Epoch: 2, Batch: 651 / 5260, Loss: 0.070, Accuracy: 0.977, F1: 0.977\n",
      "TRAIN: Epoch: 2, Batch: 701 / 5260, Loss: 0.068, Accuracy: 0.973, F1: 0.973\n",
      "TRAIN: Epoch: 2, Batch: 751 / 5260, Loss: 0.067, Accuracy: 0.982, F1: 0.982\n",
      "TRAIN: Epoch: 2, Batch: 801 / 5260, Loss: 0.037, Accuracy: 0.992, F1: 0.992\n",
      "TRAIN: Epoch: 2, Batch: 851 / 5260, Loss: 0.026, Accuracy: 0.993, F1: 0.993\n",
      "TRAIN: Epoch: 2, Batch: 901 / 5260, Loss: 0.031, Accuracy: 0.986, F1: 0.986\n",
      "TRAIN: Epoch: 2, Batch: 951 / 5260, Loss: 0.053, Accuracy: 0.984, F1: 0.984\n",
      "TRAIN: Epoch: 2, Batch: 1001 / 5260, Loss: 0.044, Accuracy: 0.986, F1: 0.986\n",
      "TRAIN: Epoch: 2, Batch: 1051 / 5260, Loss: 0.067, Accuracy: 0.977, F1: 0.977\n",
      "TRAIN: Epoch: 2, Batch: 1101 / 5260, Loss: 0.069, Accuracy: 0.970, F1: 0.970\n",
      "TRAIN: Epoch: 2, Batch: 1151 / 5260, Loss: 0.037, Accuracy: 0.987, F1: 0.987\n",
      "TRAIN: Epoch: 2, Batch: 1201 / 5260, Loss: 0.038, Accuracy: 0.983, F1: 0.983\n",
      "TRAIN: Epoch: 2, Batch: 1251 / 5260, Loss: 0.020, Accuracy: 0.994, F1: 0.994\n",
      "TRAIN: Epoch: 2, Batch: 1301 / 5260, Loss: 0.073, Accuracy: 0.979, F1: 0.979\n",
      "TRAIN: Epoch: 2, Batch: 1351 / 5260, Loss: 0.072, Accuracy: 0.976, F1: 0.976\n",
      "TRAIN: Epoch: 2, Batch: 1401 / 5260, Loss: 0.021, Accuracy: 0.994, F1: 0.994\n",
      "TRAIN: Epoch: 2, Batch: 1451 / 5260, Loss: 0.053, Accuracy: 0.982, F1: 0.982\n",
      "TRAIN: Epoch: 2, Batch: 1501 / 5260, Loss: 0.026, Accuracy: 0.993, F1: 0.993\n",
      "TRAIN: Epoch: 2, Batch: 1551 / 5260, Loss: 0.055, Accuracy: 0.977, F1: 0.977\n",
      "TRAIN: Epoch: 2, Batch: 1601 / 5260, Loss: 0.053, Accuracy: 0.987, F1: 0.987\n",
      "TRAIN: Epoch: 2, Batch: 1651 / 5260, Loss: 0.047, Accuracy: 0.984, F1: 0.984\n",
      "TRAIN: Epoch: 2, Batch: 1701 / 5260, Loss: 0.042, Accuracy: 0.993, F1: 0.993\n",
      "TRAIN: Epoch: 2, Batch: 1751 / 5260, Loss: 0.039, Accuracy: 0.984, F1: 0.984\n",
      "TRAIN: Epoch: 2, Batch: 1801 / 5260, Loss: 0.022, Accuracy: 0.991, F1: 0.991\n",
      "TRAIN: Epoch: 2, Batch: 1851 / 5260, Loss: 0.035, Accuracy: 0.988, F1: 0.988\n",
      "TRAIN: Epoch: 2, Batch: 1901 / 5260, Loss: 0.068, Accuracy: 0.978, F1: 0.978\n",
      "TRAIN: Epoch: 2, Batch: 1951 / 5260, Loss: 0.087, Accuracy: 0.975, F1: 0.975\n",
      "TRAIN: Epoch: 2, Batch: 2001 / 5260, Loss: 0.063, Accuracy: 0.981, F1: 0.981\n",
      "TRAIN: Epoch: 2, Batch: 2051 / 5260, Loss: 0.053, Accuracy: 0.983, F1: 0.983\n",
      "TRAIN: Epoch: 2, Batch: 2101 / 5260, Loss: 0.048, Accuracy: 0.984, F1: 0.984\n",
      "TRAIN: Epoch: 2, Batch: 2151 / 5260, Loss: 0.052, Accuracy: 0.980, F1: 0.980\n",
      "TRAIN: Epoch: 2, Batch: 2201 / 5260, Loss: 0.088, Accuracy: 0.970, F1: 0.970\n",
      "TRAIN: Epoch: 2, Batch: 2251 / 5260, Loss: 0.053, Accuracy: 0.979, F1: 0.979\n",
      "TRAIN: Epoch: 2, Batch: 2301 / 5260, Loss: 0.091, Accuracy: 0.976, F1: 0.976\n",
      "TRAIN: Epoch: 2, Batch: 2351 / 5260, Loss: 0.030, Accuracy: 0.990, F1: 0.990\n",
      "TRAIN: Epoch: 2, Batch: 2401 / 5260, Loss: 0.043, Accuracy: 0.981, F1: 0.981\n",
      "TRAIN: Epoch: 2, Batch: 2451 / 5260, Loss: 0.038, Accuracy: 0.988, F1: 0.988\n",
      "TRAIN: Epoch: 2, Batch: 2501 / 5260, Loss: 0.020, Accuracy: 0.996, F1: 0.996\n",
      "TRAIN: Epoch: 2, Batch: 2551 / 5260, Loss: 0.052, Accuracy: 0.980, F1: 0.980\n",
      "TRAIN: Epoch: 2, Batch: 2601 / 5260, Loss: 0.055, Accuracy: 0.985, F1: 0.985\n",
      "TRAIN: Epoch: 2, Batch: 2651 / 5260, Loss: 0.064, Accuracy: 0.981, F1: 0.981\n",
      "TRAIN: Epoch: 2, Batch: 2701 / 5260, Loss: 0.039, Accuracy: 0.984, F1: 0.984\n",
      "TRAIN: Epoch: 2, Batch: 2751 / 5260, Loss: 0.026, Accuracy: 0.990, F1: 0.990\n",
      "TRAIN: Epoch: 2, Batch: 2801 / 5260, Loss: 0.061, Accuracy: 0.983, F1: 0.983\n",
      "TRAIN: Epoch: 2, Batch: 2851 / 5260, Loss: 0.051, Accuracy: 0.986, F1: 0.986\n",
      "TRAIN: Epoch: 2, Batch: 2901 / 5260, Loss: 0.059, Accuracy: 0.985, F1: 0.985\n",
      "TRAIN: Epoch: 2, Batch: 2951 / 5260, Loss: 0.042, Accuracy: 0.985, F1: 0.985\n",
      "TRAIN: Epoch: 2, Batch: 3001 / 5260, Loss: 0.039, Accuracy: 0.986, F1: 0.986\n",
      "TRAIN: Epoch: 2, Batch: 3051 / 5260, Loss: 0.037, Accuracy: 0.993, F1: 0.993\n",
      "TRAIN: Epoch: 2, Batch: 3101 / 5260, Loss: 0.063, Accuracy: 0.978, F1: 0.978\n",
      "TRAIN: Epoch: 2, Batch: 3151 / 5260, Loss: 0.017, Accuracy: 0.995, F1: 0.995\n",
      "TRAIN: Epoch: 2, Batch: 3201 / 5260, Loss: 0.030, Accuracy: 0.986, F1: 0.986\n",
      "TRAIN: Epoch: 2, Batch: 3251 / 5260, Loss: 0.049, Accuracy: 0.984, F1: 0.984\n",
      "TRAIN: Epoch: 2, Batch: 3301 / 5260, Loss: 0.060, Accuracy: 0.979, F1: 0.979\n",
      "TRAIN: Epoch: 2, Batch: 3351 / 5260, Loss: 0.045, Accuracy: 0.982, F1: 0.982\n",
      "TRAIN: Epoch: 2, Batch: 3401 / 5260, Loss: 0.087, Accuracy: 0.979, F1: 0.979\n",
      "TRAIN: Epoch: 2, Batch: 3451 / 5260, Loss: 0.025, Accuracy: 0.994, F1: 0.994\n",
      "TRAIN: Epoch: 2, Batch: 3501 / 5260, Loss: 0.032, Accuracy: 0.988, F1: 0.988\n",
      "TRAIN: Epoch: 2, Batch: 3551 / 5260, Loss: 0.052, Accuracy: 0.983, F1: 0.983\n",
      "TRAIN: Epoch: 2, Batch: 3601 / 5260, Loss: 0.071, Accuracy: 0.974, F1: 0.974\n",
      "TRAIN: Epoch: 2, Batch: 3651 / 5260, Loss: 0.015, Accuracy: 0.996, F1: 0.996\n",
      "TRAIN: Epoch: 2, Batch: 3701 / 5260, Loss: 0.021, Accuracy: 0.990, F1: 0.990\n",
      "TRAIN: Epoch: 2, Batch: 3751 / 5260, Loss: 0.060, Accuracy: 0.982, F1: 0.982\n",
      "TRAIN: Epoch: 2, Batch: 3801 / 5260, Loss: 0.083, Accuracy: 0.978, F1: 0.978\n",
      "TRAIN: Epoch: 2, Batch: 3851 / 5260, Loss: 0.040, Accuracy: 0.987, F1: 0.987\n",
      "TRAIN: Epoch: 2, Batch: 3901 / 5260, Loss: 0.026, Accuracy: 0.989, F1: 0.989\n",
      "TRAIN: Epoch: 2, Batch: 3951 / 5260, Loss: 0.056, Accuracy: 0.981, F1: 0.981\n",
      "TRAIN: Epoch: 2, Batch: 4001 / 5260, Loss: 0.062, Accuracy: 0.979, F1: 0.979\n",
      "TRAIN: Epoch: 2, Batch: 4051 / 5260, Loss: 0.045, Accuracy: 0.985, F1: 0.985\n",
      "TRAIN: Epoch: 2, Batch: 4101 / 5260, Loss: 0.064, Accuracy: 0.979, F1: 0.979\n",
      "TRAIN: Epoch: 2, Batch: 4151 / 5260, Loss: 0.088, Accuracy: 0.959, F1: 0.959\n",
      "TRAIN: Epoch: 2, Batch: 4201 / 5260, Loss: 0.046, Accuracy: 0.982, F1: 0.982\n",
      "TRAIN: Epoch: 2, Batch: 4251 / 5260, Loss: 0.043, Accuracy: 0.984, F1: 0.984\n",
      "TRAIN: Epoch: 2, Batch: 4301 / 5260, Loss: 0.066, Accuracy: 0.983, F1: 0.983\n",
      "TRAIN: Epoch: 2, Batch: 4351 / 5260, Loss: 0.067, Accuracy: 0.979, F1: 0.979\n",
      "TRAIN: Epoch: 2, Batch: 4401 / 5260, Loss: 0.058, Accuracy: 0.977, F1: 0.977\n",
      "TRAIN: Epoch: 2, Batch: 4451 / 5260, Loss: 0.075, Accuracy: 0.977, F1: 0.977\n",
      "TRAIN: Epoch: 2, Batch: 4501 / 5260, Loss: 0.043, Accuracy: 0.984, F1: 0.984\n",
      "TRAIN: Epoch: 2, Batch: 4551 / 5260, Loss: 0.042, Accuracy: 0.989, F1: 0.989\n",
      "TRAIN: Epoch: 2, Batch: 4601 / 5260, Loss: 0.087, Accuracy: 0.972, F1: 0.972\n",
      "TRAIN: Epoch: 2, Batch: 4651 / 5260, Loss: 0.035, Accuracy: 0.990, F1: 0.990\n",
      "TRAIN: Epoch: 2, Batch: 4701 / 5260, Loss: 0.012, Accuracy: 0.995, F1: 0.995\n",
      "TRAIN: Epoch: 2, Batch: 4751 / 5260, Loss: 0.073, Accuracy: 0.972, F1: 0.972\n",
      "TRAIN: Epoch: 2, Batch: 4801 / 5260, Loss: 0.062, Accuracy: 0.978, F1: 0.978\n",
      "TRAIN: Epoch: 2, Batch: 4851 / 5260, Loss: 0.086, Accuracy: 0.967, F1: 0.967\n",
      "TRAIN: Epoch: 2, Batch: 4901 / 5260, Loss: 0.048, Accuracy: 0.984, F1: 0.984\n",
      "TRAIN: Epoch: 2, Batch: 4951 / 5260, Loss: 0.027, Accuracy: 0.988, F1: 0.988\n",
      "TRAIN: Epoch: 2, Batch: 5001 / 5260, Loss: 0.042, Accuracy: 0.987, F1: 0.987\n",
      "TRAIN: Epoch: 2, Batch: 5051 / 5260, Loss: 0.028, Accuracy: 0.985, F1: 0.985\n",
      "TRAIN: Epoch: 2, Batch: 5101 / 5260, Loss: 0.070, Accuracy: 0.979, F1: 0.979\n",
      "TRAIN: Epoch: 2, Batch: 5151 / 5260, Loss: 0.042, Accuracy: 0.986, F1: 0.986\n",
      "TRAIN: Epoch: 2, Batch: 5201 / 5260, Loss: 0.046, Accuracy: 0.987, F1: 0.987\n",
      "TRAIN: Epoch: 2, Batch: 5251 / 5260, Loss: 0.044, Accuracy: 0.986, F1: 0.986\n",
      "VAL: Epoch: 2, Batch: 1 / 275, Loss: 0.104, Accuracy: 0.967, F1: 0.967\n",
      "VAL: Epoch: 2, Batch: 51 / 275, Loss: 0.024, Accuracy: 0.991, F1: 0.991\n",
      "VAL: Epoch: 2, Batch: 101 / 275, Loss: 0.118, Accuracy: 0.956, F1: 0.956\n",
      "VAL: Epoch: 2, Batch: 151 / 275, Loss: 0.110, Accuracy: 0.959, F1: 0.959\n",
      "VAL: Epoch: 2, Batch: 201 / 275, Loss: 0.037, Accuracy: 0.989, F1: 0.989\n",
      "VAL: Epoch: 2, Batch: 251 / 275, Loss: 0.159, Accuracy: 0.956, F1: 0.956\n",
      "After epoch #2:\n",
      "Train loss: 0.046, Train Accuracy: 0.985, Train F1: 0.985\n",
      "Dev loss: 0.057, Dev Accuracy: 0.981, Dev F1: 0.981\n",
      "\n",
      "TRAIN: Epoch: 3, Batch: 1 / 5260, Loss: 0.018, Accuracy: 0.993, F1: 0.993\n",
      "TRAIN: Epoch: 3, Batch: 51 / 5260, Loss: 0.041, Accuracy: 0.988, F1: 0.988\n",
      "TRAIN: Epoch: 3, Batch: 101 / 5260, Loss: 0.031, Accuracy: 0.987, F1: 0.987\n",
      "TRAIN: Epoch: 3, Batch: 151 / 5260, Loss: 0.036, Accuracy: 0.988, F1: 0.988\n",
      "TRAIN: Epoch: 3, Batch: 201 / 5260, Loss: 0.052, Accuracy: 0.986, F1: 0.986\n",
      "TRAIN: Epoch: 3, Batch: 251 / 5260, Loss: 0.030, Accuracy: 0.993, F1: 0.993\n",
      "TRAIN: Epoch: 3, Batch: 301 / 5260, Loss: 0.027, Accuracy: 0.992, F1: 0.992\n",
      "TRAIN: Epoch: 3, Batch: 351 / 5260, Loss: 0.024, Accuracy: 0.993, F1: 0.993\n",
      "TRAIN: Epoch: 3, Batch: 401 / 5260, Loss: 0.006, Accuracy: 1.000, F1: 1.000\n",
      "TRAIN: Epoch: 3, Batch: 451 / 5260, Loss: 0.036, Accuracy: 0.988, F1: 0.988\n",
      "TRAIN: Epoch: 3, Batch: 501 / 5260, Loss: 0.030, Accuracy: 0.986, F1: 0.986\n",
      "TRAIN: Epoch: 3, Batch: 551 / 5260, Loss: 0.009, Accuracy: 0.997, F1: 0.997\n",
      "TRAIN: Epoch: 3, Batch: 601 / 5260, Loss: 0.026, Accuracy: 0.992, F1: 0.992\n",
      "TRAIN: Epoch: 3, Batch: 651 / 5260, Loss: 0.024, Accuracy: 0.995, F1: 0.995\n",
      "TRAIN: Epoch: 3, Batch: 701 / 5260, Loss: 0.009, Accuracy: 0.998, F1: 0.998\n",
      "TRAIN: Epoch: 3, Batch: 751 / 5260, Loss: 0.053, Accuracy: 0.986, F1: 0.986\n",
      "TRAIN: Epoch: 3, Batch: 801 / 5260, Loss: 0.025, Accuracy: 0.991, F1: 0.991\n",
      "TRAIN: Epoch: 3, Batch: 851 / 5260, Loss: 0.012, Accuracy: 0.997, F1: 0.997\n",
      "TRAIN: Epoch: 3, Batch: 901 / 5260, Loss: 0.021, Accuracy: 0.996, F1: 0.996\n",
      "TRAIN: Epoch: 3, Batch: 951 / 5260, Loss: 0.030, Accuracy: 0.988, F1: 0.988\n",
      "TRAIN: Epoch: 3, Batch: 1001 / 5260, Loss: 0.033, Accuracy: 0.992, F1: 0.992\n",
      "TRAIN: Epoch: 3, Batch: 1051 / 5260, Loss: 0.006, Accuracy: 0.999, F1: 0.999\n",
      "TRAIN: Epoch: 3, Batch: 1101 / 5260, Loss: 0.045, Accuracy: 0.991, F1: 0.991\n",
      "TRAIN: Epoch: 3, Batch: 1151 / 5260, Loss: 0.007, Accuracy: 0.999, F1: 0.999\n",
      "TRAIN: Epoch: 3, Batch: 1201 / 5260, Loss: 0.029, Accuracy: 0.988, F1: 0.988\n",
      "TRAIN: Epoch: 3, Batch: 1251 / 5260, Loss: 0.024, Accuracy: 0.991, F1: 0.991\n",
      "TRAIN: Epoch: 3, Batch: 1301 / 5260, Loss: 0.044, Accuracy: 0.988, F1: 0.988\n",
      "TRAIN: Epoch: 3, Batch: 1351 / 5260, Loss: 0.024, Accuracy: 0.988, F1: 0.988\n",
      "TRAIN: Epoch: 3, Batch: 1401 / 5260, Loss: 0.065, Accuracy: 0.983, F1: 0.983\n",
      "TRAIN: Epoch: 3, Batch: 1451 / 5260, Loss: 0.021, Accuracy: 0.992, F1: 0.992\n",
      "TRAIN: Epoch: 3, Batch: 1501 / 5260, Loss: 0.056, Accuracy: 0.983, F1: 0.983\n",
      "TRAIN: Epoch: 3, Batch: 1551 / 5260, Loss: 0.026, Accuracy: 0.991, F1: 0.991\n",
      "TRAIN: Epoch: 3, Batch: 1601 / 5260, Loss: 0.026, Accuracy: 0.991, F1: 0.991\n",
      "TRAIN: Epoch: 3, Batch: 1651 / 5260, Loss: 0.031, Accuracy: 0.992, F1: 0.992\n",
      "TRAIN: Epoch: 3, Batch: 1701 / 5260, Loss: 0.042, Accuracy: 0.990, F1: 0.990\n",
      "TRAIN: Epoch: 3, Batch: 1751 / 5260, Loss: 0.036, Accuracy: 0.990, F1: 0.990\n",
      "TRAIN: Epoch: 3, Batch: 1801 / 5260, Loss: 0.030, Accuracy: 0.990, F1: 0.990\n",
      "TRAIN: Epoch: 3, Batch: 1851 / 5260, Loss: 0.036, Accuracy: 0.989, F1: 0.989\n",
      "TRAIN: Epoch: 3, Batch: 1901 / 5260, Loss: 0.024, Accuracy: 0.994, F1: 0.994\n",
      "TRAIN: Epoch: 3, Batch: 1951 / 5260, Loss: 0.030, Accuracy: 0.991, F1: 0.991\n",
      "TRAIN: Epoch: 3, Batch: 2001 / 5260, Loss: 0.032, Accuracy: 0.991, F1: 0.991\n",
      "TRAIN: Epoch: 3, Batch: 2051 / 5260, Loss: 0.024, Accuracy: 0.992, F1: 0.992\n",
      "TRAIN: Epoch: 3, Batch: 2101 / 5260, Loss: 0.024, Accuracy: 0.994, F1: 0.994\n",
      "TRAIN: Epoch: 3, Batch: 2151 / 5260, Loss: 0.021, Accuracy: 0.991, F1: 0.991\n",
      "TRAIN: Epoch: 3, Batch: 2201 / 5260, Loss: 0.035, Accuracy: 0.986, F1: 0.986\n",
      "TRAIN: Epoch: 3, Batch: 2251 / 5260, Loss: 0.029, Accuracy: 0.989, F1: 0.989\n",
      "TRAIN: Epoch: 3, Batch: 2301 / 5260, Loss: 0.026, Accuracy: 0.992, F1: 0.992\n",
      "TRAIN: Epoch: 3, Batch: 2351 / 5260, Loss: 0.019, Accuracy: 0.992, F1: 0.992\n",
      "TRAIN: Epoch: 3, Batch: 2401 / 5260, Loss: 0.030, Accuracy: 0.994, F1: 0.994\n",
      "TRAIN: Epoch: 3, Batch: 2451 / 5260, Loss: 0.039, Accuracy: 0.987, F1: 0.987\n",
      "TRAIN: Epoch: 3, Batch: 2501 / 5260, Loss: 0.030, Accuracy: 0.988, F1: 0.988\n",
      "TRAIN: Epoch: 3, Batch: 2551 / 5260, Loss: 0.035, Accuracy: 0.991, F1: 0.991\n",
      "TRAIN: Epoch: 3, Batch: 2601 / 5260, Loss: 0.032, Accuracy: 0.987, F1: 0.987\n",
      "TRAIN: Epoch: 3, Batch: 2651 / 5260, Loss: 0.012, Accuracy: 0.995, F1: 0.995\n",
      "TRAIN: Epoch: 3, Batch: 2701 / 5260, Loss: 0.023, Accuracy: 0.991, F1: 0.991\n",
      "TRAIN: Epoch: 3, Batch: 2751 / 5260, Loss: 0.029, Accuracy: 0.989, F1: 0.989\n",
      "TRAIN: Epoch: 3, Batch: 2801 / 5260, Loss: 0.065, Accuracy: 0.981, F1: 0.981\n",
      "TRAIN: Epoch: 3, Batch: 2851 / 5260, Loss: 0.017, Accuracy: 0.995, F1: 0.995\n",
      "TRAIN: Epoch: 3, Batch: 2901 / 5260, Loss: 0.030, Accuracy: 0.987, F1: 0.987\n",
      "TRAIN: Epoch: 3, Batch: 2951 / 5260, Loss: 0.018, Accuracy: 0.995, F1: 0.995\n",
      "TRAIN: Epoch: 3, Batch: 3001 / 5260, Loss: 0.017, Accuracy: 0.994, F1: 0.994\n",
      "TRAIN: Epoch: 3, Batch: 3051 / 5260, Loss: 0.028, Accuracy: 0.992, F1: 0.992\n",
      "TRAIN: Epoch: 3, Batch: 3101 / 5260, Loss: 0.021, Accuracy: 0.992, F1: 0.992\n",
      "TRAIN: Epoch: 3, Batch: 3151 / 5260, Loss: 0.035, Accuracy: 0.983, F1: 0.983\n",
      "TRAIN: Epoch: 3, Batch: 3201 / 5260, Loss: 0.023, Accuracy: 0.992, F1: 0.992\n",
      "TRAIN: Epoch: 3, Batch: 3251 / 5260, Loss: 0.036, Accuracy: 0.986, F1: 0.986\n",
      "TRAIN: Epoch: 3, Batch: 3301 / 5260, Loss: 0.026, Accuracy: 0.993, F1: 0.993\n",
      "TRAIN: Epoch: 3, Batch: 3351 / 5260, Loss: 0.027, Accuracy: 0.992, F1: 0.992\n",
      "TRAIN: Epoch: 3, Batch: 3401 / 5260, Loss: 0.027, Accuracy: 0.990, F1: 0.990\n",
      "TRAIN: Epoch: 3, Batch: 3451 / 5260, Loss: 0.013, Accuracy: 0.997, F1: 0.997\n",
      "TRAIN: Epoch: 3, Batch: 3501 / 5260, Loss: 0.011, Accuracy: 0.997, F1: 0.997\n",
      "TRAIN: Epoch: 3, Batch: 3551 / 5260, Loss: 0.009, Accuracy: 0.999, F1: 0.999\n",
      "TRAIN: Epoch: 3, Batch: 3601 / 5260, Loss: 0.039, Accuracy: 0.985, F1: 0.985\n",
      "TRAIN: Epoch: 3, Batch: 3651 / 5260, Loss: 0.030, Accuracy: 0.992, F1: 0.992\n",
      "TRAIN: Epoch: 3, Batch: 3701 / 5260, Loss: 0.012, Accuracy: 0.997, F1: 0.997\n",
      "TRAIN: Epoch: 3, Batch: 3751 / 5260, Loss: 0.008, Accuracy: 0.999, F1: 0.999\n",
      "TRAIN: Epoch: 3, Batch: 3801 / 5260, Loss: 0.015, Accuracy: 0.997, F1: 0.997\n",
      "TRAIN: Epoch: 3, Batch: 3851 / 5260, Loss: 0.039, Accuracy: 0.986, F1: 0.986\n",
      "TRAIN: Epoch: 3, Batch: 3901 / 5260, Loss: 0.046, Accuracy: 0.984, F1: 0.984\n",
      "TRAIN: Epoch: 3, Batch: 3951 / 5260, Loss: 0.055, Accuracy: 0.982, F1: 0.982\n",
      "TRAIN: Epoch: 3, Batch: 4001 / 5260, Loss: 0.011, Accuracy: 0.997, F1: 0.997\n",
      "TRAIN: Epoch: 3, Batch: 4051 / 5260, Loss: 0.023, Accuracy: 0.992, F1: 0.992\n",
      "TRAIN: Epoch: 3, Batch: 4101 / 5260, Loss: 0.021, Accuracy: 0.991, F1: 0.991\n",
      "TRAIN: Epoch: 3, Batch: 4151 / 5260, Loss: 0.027, Accuracy: 0.988, F1: 0.988\n",
      "TRAIN: Epoch: 3, Batch: 4201 / 5260, Loss: 0.016, Accuracy: 0.996, F1: 0.996\n",
      "TRAIN: Epoch: 3, Batch: 4251 / 5260, Loss: 0.010, Accuracy: 0.998, F1: 0.998\n",
      "TRAIN: Epoch: 3, Batch: 4301 / 5260, Loss: 0.041, Accuracy: 0.988, F1: 0.988\n",
      "TRAIN: Epoch: 3, Batch: 4351 / 5260, Loss: 0.025, Accuracy: 0.993, F1: 0.993\n",
      "TRAIN: Epoch: 3, Batch: 4401 / 5260, Loss: 0.040, Accuracy: 0.989, F1: 0.989\n",
      "TRAIN: Epoch: 3, Batch: 4451 / 5260, Loss: 0.036, Accuracy: 0.986, F1: 0.986\n",
      "TRAIN: Epoch: 3, Batch: 4501 / 5260, Loss: 0.010, Accuracy: 0.998, F1: 0.998\n",
      "TRAIN: Epoch: 3, Batch: 4551 / 5260, Loss: 0.013, Accuracy: 0.996, F1: 0.996\n",
      "TRAIN: Epoch: 3, Batch: 4601 / 5260, Loss: 0.024, Accuracy: 0.990, F1: 0.990\n",
      "TRAIN: Epoch: 3, Batch: 4651 / 5260, Loss: 0.023, Accuracy: 0.990, F1: 0.990\n",
      "TRAIN: Epoch: 3, Batch: 4701 / 5260, Loss: 0.046, Accuracy: 0.989, F1: 0.989\n",
      "TRAIN: Epoch: 3, Batch: 4751 / 5260, Loss: 0.029, Accuracy: 0.989, F1: 0.989\n",
      "TRAIN: Epoch: 3, Batch: 4801 / 5260, Loss: 0.045, Accuracy: 0.985, F1: 0.985\n",
      "TRAIN: Epoch: 3, Batch: 4851 / 5260, Loss: 0.030, Accuracy: 0.990, F1: 0.990\n",
      "TRAIN: Epoch: 3, Batch: 4901 / 5260, Loss: 0.022, Accuracy: 0.996, F1: 0.996\n",
      "TRAIN: Epoch: 3, Batch: 4951 / 5260, Loss: 0.013, Accuracy: 0.997, F1: 0.997\n",
      "TRAIN: Epoch: 3, Batch: 5001 / 5260, Loss: 0.032, Accuracy: 0.988, F1: 0.988\n",
      "TRAIN: Epoch: 3, Batch: 5051 / 5260, Loss: 0.018, Accuracy: 0.994, F1: 0.994\n",
      "TRAIN: Epoch: 3, Batch: 5101 / 5260, Loss: 0.022, Accuracy: 0.993, F1: 0.993\n",
      "TRAIN: Epoch: 3, Batch: 5151 / 5260, Loss: 0.042, Accuracy: 0.987, F1: 0.987\n",
      "TRAIN: Epoch: 3, Batch: 5201 / 5260, Loss: 0.026, Accuracy: 0.991, F1: 0.991\n",
      "TRAIN: Epoch: 3, Batch: 5251 / 5260, Loss: 0.016, Accuracy: 0.996, F1: 0.996\n",
      "VAL: Epoch: 3, Batch: 1 / 275, Loss: 0.137, Accuracy: 0.972, F1: 0.972\n",
      "VAL: Epoch: 3, Batch: 51 / 275, Loss: 0.040, Accuracy: 0.984, F1: 0.984\n",
      "VAL: Epoch: 3, Batch: 101 / 275, Loss: 0.108, Accuracy: 0.963, F1: 0.963\n",
      "VAL: Epoch: 3, Batch: 151 / 275, Loss: 0.125, Accuracy: 0.953, F1: 0.953\n",
      "VAL: Epoch: 3, Batch: 201 / 275, Loss: 0.038, Accuracy: 0.984, F1: 0.984\n",
      "VAL: Epoch: 3, Batch: 251 / 275, Loss: 0.128, Accuracy: 0.961, F1: 0.961\n",
      "After epoch #3:\n",
      "Train loss: 0.028, Train Accuracy: 0.991, Train F1: 0.991\n",
      "Dev loss: 0.062, Dev Accuracy: 0.981, Dev F1: 0.981\n",
      "\n",
      "TRAIN: Epoch: 4, Batch: 1 / 5260, Loss: 0.013, Accuracy: 0.997, F1: 0.997\n",
      "TRAIN: Epoch: 4, Batch: 51 / 5260, Loss: 0.007, Accuracy: 0.999, F1: 0.999\n",
      "TRAIN: Epoch: 4, Batch: 101 / 5260, Loss: 0.020, Accuracy: 0.992, F1: 0.992\n",
      "TRAIN: Epoch: 4, Batch: 151 / 5260, Loss: 0.008, Accuracy: 0.998, F1: 0.998\n",
      "TRAIN: Epoch: 4, Batch: 201 / 5260, Loss: 0.010, Accuracy: 0.997, F1: 0.997\n",
      "TRAIN: Epoch: 4, Batch: 251 / 5260, Loss: 0.029, Accuracy: 0.992, F1: 0.992\n",
      "TRAIN: Epoch: 4, Batch: 301 / 5260, Loss: 0.011, Accuracy: 0.997, F1: 0.997\n",
      "TRAIN: Epoch: 4, Batch: 351 / 5260, Loss: 0.023, Accuracy: 0.994, F1: 0.994\n",
      "TRAIN: Epoch: 4, Batch: 401 / 5260, Loss: 0.011, Accuracy: 0.995, F1: 0.995\n",
      "TRAIN: Epoch: 4, Batch: 451 / 5260, Loss: 0.019, Accuracy: 0.992, F1: 0.992\n",
      "TRAIN: Epoch: 4, Batch: 501 / 5260, Loss: 0.020, Accuracy: 0.993, F1: 0.993\n",
      "TRAIN: Epoch: 4, Batch: 551 / 5260, Loss: 0.046, Accuracy: 0.985, F1: 0.985\n",
      "TRAIN: Epoch: 4, Batch: 601 / 5260, Loss: 0.021, Accuracy: 0.989, F1: 0.989\n",
      "TRAIN: Epoch: 4, Batch: 651 / 5260, Loss: 0.010, Accuracy: 0.996, F1: 0.996\n",
      "TRAIN: Epoch: 4, Batch: 701 / 5260, Loss: 0.015, Accuracy: 0.996, F1: 0.996\n",
      "TRAIN: Epoch: 4, Batch: 751 / 5260, Loss: 0.017, Accuracy: 0.994, F1: 0.994\n",
      "TRAIN: Epoch: 4, Batch: 801 / 5260, Loss: 0.018, Accuracy: 0.993, F1: 0.993\n",
      "TRAIN: Epoch: 4, Batch: 851 / 5260, Loss: 0.031, Accuracy: 0.992, F1: 0.992\n",
      "TRAIN: Epoch: 4, Batch: 901 / 5260, Loss: 0.020, Accuracy: 0.994, F1: 0.994\n",
      "TRAIN: Epoch: 4, Batch: 951 / 5260, Loss: 0.018, Accuracy: 0.991, F1: 0.991\n",
      "TRAIN: Epoch: 4, Batch: 1001 / 5260, Loss: 0.011, Accuracy: 0.999, F1: 0.999\n",
      "TRAIN: Epoch: 4, Batch: 1051 / 5260, Loss: 0.007, Accuracy: 0.996, F1: 0.996\n",
      "TRAIN: Epoch: 4, Batch: 1101 / 5260, Loss: 0.023, Accuracy: 0.992, F1: 0.992\n",
      "TRAIN: Epoch: 4, Batch: 1151 / 5260, Loss: 0.009, Accuracy: 0.998, F1: 0.998\n",
      "TRAIN: Epoch: 4, Batch: 1201 / 5260, Loss: 0.030, Accuracy: 0.990, F1: 0.990\n",
      "TRAIN: Epoch: 4, Batch: 1251 / 5260, Loss: 0.014, Accuracy: 0.993, F1: 0.993\n",
      "TRAIN: Epoch: 4, Batch: 1301 / 5260, Loss: 0.027, Accuracy: 0.987, F1: 0.987\n",
      "TRAIN: Epoch: 4, Batch: 1351 / 5260, Loss: 0.044, Accuracy: 0.988, F1: 0.988\n",
      "TRAIN: Epoch: 4, Batch: 1401 / 5260, Loss: 0.005, Accuracy: 0.999, F1: 0.999\n",
      "TRAIN: Epoch: 4, Batch: 1451 / 5260, Loss: 0.022, Accuracy: 0.992, F1: 0.992\n",
      "TRAIN: Epoch: 4, Batch: 1501 / 5260, Loss: 0.028, Accuracy: 0.991, F1: 0.991\n",
      "TRAIN: Epoch: 4, Batch: 1551 / 5260, Loss: 0.025, Accuracy: 0.993, F1: 0.993\n",
      "TRAIN: Epoch: 4, Batch: 1601 / 5260, Loss: 0.008, Accuracy: 0.998, F1: 0.998\n",
      "TRAIN: Epoch: 4, Batch: 1651 / 5260, Loss: 0.044, Accuracy: 0.985, F1: 0.985\n",
      "TRAIN: Epoch: 4, Batch: 1701 / 5260, Loss: 0.006, Accuracy: 0.999, F1: 0.999\n",
      "TRAIN: Epoch: 4, Batch: 1751 / 5260, Loss: 0.027, Accuracy: 0.994, F1: 0.994\n",
      "TRAIN: Epoch: 4, Batch: 1801 / 5260, Loss: 0.041, Accuracy: 0.991, F1: 0.991\n",
      "TRAIN: Epoch: 4, Batch: 1851 / 5260, Loss: 0.019, Accuracy: 0.993, F1: 0.993\n",
      "TRAIN: Epoch: 4, Batch: 1901 / 5260, Loss: 0.038, Accuracy: 0.986, F1: 0.986\n",
      "TRAIN: Epoch: 4, Batch: 1951 / 5260, Loss: 0.005, Accuracy: 0.999, F1: 0.999\n",
      "TRAIN: Epoch: 4, Batch: 2001 / 5260, Loss: 0.027, Accuracy: 0.992, F1: 0.992\n",
      "TRAIN: Epoch: 4, Batch: 2051 / 5260, Loss: 0.005, Accuracy: 1.000, F1: 1.000\n",
      "TRAIN: Epoch: 4, Batch: 2101 / 5260, Loss: 0.014, Accuracy: 0.994, F1: 0.994\n",
      "TRAIN: Epoch: 4, Batch: 2151 / 5260, Loss: 0.023, Accuracy: 0.995, F1: 0.995\n",
      "TRAIN: Epoch: 4, Batch: 2201 / 5260, Loss: 0.011, Accuracy: 0.997, F1: 0.997\n",
      "TRAIN: Epoch: 4, Batch: 2251 / 5260, Loss: 0.021, Accuracy: 0.994, F1: 0.994\n",
      "TRAIN: Epoch: 4, Batch: 2301 / 5260, Loss: 0.031, Accuracy: 0.988, F1: 0.988\n",
      "TRAIN: Epoch: 4, Batch: 2351 / 5260, Loss: 0.007, Accuracy: 0.998, F1: 0.998\n",
      "TRAIN: Epoch: 4, Batch: 2401 / 5260, Loss: 0.030, Accuracy: 0.991, F1: 0.991\n",
      "TRAIN: Epoch: 4, Batch: 2451 / 5260, Loss: 0.016, Accuracy: 0.992, F1: 0.992\n",
      "TRAIN: Epoch: 4, Batch: 2501 / 5260, Loss: 0.011, Accuracy: 0.998, F1: 0.998\n",
      "TRAIN: Epoch: 4, Batch: 2551 / 5260, Loss: 0.008, Accuracy: 0.999, F1: 0.999\n",
      "TRAIN: Epoch: 4, Batch: 2601 / 5260, Loss: 0.017, Accuracy: 0.993, F1: 0.993\n",
      "TRAIN: Epoch: 4, Batch: 2651 / 5260, Loss: 0.014, Accuracy: 0.995, F1: 0.995\n",
      "TRAIN: Epoch: 4, Batch: 2701 / 5260, Loss: 0.014, Accuracy: 0.997, F1: 0.997\n",
      "TRAIN: Epoch: 4, Batch: 2751 / 5260, Loss: 0.016, Accuracy: 0.994, F1: 0.994\n",
      "TRAIN: Epoch: 4, Batch: 2801 / 5260, Loss: 0.036, Accuracy: 0.992, F1: 0.992\n",
      "TRAIN: Epoch: 4, Batch: 2851 / 5260, Loss: 0.025, Accuracy: 0.990, F1: 0.990\n",
      "TRAIN: Epoch: 4, Batch: 2901 / 5260, Loss: 0.019, Accuracy: 0.990, F1: 0.990\n",
      "TRAIN: Epoch: 4, Batch: 2951 / 5260, Loss: 0.015, Accuracy: 0.994, F1: 0.994\n",
      "TRAIN: Epoch: 4, Batch: 3001 / 5260, Loss: 0.009, Accuracy: 0.996, F1: 0.996\n",
      "TRAIN: Epoch: 4, Batch: 3051 / 5260, Loss: 0.019, Accuracy: 0.992, F1: 0.992\n",
      "TRAIN: Epoch: 4, Batch: 3101 / 5260, Loss: 0.014, Accuracy: 0.997, F1: 0.997\n",
      "TRAIN: Epoch: 4, Batch: 3151 / 5260, Loss: 0.012, Accuracy: 0.996, F1: 0.996\n",
      "TRAIN: Epoch: 4, Batch: 3201 / 5260, Loss: 0.006, Accuracy: 0.999, F1: 0.999\n",
      "TRAIN: Epoch: 4, Batch: 3251 / 5260, Loss: 0.016, Accuracy: 0.996, F1: 0.996\n",
      "TRAIN: Epoch: 4, Batch: 3301 / 5260, Loss: 0.015, Accuracy: 0.993, F1: 0.993\n",
      "TRAIN: Epoch: 4, Batch: 3351 / 5260, Loss: 0.020, Accuracy: 0.988, F1: 0.988\n",
      "TRAIN: Epoch: 4, Batch: 3401 / 5260, Loss: 0.025, Accuracy: 0.990, F1: 0.990\n",
      "TRAIN: Epoch: 4, Batch: 3451 / 5260, Loss: 0.009, Accuracy: 0.996, F1: 0.996\n",
      "TRAIN: Epoch: 4, Batch: 3501 / 5260, Loss: 0.014, Accuracy: 0.996, F1: 0.996\n",
      "TRAIN: Epoch: 4, Batch: 3551 / 5260, Loss: 0.025, Accuracy: 0.991, F1: 0.991\n",
      "TRAIN: Epoch: 4, Batch: 3601 / 5260, Loss: 0.006, Accuracy: 0.999, F1: 0.999\n",
      "TRAIN: Epoch: 4, Batch: 3651 / 5260, Loss: 0.013, Accuracy: 0.995, F1: 0.995\n",
      "TRAIN: Epoch: 4, Batch: 3701 / 5260, Loss: 0.017, Accuracy: 0.992, F1: 0.992\n",
      "TRAIN: Epoch: 4, Batch: 3751 / 5260, Loss: 0.015, Accuracy: 0.995, F1: 0.995\n",
      "TRAIN: Epoch: 4, Batch: 3801 / 5260, Loss: 0.008, Accuracy: 0.996, F1: 0.996\n",
      "TRAIN: Epoch: 4, Batch: 3851 / 5260, Loss: 0.033, Accuracy: 0.990, F1: 0.990\n",
      "TRAIN: Epoch: 4, Batch: 3901 / 5260, Loss: 0.016, Accuracy: 0.993, F1: 0.993\n",
      "TRAIN: Epoch: 4, Batch: 3951 / 5260, Loss: 0.007, Accuracy: 0.997, F1: 0.997\n",
      "TRAIN: Epoch: 4, Batch: 4001 / 5260, Loss: 0.016, Accuracy: 0.996, F1: 0.996\n",
      "TRAIN: Epoch: 4, Batch: 4051 / 5260, Loss: 0.017, Accuracy: 0.993, F1: 0.993\n",
      "TRAIN: Epoch: 4, Batch: 4101 / 5260, Loss: 0.021, Accuracy: 0.992, F1: 0.992\n",
      "TRAIN: Epoch: 4, Batch: 4151 / 5260, Loss: 0.023, Accuracy: 0.991, F1: 0.991\n",
      "TRAIN: Epoch: 4, Batch: 4201 / 5260, Loss: 0.032, Accuracy: 0.989, F1: 0.989\n",
      "TRAIN: Epoch: 4, Batch: 4251 / 5260, Loss: 0.038, Accuracy: 0.988, F1: 0.988\n",
      "TRAIN: Epoch: 4, Batch: 4301 / 5260, Loss: 0.012, Accuracy: 0.996, F1: 0.996\n",
      "TRAIN: Epoch: 4, Batch: 4351 / 5260, Loss: 0.005, Accuracy: 0.997, F1: 0.997\n",
      "TRAIN: Epoch: 4, Batch: 4401 / 5260, Loss: 0.004, Accuracy: 0.999, F1: 0.999\n",
      "TRAIN: Epoch: 4, Batch: 4451 / 5260, Loss: 0.005, Accuracy: 0.999, F1: 0.999\n",
      "TRAIN: Epoch: 4, Batch: 4501 / 5260, Loss: 0.015, Accuracy: 0.996, F1: 0.996\n",
      "TRAIN: Epoch: 4, Batch: 4551 / 5260, Loss: 0.018, Accuracy: 0.996, F1: 0.996\n",
      "TRAIN: Epoch: 4, Batch: 4601 / 5260, Loss: 0.020, Accuracy: 0.993, F1: 0.993\n",
      "TRAIN: Epoch: 4, Batch: 4651 / 5260, Loss: 0.017, Accuracy: 0.998, F1: 0.998\n",
      "TRAIN: Epoch: 4, Batch: 4701 / 5260, Loss: 0.027, Accuracy: 0.992, F1: 0.992\n",
      "TRAIN: Epoch: 4, Batch: 4751 / 5260, Loss: 0.005, Accuracy: 0.999, F1: 0.999\n",
      "TRAIN: Epoch: 4, Batch: 4801 / 5260, Loss: 0.006, Accuracy: 0.999, F1: 0.999\n",
      "TRAIN: Epoch: 4, Batch: 4851 / 5260, Loss: 0.045, Accuracy: 0.984, F1: 0.984\n",
      "TRAIN: Epoch: 4, Batch: 4901 / 5260, Loss: 0.031, Accuracy: 0.995, F1: 0.995\n",
      "TRAIN: Epoch: 4, Batch: 4951 / 5260, Loss: 0.020, Accuracy: 0.995, F1: 0.995\n",
      "TRAIN: Epoch: 4, Batch: 5001 / 5260, Loss: 0.008, Accuracy: 0.997, F1: 0.997\n",
      "TRAIN: Epoch: 4, Batch: 5051 / 5260, Loss: 0.016, Accuracy: 0.998, F1: 0.998\n",
      "TRAIN: Epoch: 4, Batch: 5101 / 5260, Loss: 0.030, Accuracy: 0.987, F1: 0.987\n",
      "TRAIN: Epoch: 4, Batch: 5151 / 5260, Loss: 0.007, Accuracy: 0.999, F1: 0.999\n",
      "TRAIN: Epoch: 4, Batch: 5201 / 5260, Loss: 0.026, Accuracy: 0.993, F1: 0.993\n",
      "TRAIN: Epoch: 4, Batch: 5251 / 5260, Loss: 0.052, Accuracy: 0.987, F1: 0.987\n",
      "VAL: Epoch: 4, Batch: 1 / 275, Loss: 0.126, Accuracy: 0.964, F1: 0.964\n",
      "VAL: Epoch: 4, Batch: 51 / 275, Loss: 0.043, Accuracy: 0.986, F1: 0.986\n",
      "VAL: Epoch: 4, Batch: 101 / 275, Loss: 0.111, Accuracy: 0.965, F1: 0.965\n",
      "VAL: Epoch: 4, Batch: 151 / 275, Loss: 0.138, Accuracy: 0.951, F1: 0.951\n",
      "VAL: Epoch: 4, Batch: 201 / 275, Loss: 0.038, Accuracy: 0.988, F1: 0.988\n",
      "VAL: Epoch: 4, Batch: 251 / 275, Loss: 0.206, Accuracy: 0.952, F1: 0.952\n",
      "After epoch #4:\n",
      "Train loss: 0.019, Train Accuracy: 0.994, Train F1: 0.994\n",
      "Dev loss: 0.068, Dev Accuracy: 0.980, Dev F1: 0.980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "last_epoch = 0\n",
    "dev_losses = []\n",
    "patience = 1\n",
    "\n",
    "for epoch in range(1, 20):\n",
    "    train_loss, train_acc, train_f1 = _train(model, train_batches, optimizer, epoch)\n",
    "    dev_loss, dev_acc, dev_f1 = _val(model, dev_batches, epoch)\n",
    "    scheduler.step(dev_loss)\n",
    "\n",
    "    if len(dev_losses) == 0 or dev_loss < dev_losses[-1]:\n",
    "        model.save_pretrained(SAVE_PATH + str(epoch))\n",
    "\n",
    "    elif last_epoch == 0:\n",
    "        last_epoch = epoch + patience\n",
    "\n",
    "    print(f\"After epoch #{epoch}:\")\n",
    "    print(f\"Train loss: {train_loss:.3f}, Train Accuracy: {train_acc:.3f}, Train F1: {train_f1:.3f}\")\n",
    "    print(f\"Dev loss: {dev_loss:.3f}, Dev Accuracy: {dev_acc:.3f}, Dev F1: {dev_f1:.3f}\\n\")\n",
    "\n",
    "    dev_losses.append(dev_loss)\n",
    "    if epoch == last_epoch:\n",
    "        model.save_pretrained(SAVE_PATH + str(epoch))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4b60aa",
   "metadata": {
    "cellId": "0gernq71wx2g7oeubz7z7ti"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e5693f18",
   "metadata": {
    "cellId": "7oshwfxkcbb4aefmvm35ee",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting allennlp\n",
      "  Downloading allennlp-2.8.0-py3-none-any.whl (738 kB)\n",
      "     |████████████████████████████████| 738 kB 1.3 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.8/dist-packages (from allennlp) (2.4)\n",
      "Requirement already satisfied: torchvision<0.12.0,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from allennlp) (0.10.1+cu111)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.16 in /usr/local/lib/python3.8/dist-packages (from allennlp) (0.2.1)\n",
      "Collecting lmdb\n",
      "  Downloading lmdb-1.3.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (305 kB)\n",
      "     |████████████████████████████████| 305 kB 27.7 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: torch<1.11.0,>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from allennlp) (1.9.1+cu111)\n",
      "Collecting datasets<2.0,>=1.2.1\n",
      "  Downloading datasets-1.17.0-py3-none-any.whl (306 kB)\n",
      "     |████████████████████████████████| 306 kB 29.2 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: sentencepiece in /home/jupyter/.local/lib/python3.8/site-packages (from allennlp) (0.1.96)\n",
      "Collecting base58\n",
      "  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
      "Collecting checklist==0.0.11\n",
      "  Downloading checklist-0.0.11.tar.gz (12.1 MB)\n",
      "     |████████████████████████████████| 12.1 MB 15.3 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting jsonnet>=0.10.0\n",
      "  Downloading jsonnet-0.18.0.tar.gz (592 kB)\n",
      "     |████████████████████████████████| 592 kB 38.2 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from allennlp) (0.22.1)\n",
      "Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.8/dist-packages (from allennlp) (1.1.0)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from allennlp) (0.3.4)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.8/dist-packages (from allennlp) (3.1.0)\n",
      "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.8/dist-packages (from allennlp) (4.50.0)\n",
      "Collecting filelock<3.4,>=3.3\n",
      "  Downloading filelock-3.3.2-py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from allennlp) (1.4.1)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from allennlp) (3.4.5)\n",
      "Collecting wandb<0.13.0,>=0.10.0\n",
      "  Downloading wandb-0.12.9-py2.py3-none-any.whl (1.7 MB)\n",
      "     |████████████████████████████████| 1.7 MB 73.0 MB/s            \n",
      "\u001b[?25hCollecting fairscale==0.4.0\n",
      "  Downloading fairscale-0.4.0.tar.gz (190 kB)\n",
      "     |████████████████████████████████| 190 kB 104.2 MB/s            \n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pytest in /kernel/lib/python3.8/site-packages (from allennlp) (6.2.5)\n",
      "Collecting overrides==3.1.0\n",
      "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /kernel/lib/python3.8/site-packages (from allennlp) (1.19.4)\n",
      "Collecting spacy<3.2,>=2.1.0\n",
      "  Downloading spacy-3.1.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.1 MB)\n",
      "     |████████████████████████████████| 6.1 MB 85.2 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: more-itertools in /usr/local/lib/python3.8/dist-packages (from allennlp) (8.12.0)\n",
      "Collecting sqlitedict\n",
      "  Downloading sqlitedict-1.7.0.tar.gz (28 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers<4.13,>=4.1\n",
      "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
      "     |████████████████████████████████| 3.1 MB 86.5 MB/s            \n",
      "\u001b[?25hCollecting cached-path<0.4.0,>=0.3.1\n",
      "  Downloading cached_path-0.3.4-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: requests>=2.18 in /kernel/lib/python3.8/site-packages (from allennlp) (2.25.1)\n",
      "Collecting munch>=2.5\n",
      "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting jupyter>=1.0\n",
      "  Using cached jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
      "Requirement already satisfied: ipywidgets>=7.5 in /usr/local/lib/python3.8/dist-packages (from checklist==0.0.11->allennlp) (7.6.5)\n",
      "Collecting patternfork-nosql\n",
      "  Downloading patternfork_nosql-3.6.tar.gz (22.3 MB)\n",
      "     |████████████████████████████████| 22.3 MB 92.7 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting iso-639\n",
      "  Downloading iso-639-0.4.5.tar.gz (167 kB)\n",
      "     |████████████████████████████████| 167 kB 66.9 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting google-cloud-storage<2.0,>=1.0\n",
      "  Downloading google_cloud_storage-1.44.0-py2.py3-none-any.whl (106 kB)\n",
      "     |████████████████████████████████| 106 kB 84.6 MB/s            \n",
      "\u001b[?25hCollecting cached-path<0.4.0,>=0.3.1\n",
      "  Downloading cached_path-0.3.3-py3-none-any.whl (26 kB)\n",
      "  Downloading cached_path-0.3.2-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: boto3<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from cached-path<0.4.0,>=0.3.1->allennlp) (1.12.31)\n",
      "Collecting huggingface-hub>=0.0.16\n",
      "  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n",
      "     |████████████████████████████████| 59 kB 10.1 MB/s            \n",
      "\u001b[?25hCollecting tqdm>=4.19\n",
      "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "     |████████████████████████████████| 76 kB 7.2 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: packaging in /kernel/lib/python3.8/site-packages (from datasets<2.0,>=1.2.1->allennlp) (20.9)\n",
      "Requirement already satisfied: xxhash in /kernel/lib/python3.8/site-packages (from datasets<2.0,>=1.2.1->allennlp) (2.0.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (0.25.3)\n",
      "Collecting datasets<2.0,>=1.2.1\n",
      "  Downloading datasets-1.16.1-py3-none-any.whl (298 kB)\n",
      "     |████████████████████████████████| 298 kB 92.2 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.16.0-py3-none-any.whl (298 kB)\n",
      "     |████████████████████████████████| 298 kB 57.6 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.15.1-py3-none-any.whl (290 kB)\n",
      "     |████████████████████████████████| 290 kB 94.9 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.8/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (2021.11.1)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.12.2-py38-none-any.whl (128 kB)\n",
      "     |████████████████████████████████| 128 kB 92.2 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (3.8.1)\n",
      "Collecting datasets<2.0,>=1.2.1\n",
      "  Downloading datasets-1.15.0-py3-none-any.whl (290 kB)\n",
      "     |████████████████████████████████| 290 kB 38.7 MB/s            \n",
      "\u001b[?25hCollecting huggingface-hub>=0.0.16\n",
      "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
      "     |████████████████████████████████| 56 kB 7.3 MB/s             \n",
      "\u001b[?25hCollecting datasets<2.0,>=1.2.1\n",
      "  Downloading datasets-1.14.0-py3-none-any.whl (290 kB)\n",
      "     |████████████████████████████████| 290 kB 99.4 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.13.3-py3-none-any.whl (287 kB)\n",
      "     |████████████████████████████████| 287 kB 104.1 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.13.2-py3-none-any.whl (287 kB)\n",
      "     |████████████████████████████████| 287 kB 63.2 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.13.1-py3-none-any.whl (287 kB)\n",
      "     |████████████████████████████████| 287 kB 59.8 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.13.0-py3-none-any.whl (285 kB)\n",
      "     |████████████████████████████████| 285 kB 7.1 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.12.1-py3-none-any.whl (270 kB)\n",
      "     |████████████████████████████████| 270 kB 79.0 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.12.0-py3-none-any.whl (269 kB)\n",
      "     |████████████████████████████████| 269 kB 94.4 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.11.0-py3-none-any.whl (264 kB)\n",
      "     |████████████████████████████████| 264 kB 70.7 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.10.2-py3-none-any.whl (542 kB)\n",
      "     |████████████████████████████████| 542 kB 103.2 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.10.1-py3-none-any.whl (542 kB)\n",
      "     |████████████████████████████████| 542 kB 83.0 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.10.0-py3-none-any.whl (542 kB)\n",
      "     |████████████████████████████████| 542 kB 95.1 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.9.0-py3-none-any.whl (262 kB)\n",
      "     |████████████████████████████████| 262 kB 95.1 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.8.0-py3-none-any.whl (237 kB)\n",
      "     |████████████████████████████████| 237 kB 93.9 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.7.0-py3-none-any.whl (234 kB)\n",
      "     |████████████████████████████████| 234 kB 100.9 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.6.2-py3-none-any.whl (221 kB)\n",
      "     |████████████████████████████████| 221 kB 99.1 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.6.1-py3-none-any.whl (220 kB)\n",
      "     |████████████████████████████████| 220 kB 103.8 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.6.0-py3-none-any.whl (202 kB)\n",
      "     |████████████████████████████████| 202 kB 26.9 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.5.0-py3-none-any.whl (192 kB)\n",
      "     |████████████████████████████████| 192 kB 95.9 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.4.1-py3-none-any.whl (186 kB)\n",
      "     |████████████████████████████████| 186 kB 102.2 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.4.0-py3-none-any.whl (186 kB)\n",
      "     |████████████████████████████████| 186 kB 102.5 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.3.0-py3-none-any.whl (181 kB)\n",
      "     |████████████████████████████████| 181 kB 104.1 MB/s            \n",
      "\u001b[?25h  Downloading datasets-1.2.1-py3-none-any.whl (159 kB)\n",
      "     |████████████████████████████████| 159 kB 40.2 MB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of cached-path to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting cached-path<0.4.0,>=0.3.1\n",
      "  Downloading cached_path-0.3.1-py3-none-any.whl (26 kB)\n",
      "INFO: pip is looking at multiple versions of termcolor to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting termcolor==1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of filelock to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting filelock<3.4,>=3.3\n",
      "  Downloading filelock-3.3.1-py3-none-any.whl (9.7 kB)\n",
      "INFO: pip is looking at multiple versions of overrides to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of fairscale to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of checklist to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of <Python from Requires-Python> to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of allennlp to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting allennlp\n",
      "  Downloading allennlp-2.7.0-py3-none-any.whl (738 kB)\n",
      "     |████████████████████████████████| 738 kB 99.8 MB/s            \n",
      "\u001b[?25hCollecting boto3<2.0,>=1.14\n",
      "  Downloading boto3-1.20.39-py3-none-any.whl (131 kB)\n",
      "     |████████████████████████████████| 131 kB 103.5 MB/s            \n",
      "\u001b[?25hCollecting google-cloud-storage<1.43.0,>=1.38.0\n",
      "  Downloading google_cloud_storage-1.42.3-py2.py3-none-any.whl (105 kB)\n",
      "     |████████████████████████████████| 105 kB 25.3 MB/s            \n",
      "\u001b[?25hCollecting filelock<3.1,>=3.0\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Collecting transformers<4.10,>=4.1\n",
      "  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
      "     |████████████████████████████████| 2.6 MB 105.0 MB/s            \n",
      "\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n",
      "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
      "     |████████████████████████████████| 79 kB 11.0 MB/s            \n",
      "\u001b[?25hCollecting botocore<1.24.0,>=1.23.39\n",
      "  Downloading botocore-1.23.39-py3-none-any.whl (8.5 MB)\n",
      "     |████████████████████████████████| 8.5 MB 95.3 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from boto3<2.0,>=1.14->allennlp) (0.10.0)\n",
      "Collecting tqdm>=4.19\n",
      "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
      "     |████████████████████████████████| 69 kB 10.8 MB/s            \n",
      "\u001b[?25hCollecting pyarrow>=0.17.1\n",
      "  Downloading pyarrow-0.17.1-cp38-cp38-manylinux2014_x86_64.whl (63.8 MB)\n",
      "     |████████████████████████████████| 63.8 MB 56 kB/s               \n",
      "\u001b[?25hRequirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp) (1.35.0)\n",
      "Collecting google-cloud-core<3.0dev,>=1.6.0\n",
      "  Downloading google_cloud_core-2.2.2-py2.py3-none-any.whl (29 kB)\n",
      "Collecting google-resumable-media<3.0dev,>=1.3.0\n",
      "  Downloading google_resumable_media-2.1.0-py2.py3-none-any.whl (75 kB)\n",
      "     |████████████████████████████████| 75 kB 7.1 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: six in /kernel/lib/python3.8/site-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp) (1.16.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp) (3.19.1)\n",
      "Requirement already satisfied: google-api-core<3.0dev,>=1.29.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp) (1.31.5)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.0.16->allennlp) (3.7.4.3)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.0.16->allennlp) (5.3.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /kernel/lib/python3.8/site-packages (from requests>=2.18->allennlp) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /kernel/lib/python3.8/site-packages (from requests>=2.18->allennlp) (1.26.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /kernel/lib/python3.8/site-packages (from requests>=2.18->allennlp) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /kernel/lib/python3.8/site-packages (from requests>=2.18->allennlp) (2021.10.8)\n",
      "Requirement already satisfied: jinja2 in /kernel/lib/python3.8/site-packages (from spacy<3.2,>=2.1.0->allennlp) (3.0.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (3.0.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (0.7.5)\n",
      "Requirement already satisfied: setuptools in /kernel/lib/python3.8/site-packages (from spacy<3.2,>=2.1.0->allennlp) (51.0.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (2.4.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.8/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (8.0.13)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (1.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (0.9.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (1.8.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (0.6.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (2.0.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (0.4.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (2.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.8/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (3.0.8)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /kernel/lib/python3.8/site-packages (from torchvision<0.12.0,>=0.8.1->allennlp) (9.0.0)\n",
      "Collecting huggingface-hub>=0.0.8\n",
      "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from transformers<4.10,>=4.1->allennlp) (0.0.46)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.8/dist-packages (from transformers<4.10,>=4.1->allennlp) (0.10.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<4.10,>=4.1->allennlp) (2021.11.10)\n",
      "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.8/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (4.0.2)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (8.0.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /kernel/lib/python3.8/site-packages (from wandb<0.13.0,>=0.10.0->allennlp) (5.7.3)\n",
      "Collecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting subprocess32>=3.5.3\n",
      "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
      "     |████████████████████████████████| 97 kB 10.3 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.8/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (2.3)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.5.3-py2.py3-none-any.whl (142 kB)\n",
      "     |████████████████████████████████| 142 kB 93.1 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /kernel/lib/python3.8/site-packages (from wandb<0.13.0,>=0.10.0->allennlp) (2.8.2)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.0.8)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (3.1.24)\n",
      "Collecting yaspin>=1.0.0\n",
      "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /kernel/lib/python3.8/site-packages (from pytest->allennlp) (21.4.0)\n",
      "Requirement already satisfied: iniconfig in /kernel/lib/python3.8/site-packages (from pytest->allennlp) (1.1.1)\n",
      "Requirement already satisfied: toml in /kernel/lib/python3.8/site-packages (from pytest->allennlp) (0.10.2)\n",
      "Requirement already satisfied: py>=1.8.2 in /kernel/lib/python3.8/site-packages (from pytest->allennlp) (1.11.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /kernel/lib/python3.8/site-packages (from pytest->allennlp) (1.0.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->allennlp) (1.1.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp) (4.0.9)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp) (1.54.0)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.8/dist-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp) (2021.3)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp) (4.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp) (0.2.8)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.3.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (37 kB)\n",
      "Collecting ipykernel>=4.5.1\n",
      "  Downloading ipykernel-5.1.4-py3-none-any.whl (116 kB)\n",
      "     |████████████████████████████████| 116 kB 101.4 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: ipython-genutils~=0.2.0 in /kernel/lib/python3.8/site-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.2.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (1.0.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /kernel/lib/python3.8/site-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.1.1)\n",
      "Collecting ipython>=4.0.0\n",
      "  Downloading ipython-7.13.0-py3-none-any.whl (780 kB)\n",
      "     |████████████████████████████████| 780 kB 106.8 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: nbformat>=4.2.0 in /kernel/lib/python3.8/site-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.1.3)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (3.5.2)\n",
      "Collecting jupyter-console\n",
      "  Using cached jupyter_console-6.4.0-py3-none-any.whl (22 kB)\n",
      "Collecting qtconsole\n",
      "  Using cached qtconsole-5.2.2-py3-none-any.whl (120 kB)\n",
      "Requirement already satisfied: nbconvert in /kernel/lib/python3.8/site-packages (from jupyter>=1.0->checklist==0.0.11->allennlp) (6.4.0)\n",
      "Collecting notebook\n",
      "  Downloading notebook-6.1.1-py3-none-any.whl (9.4 MB)\n",
      "     |████████████████████████████████| 9.4 MB 103.6 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /kernel/lib/python3.8/site-packages (from packaging->datasets<2.0,>=1.2.1->allennlp) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from pathy>=0.3.5->spacy<3.2,>=2.1.0->allennlp) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /kernel/lib/python3.8/site-packages (from jinja2->spacy<3.2,>=2.1.0->allennlp) (2.0.1)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp) (0.18.2)\n",
      "Collecting backports.csv\n",
      "  Downloading backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp) (4.10.0)\n",
      "Collecting lxml\n",
      "  Downloading lxml-4.7.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.9 MB)\n",
      "     |████████████████████████████████| 6.9 MB 101.2 MB/s            \n",
      "\u001b[?25hCollecting feedparser\n",
      "  Downloading feedparser-6.0.8-py3-none-any.whl (81 kB)\n",
      "     |████████████████████████████████| 81 kB 14.7 MB/s            \n",
      "\u001b[?25hCollecting pdfminer.six\n",
      "  Downloading pdfminer.six-20211012-py3-none-any.whl (5.6 MB)\n",
      "     |████████████████████████████████| 5.6 MB 96.9 MB/s            \n",
      "\u001b[?25hCollecting python-docx\n",
      "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
      "     |████████████████████████████████| 5.6 MB 103.2 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cherrypy\n",
      "  Downloading CherryPy-18.6.1-py2.py3-none-any.whl (419 kB)\n",
      "     |████████████████████████████████| 419 kB 81.5 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp) (5.0.0)\n",
      "Requirement already satisfied: jupyter-client in /kernel/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp) (6.1.12)\n",
      "Requirement already satisfied: tornado>=4.2 in /kernel/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp) (6.1)\n",
      "Requirement already satisfied: backcall in /kernel/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.10 in /kernel/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.17.2)\n",
      "Requirement already satisfied: decorator in /kernel/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.1.1)\n",
      "Requirement already satisfied: pickleshare in /kernel/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /kernel/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (3.0.24)\n",
      "Requirement already satisfied: pexpect in /kernel/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.8.0)\n",
      "Requirement already satisfied: pygments in /kernel/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (2.11.2)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /kernel/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.4.0)\n",
      "Requirement already satisfied: jupyter-core in /kernel/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.9.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp) (0.4.8)\n",
      "Requirement already satisfied: prometheus-client in /kernel/lib/python3.8/site-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (0.12.0)\n",
      "Requirement already satisfied: pyzmq>=17 in /kernel/lib/python3.8/site-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (22.3.0)\n",
      "Requirement already satisfied: Send2Trash in /kernel/lib/python3.8/site-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (1.8.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /kernel/lib/python3.8/site-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (0.12.1)\n",
      "Requirement already satisfied: argon2-cffi in /kernel/lib/python3.8/site-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (21.3.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.8/dist-packages (from beautifulsoup4->patternfork-nosql->checklist==0.0.11->allennlp) (2.3.1)\n",
      "Requirement already satisfied: zc.lockfile in /usr/local/lib/python3.8/dist-packages (from cherrypy->patternfork-nosql->checklist==0.0.11->allennlp) (2.0)\n",
      "Collecting cheroot>=8.2.1\n",
      "  Downloading cheroot-8.6.0-py2.py3-none-any.whl (104 kB)\n",
      "     |████████████████████████████████| 104 kB 98.9 MB/s            \n",
      "\u001b[?25hCollecting portend>=2.1.1\n",
      "  Downloading portend-3.1.0-py3-none-any.whl (5.3 kB)\n",
      "Collecting jaraco.collections\n",
      "  Downloading jaraco.collections-3.5.1-py3-none-any.whl (10 kB)\n",
      "Collecting sgmllib3k\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: entrypoints>=0.2.2 in /kernel/lib/python3.8/site-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.3)\n",
      "Requirement already satisfied: jupyterlab-pygments in /kernel/lib/python3.8/site-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.1.2)\n",
      "Requirement already satisfied: defusedxml in /kernel/lib/python3.8/site-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.7.1)\n",
      "Requirement already satisfied: bleach in /kernel/lib/python3.8/site-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (4.1.0)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /kernel/lib/python3.8/site-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.5.10)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /kernel/lib/python3.8/site-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.8.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /kernel/lib/python3.8/site-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (1.5.0)\n",
      "Requirement already satisfied: testpath in /kernel/lib/python3.8/site-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.5.0)\n",
      "Requirement already satisfied: cryptography in /usr/local/lib/python3.8/dist-packages (from pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp) (36.0.1)\n",
      "Collecting qtpy\n",
      "  Using cached QtPy-2.0.0-py3-none-any.whl (62 kB)\n",
      "Collecting jaraco.functools\n",
      "  Downloading jaraco.functools-3.5.0-py3-none-any.whl (7.0 kB)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /kernel/lib/python3.8/site-packages (from jedi>=0.10->ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.7.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /kernel/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.18.1)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /kernel/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.4.0)\n",
      "Requirement already satisfied: nest-asyncio in /kernel/lib/python3.8/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (1.5.4)\n",
      "Collecting tempora>=1.8\n",
      "  Downloading tempora-5.0.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: wcwidth in /kernel/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.2.5)\n",
      "Requirement already satisfied: ptyprocess in /kernel/lib/python3.8/site-packages (from terminado>=0.8.3->notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (0.7.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /kernel/lib/python3.8/site-packages (from argon2-cffi->notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (21.2.0)\n",
      "Requirement already satisfied: webencodings in /kernel/lib/python3.8/site-packages (from bleach->nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.5.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /kernel/lib/python3.8/site-packages (from cryptography->pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp) (1.15.0)\n",
      "Collecting jaraco.text\n",
      "  Downloading jaraco.text-3.6.0-py3-none-any.whl (8.1 kB)\n",
      "Collecting jaraco.classes\n",
      "  Downloading jaraco.classes-3.2.1-py3-none-any.whl (5.6 kB)\n",
      "Requirement already satisfied: pycparser in /kernel/lib/python3.8/site-packages (from cffi>=1.12->cryptography->pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp) (2.21)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /kernel/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (3.7.0)\n",
      "Building wheels for collected packages: checklist, fairscale, overrides, jsonnet, sqlitedict, subprocess32, iso-639, pathtools, patternfork-nosql, python-docx, sgmllib3k\n",
      "  Building wheel for checklist (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for checklist: filename=checklist-0.0.11-py3-none-any.whl size=12165620 sha256=f76d3594d4e50bc7d4ee173b63cddb9fc98b753281150f79f6bd0087d84bb1e6\n",
      "  Stored in directory: /tmp/xdg_cache/pip/wheels/43/3e/c3/674b24b25419fb425053ee2e3feb3515acc95552c27610167d\n",
      "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fairscale: filename=fairscale-0.4.0-py3-none-any.whl size=239930 sha256=d267503b3ea548a76862f5daaa1e82300638207c0ec4115d8445f8558ede725e\n",
      "  Stored in directory: /tmp/xdg_cache/pip/wheels/ba/a8/59/b9f425839b6970156e701b1ea4661f9f61cbc1a0f57fa2f214\n",
      "  Building wheel for overrides (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10174 sha256=c2eb5cedd232c87f756a5037856ea44812cc4dec28746fae4c325f0c3e6b29d2\n",
      "  Stored in directory: /tmp/xdg_cache/pip/wheels/6a/4f/72/28857f75625b263e2e3f5ab2fc4416c0a85960ac6485007eaa\n",
      "  Building wheel for jsonnet (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jsonnet: filename=jsonnet-0.18.0-cp38-cp38-linux_x86_64.whl size=3994036 sha256=593eaee327806147c89f520fabfbc5aa52f09d2b0e1d3ac061dda72172f2bc33\n",
      "  Stored in directory: /tmp/xdg_cache/pip/wheels/f0/ea/d5/3c0d40b9cde620d70643928d9583410e9c93471bf891bc14a5\n",
      "  Building wheel for sqlitedict (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-py3-none-any.whl size=14376 sha256=3db18dacb1e8f6c21230e09eaddcde8a14bd96ce68f9da99b6d2812354aac1cd\n",
      "  Stored in directory: /tmp/xdg_cache/pip/wheels/92/82/8c/54ef8d8770fd1a80938197e55d3ccd26eccd117f44c58f601b\n",
      "  Building wheel for subprocess32 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6487 sha256=981e058ac39fefe5c99a4e31b988a9a6fbedbed3e19ee66055b37acf936b4fa1\n",
      "  Stored in directory: /tmp/xdg_cache/pip/wheels/9f/69/d1/50b39b308a87998eaf5c1d9095e5a5bd2ad98501e2b7936d36\n",
      "  Building wheel for iso-639 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for iso-639: filename=iso_639-0.4.5-py3-none-any.whl size=169061 sha256=bbee632657501eac3a6a18b1182e1a8c6463ae8f95c59220f9faa0f42e7aacfd\n",
      "  Stored in directory: /tmp/xdg_cache/pip/wheels/ed/ce/cc/1961a4de7090b2e92895fb087abfa0080a542a5706c5948bcc\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8785 sha256=6d42b8e9cfec72c2c5712249bde7bb9b11a21b8b93529cf23f9de26d3f152b1c\n",
      "  Stored in directory: /tmp/xdg_cache/pip/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n",
      "  Building wheel for patternfork-nosql (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for patternfork-nosql: filename=patternfork_nosql-3.6-py3-none-any.whl size=22332804 sha256=3c8713439456132266b768d4dfb12f35a39fd265ab7ad53fd45e20d2a83aea42\n",
      "  Stored in directory: /tmp/xdg_cache/pip/wheels/dd/7c/f8/9eaf78f9398ddf7cc5dec58a1ad8c165ea0c28d976d422e684\n",
      "  Building wheel for python-docx (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184600 sha256=5b6ea90ff4c01e7822a1ed966c254a6a7a3d934c2da1cc9c5e9a0b790894cbad\n",
      "  Stored in directory: /tmp/xdg_cache/pip/wheels/32/b8/b2/c4c2b95765e615fe139b0b17b5ea7c0e1b6519b0a9ec8fb34d\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6066 sha256=a2aee41aa8038c804d270146dd04be1c515837d46c04d9c7d342fe33fc6d1d36\n",
      "  Stored in directory: /tmp/xdg_cache/pip/wheels/83/63/2f/117884c3b19d46b64d3d61690333aa80c88dc14050e269c546\n",
      "Successfully built checklist fairscale overrides jsonnet sqlitedict subprocess32 iso-639 pathtools patternfork-nosql python-docx sgmllib3k\n",
      "Installing collected packages: ipython, jaraco.functools, ipykernel, tempora, notebook, jaraco.text, jaraco.classes, tqdm, sgmllib3k, qtpy, portend, lxml, jaraco.collections, filelock, cheroot, qtconsole, python-docx, pdfminer.six, jupyter-console, huggingface-hub, google-crc32c, feedparser, cherrypy, botocore, backports.csv, yaspin, transformers, subprocess32, spacy, sentry-sdk, s3transfer, pyarrow, patternfork-nosql, pathtools, munch, multiprocess, jupyter, iso-639, google-resumable-media, google-cloud-core, docker-pycreds, wandb, sqlitedict, overrides, lmdb, jsonnet, google-cloud-storage, fairscale, datasets, checklist, boto3, base58, allennlp\n",
      "\u001b[33m  WARNING: The scripts iptest, iptest3, ipython and ipython3 are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script calc-prorate is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts jupyter-bundlerextension, jupyter-nbextension, jupyter-notebook and jupyter-serverextension are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script tqdm is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script cheroot is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script jupyter-console is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script huggingface-cli is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script cherryd is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script transformers-cli is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script spacy is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script plasma_store is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts wandb and wb are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script datasets-cli is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script base58 is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script allennlp is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "moto 1.3.14 requires idna<2.9,>=2.5, but you have idna 2.10 which is incompatible.\n",
      "kaggle 1.5.8 requires urllib3<1.25,>=1.21.1, but you have urllib3 1.26.8 which is incompatible.\n",
      "cloud-ml 0.0.1 requires boto3<=1.16.38,>=1.12.31, but you have boto3 1.20.39 which is incompatible.\n",
      "aws-sam-translator 1.42.0 requires jsonschema~=3.2, but you have jsonschema 4.4.0 which is incompatible.\u001b[0m\n",
      "Successfully installed allennlp-2.7.0 backports.csv-1.0.7 base58-2.1.1 boto3-1.20.39 botocore-1.23.39 checklist-0.0.11 cheroot-8.6.0 cherrypy-18.6.1 datasets-1.5.0 docker-pycreds-0.4.0 fairscale-0.4.0 feedparser-6.0.8 filelock-3.0.12 google-cloud-core-2.2.2 google-cloud-storage-1.42.3 google-crc32c-1.3.0 google-resumable-media-2.1.0 huggingface-hub-0.0.12 ipykernel-5.1.4 ipython-7.13.0 iso-639-0.4.5 jaraco.classes-3.2.1 jaraco.collections-3.5.1 jaraco.functools-3.5.0 jaraco.text-3.6.0 jsonnet-0.18.0 jupyter-1.0.0 jupyter-console-6.4.0 lmdb-1.3.0 lxml-4.7.1 multiprocess-0.70.12.2 munch-2.5.0 notebook-6.1.1 overrides-3.1.0 pathtools-0.1.2 patternfork-nosql-3.6 pdfminer.six-20211012 portend-3.1.0 pyarrow-0.17.1 python-docx-0.8.11 qtconsole-5.2.2 qtpy-2.0.0 s3transfer-0.5.0 sentry-sdk-1.5.3 sgmllib3k-1.0.0 spacy-3.1.4 sqlitedict-1.7.0 subprocess32-3.5.4 tempora-5.0.0 tqdm-4.49.0 transformers-4.9.2 wandb-0.12.9 yaspin-2.1.0\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "%pip install allennlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "4e38efb8",
   "metadata": {
    "cellId": "plah5s9fozsbm6goz6mjn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "model = RemBertForTokenClassification.from_pretrained(SAVE_PATH + '2', local_files_only=True)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "bcdefcc6",
   "metadata": {
    "cellId": "1bnk50b7zjbd9gljixjrhm"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def get_predictions(c_model, batches):\n",
    "    pred_labels = []\n",
    "\n",
    "    for item in tqdm(batches):\n",
    "        item = item[:, 0, :]\n",
    "        out = c_model(item.to(device))\n",
    "        logits = out.logits\n",
    "        logits = logits.argmax(axis=-1).tolist()\n",
    "        pred_labels.extend(logits)\n",
    "        \n",
    "    return pred_labels\n",
    "\n",
    "def prepare_preds_for_calc_metrics(pred_encoded, test_encoded, test_words):\n",
    "    pred_extended, test_extended, pred_decoded = [], [], []\n",
    "\n",
    "    for pred, test, words in tqdm(zip(pred_encoded, test_encoded, test_words)):\n",
    "        words = words.tolist()\n",
    "        \n",
    "        words_encoded = tokenizer.convert_ids_to_tokens(words)\n",
    "\n",
    "        if 0 in words:\n",
    "            cut_ind = words.index(0)\n",
    "        else:\n",
    "            cut_ind = max_len\n",
    "\n",
    "        pred, test = pred[:cut_ind], test[:cut_ind]\n",
    "        pred, test = pred[1:-1], test[1:-1]\n",
    "        \n",
    "        pred = [GLOBAL_ID2LABEL[num] for i, num in enumerate(pred) \n",
    "            if not words_encoded[i].startswith('##')]\n",
    "        test = [GLOBAL_ID2LABEL[num] for i, num in enumerate(test) \n",
    "            if not words_encoded[i].startswith('##')]\n",
    "\n",
    "        pred_extended.extend(pred)\n",
    "        pred_decoded.append(pred)\n",
    "        test_extended.append(test)\n",
    "        assert len(pred) == len(test)\n",
    "    \n",
    "    return pred_extended, test_extended, pred_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "52463d4e",
   "metadata": {
    "cellId": "v10ng3nh8wacqrydk4teu"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae9a51ffb9b4218aff439320c7a9d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11ce6745946941cbae79aa20a0e2e58f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "808caa15bc334f96b3941e402cb67d39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daea481494474035b9d501b273e0cf45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1c5c11f4c774849b55a15f35ba19c55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a44976ad7542b5825cff6153fabc7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8d38c77cd848acb334443abeadca56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "200a0badd29d46f2b31313f17a99f141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92be871cc3b94a38a67bd601b9a071d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb0d242a192845bd8639a71743eaae6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc9a457a41b746ce8a5bc2c383a7a706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf6c59a73b98474caf37739f068de7aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba577e1e61084d16bcaedc8ee7c03b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a5018487961432ea15ca867429a679a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa8620087f44127a4370f64f3b0a5db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c4beb3321174cc3b3fb11b5a9c6909e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a22f43683c1469b858ca7e635a3a530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b604e993508411bbded7396cd08d8c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c11c7c49cfa94b13a25aaf693d58e76f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1faf3fa9ad5a49b3b52f6ee759e6d6c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33fd466643804309abcee9a7b67fb96e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e404e102944befbba02dc90acbdd4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0822edeca1b343bab353d11779595ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eca9ca5d0a04cdbb14476d092fa0cfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e35de8f41bdf444fa6981f007ff7f908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0574a26b3fc42978be379943a988c55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3033ff2bf4f4d2e8ada4b13793342e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a81683596f484262a211c74eb92bea5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2142bc61f6bb4263bc46a5afdaf905fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee6551e6818e4af9afdb1347a3a3fe5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f2211e81952430ba0ad6b9e336b41ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9760177c39c44758b0085a2e3971637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "695c1b3026454bcd9bfc2734b2d5f8c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "true = {}\n",
    "preds = {}\n",
    "for l in languages:\n",
    "    dev_path = data_path + l.upper() + '/' + l + '_dev.conll'\n",
    "    dev_texts, dev_labels = parse_conll(dev_path)\n",
    "    dev_words_enc, dev_labels_enc = prepare_data_for_ner(dev_texts, dev_labels)\n",
    "    dev_data = np.stack((dev_words_enc, dev_labels_enc), axis=1)\n",
    "    dev_batches = DataLoader(dev_data, batch_size=batch_size, shuffle=False)\n",
    "    pred_labels_enc = get_predictions(model, dev_batches)\n",
    "    pred_extended, true_extended, pred_labels_decoded = prepare_preds_for_calc_metrics(\n",
    "        pred_labels_enc, dev_labels_enc, dev_words_enc\n",
    "    )\n",
    "    true[l] = true_extended\n",
    "    preds[l] = pred_labels_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "1928cb9a",
   "metadata": {
    "cellId": "6ngmqk8jyvg7h08wwd75pu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'B-PER', 'B-PER', 'I-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CW', 'B-CW', 'B-CW', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'B-PER', 'B-PER', 'I-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CW', 'B-CW', 'B-CW', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "print(true['ru'][3])\n",
    "print(preds['ru'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "a995d3b0",
   "metadata": {
    "cellId": "l3johhgx5lis3sgtptbi3s"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def get_spans(labels):\n",
    "    fin_spans = []\n",
    "    for item_ in labels:\n",
    "\n",
    "        item = deepcopy(item_)\n",
    "        item.insert(0, \"O\")\n",
    "        item.append(\"O\")\n",
    "\n",
    "        new_spans = {}\n",
    "        for i, label in enumerate(item[1:-1], 1):\n",
    "\n",
    "            if item[i] == \"O\":\n",
    "                new_spans[(i-1, i-1)] = \"O\"\n",
    "            else:\n",
    "                if item[i-1] == 'O':\n",
    "                    start_i = i\n",
    "                if item[i+1] == 'O':\n",
    "                    new_spans[(start_i-1, i-1)] = item[i].split('-')[1]\n",
    "                    \n",
    "        fin_spans.append(new_spans)\n",
    "                \n",
    "    return fin_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "fe8798eb",
   "metadata": {
    "cellId": "nh0xfzh9jcppoductw23n"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "bf25853e",
   "metadata": {
    "cellId": "rymk2cds6rfboz25eydoo"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from collections import defaultdict\n",
    "from typing import Set\n",
    "from allennlp.training.metrics.metric import Metric\n",
    "\n",
    "\n",
    "class SpanF1(Metric):\n",
    "    def __init__(self, non_entity_labels=['O']) -> None:\n",
    "        self._num_gold_mentions = 0\n",
    "        self._num_recalled_mentions = 0\n",
    "        self._num_predicted_mentions = 0\n",
    "        self._TP, self._FP, self._GT = defaultdict(int), defaultdict(int), defaultdict(int)\n",
    "        self.non_entity_labels = set(non_entity_labels)\n",
    "\n",
    "    def __call__(self, batched_predicted_spans, batched_gold_spans, sentences=None):\n",
    "        non_entity_labels = self.non_entity_labels\n",
    "        for predicted_spans, gold_spans in zip(batched_predicted_spans, batched_gold_spans):\n",
    "            gold_spans_set = set([x for x, y in gold_spans.items() if y not in non_entity_labels])\n",
    "            pred_spans_set = set([x for x, y in predicted_spans.items() if y not in non_entity_labels])\n",
    "\n",
    "            self._num_gold_mentions += len(gold_spans_set)\n",
    "            self._num_recalled_mentions += len(gold_spans_set & pred_spans_set)\n",
    "            self._num_predicted_mentions += len(pred_spans_set)\n",
    "\n",
    "            for ky, val in gold_spans.items():\n",
    "                if val not in non_entity_labels:\n",
    "                    self._GT[val] += 1\n",
    "\n",
    "            for ky, val in predicted_spans.items():\n",
    "                if val in non_entity_labels:\n",
    "                    continue\n",
    "                if ky in gold_spans and val == gold_spans[ky]:\n",
    "                    self._TP[val] += 1\n",
    "                else:\n",
    "                    self._FP[val] += 1\n",
    "    \n",
    "    def get_metric(self, reset: bool = False) -> float:\n",
    "        all_tags: Set[str] = set()\n",
    "        all_tags.update(self._TP.keys())\n",
    "        all_tags.update(self._FP.keys())\n",
    "        all_tags.update(self._GT.keys())\n",
    "        all_metrics = {}\n",
    "\n",
    "        for tag in all_tags:\n",
    "            precision, recall, f1_measure = self.compute_prf_metrics(true_positives=self._TP[tag],\n",
    "                                                                     false_negatives=self._GT[tag] - self._TP[tag],\n",
    "                                                                     false_positives=self._FP[tag])\n",
    "            all_metrics['P@{}'.format(tag)] = precision\n",
    "            all_metrics['R@{}'.format(tag)] = recall\n",
    "            all_metrics['F1@{}'.format(tag)] = f1_measure\n",
    "\n",
    "        # Compute the precision, recall and f1 for all spans jointly.\n",
    "        precision, recall, f1_measure = self.compute_prf_metrics(true_positives=sum(self._TP.values()),\n",
    "                                                                 false_positives=sum(self._FP.values()),\n",
    "                                                                 false_negatives=sum(self._GT.values())-sum(self._TP.values()))\n",
    "        all_metrics[\"micro@P\"] = precision\n",
    "        all_metrics[\"micro@R\"] = recall\n",
    "        all_metrics[\"micro@F1\"] = f1_measure\n",
    "\n",
    "        if self._num_gold_mentions == 0:\n",
    "            entity_recall = 0.0\n",
    "        else:\n",
    "            entity_recall = self._num_recalled_mentions / float(self._num_gold_mentions)\n",
    "\n",
    "        if self._num_predicted_mentions == 0:\n",
    "            entity_precision = 0.0\n",
    "        else:\n",
    "            entity_precision = self._num_recalled_mentions / float(self._num_predicted_mentions)\n",
    "\n",
    "        all_metrics['MD@R'] = entity_recall\n",
    "        all_metrics['MD@P'] = entity_precision\n",
    "        all_metrics['MD@F1'] = 2. * ((entity_precision * entity_recall) / (entity_precision + entity_recall + 1e-13))\n",
    "        all_metrics['ALLTRUE'] = self._num_gold_mentions\n",
    "        all_metrics['ALLRECALLED'] = self._num_recalled_mentions\n",
    "        all_metrics['ALLPRED'] = self._num_predicted_mentions\n",
    "        if reset:\n",
    "            self.reset()\n",
    "        return all_metrics\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_prf_metrics(true_positives: int, false_positives: int, false_negatives: int):\n",
    "        precision = float(true_positives) / float(true_positives + false_positives + 1e-13)\n",
    "        recall = float(true_positives) / float(true_positives + false_negatives + 1e-13)\n",
    "        f1_measure = 2. * ((precision * recall) / (precision + recall + 1e-13))\n",
    "        return precision, recall, f1_measure\n",
    "    \n",
    "    def reset(self):\n",
    "        self._num_gold_mentions = 0\n",
    "        self._num_recalled_mentions = 0\n",
    "        self._num_predicted_mentions = 0\n",
    "        self._TP.clear()\n",
    "        self._FP.clear()\n",
    "        self._GT.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "141be876",
   "metadata": {
    "cellId": "h69ylqj0drngj2j2lem4nu"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "a84f0644",
   "metadata": {
    "cellId": "9mxbe2i2v58kt7qokjh5s"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "metrics = {}\n",
    "\n",
    "for lang in languages:\n",
    "    true_spans = get_spans(true[lang])\n",
    "    pred_spans = get_spans(preds[lang])\n",
    "    \n",
    "    span_f1 = SpanF1()\n",
    "    span_f1(pred_spans, true_spans)\n",
    "    cur_metric = span_f1.get_metric()\n",
    "    metrics[lang] = cur_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "e3850625",
   "metadata": {
    "cellId": "4kgoshubzgjec9yq74q4g5"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "df = pd.DataFrame(index=list(metrics[\"ru\"].keys()))\n",
    "\n",
    "for lang, metric in metrics.items():\n",
    "    df[lang] = list(metric.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "dd34e9cf",
   "metadata": {
    "cellId": "j6j6vz1myvmsmw7dxf704"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bn</th>\n",
       "      <th>de</th>\n",
       "      <th>en</th>\n",
       "      <th>es</th>\n",
       "      <th>fa</th>\n",
       "      <th>hi</th>\n",
       "      <th>ko</th>\n",
       "      <th>nl</th>\n",
       "      <th>ru</th>\n",
       "      <th>tr</th>\n",
       "      <th>zh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P@PER</th>\n",
       "      <td>0.919</td>\n",
       "      <td>0.937</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R@PER</th>\n",
       "      <td>0.944</td>\n",
       "      <td>0.957</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1@PER</th>\n",
       "      <td>0.932</td>\n",
       "      <td>0.947</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.814</td>\n",
       "      <td>0.862</td>\n",
       "      <td>0.880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P@LOC</th>\n",
       "      <td>0.763</td>\n",
       "      <td>0.926</td>\n",
       "      <td>0.935</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.811</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R@LOC</th>\n",
       "      <td>0.861</td>\n",
       "      <td>0.906</td>\n",
       "      <td>0.939</td>\n",
       "      <td>0.881</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.899</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1@LOC</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.916</td>\n",
       "      <td>0.937</td>\n",
       "      <td>0.865</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.912</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.867</td>\n",
       "      <td>0.905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P@CW</th>\n",
       "      <td>0.696</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R@CW</th>\n",
       "      <td>0.650</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.673</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.799</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1@CW</th>\n",
       "      <td>0.672</td>\n",
       "      <td>0.847</td>\n",
       "      <td>0.752</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.708</td>\n",
       "      <td>0.669</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.768</td>\n",
       "      <td>0.720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P@PROD</th>\n",
       "      <td>0.630</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.743</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R@PROD</th>\n",
       "      <td>0.689</td>\n",
       "      <td>0.871</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.647</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1@PROD</th>\n",
       "      <td>0.658</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.698</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P@GRP</th>\n",
       "      <td>0.752</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.732</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.856</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.866</td>\n",
       "      <td>0.682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R@GRP</th>\n",
       "      <td>0.720</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.920</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1@GRP</th>\n",
       "      <td>0.736</td>\n",
       "      <td>0.871</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.791</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.863</td>\n",
       "      <td>0.652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P@CORP</th>\n",
       "      <td>0.706</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.661</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.912</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.881</td>\n",
       "      <td>0.842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R@CORP</th>\n",
       "      <td>0.756</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.716</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1@CORP</th>\n",
       "      <td>0.730</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.863</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro@P</th>\n",
       "      <td>0.738</td>\n",
       "      <td>0.891</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.836</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.884</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro@R</th>\n",
       "      <td>0.766</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.877</td>\n",
       "      <td>0.841</td>\n",
       "      <td>0.763</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.798</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro@F1</th>\n",
       "      <td>0.752</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.883</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MD@R</th>\n",
       "      <td>0.848</td>\n",
       "      <td>0.943</td>\n",
       "      <td>0.914</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.862</td>\n",
       "      <td>0.843</td>\n",
       "      <td>0.912</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MD@P</th>\n",
       "      <td>0.816</td>\n",
       "      <td>0.934</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.914</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MD@F1</th>\n",
       "      <td>0.831</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.912</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALLTRUE</th>\n",
       "      <td>800.000</td>\n",
       "      <td>1195.000</td>\n",
       "      <td>1215.000</td>\n",
       "      <td>1166.000</td>\n",
       "      <td>1142.000</td>\n",
       "      <td>814.000</td>\n",
       "      <td>1149.000</td>\n",
       "      <td>1136.000</td>\n",
       "      <td>1032.000</td>\n",
       "      <td>1222.000</td>\n",
       "      <td>1213.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALLRECALLED</th>\n",
       "      <td>678.000</td>\n",
       "      <td>1127.000</td>\n",
       "      <td>1110.000</td>\n",
       "      <td>1021.000</td>\n",
       "      <td>899.000</td>\n",
       "      <td>702.000</td>\n",
       "      <td>969.000</td>\n",
       "      <td>1036.000</td>\n",
       "      <td>850.000</td>\n",
       "      <td>1075.000</td>\n",
       "      <td>1074.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALLPRED</th>\n",
       "      <td>831.000</td>\n",
       "      <td>1207.000</td>\n",
       "      <td>1220.000</td>\n",
       "      <td>1174.000</td>\n",
       "      <td>1191.000</td>\n",
       "      <td>837.000</td>\n",
       "      <td>1221.000</td>\n",
       "      <td>1133.000</td>\n",
       "      <td>1058.000</td>\n",
       "      <td>1288.000</td>\n",
       "      <td>1263.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 bn       de       en       es       fa      hi       ko  \\\n",
       "P@PER         0.919    0.937    0.946    0.931    0.731   0.909    0.759   \n",
       "R@PER         0.944    0.957    0.979    0.927    0.870   0.846    0.824   \n",
       "F1@PER        0.932    0.947    0.962    0.929    0.795   0.876    0.790   \n",
       "P@LOC         0.763    0.926    0.935    0.849    0.811   0.786    0.769   \n",
       "R@LOC         0.861    0.906    0.939    0.881    0.808   0.853    0.775   \n",
       "F1@LOC        0.809    0.916    0.937    0.865    0.810   0.818    0.772   \n",
       "P@CW          0.696    0.845    0.746    0.796    0.747   0.620    0.679   \n",
       "R@CW          0.650    0.849    0.759    0.771    0.673   0.727    0.759   \n",
       "F1@CW         0.672    0.847    0.752    0.783    0.708   0.669    0.717   \n",
       "P@PROD        0.630    0.833    0.771    0.721    0.652   0.686    0.743   \n",
       "R@PROD        0.689    0.871    0.760    0.778    0.647   0.710    0.806   \n",
       "F1@PROD       0.658    0.852    0.766    0.748    0.650   0.698    0.773   \n",
       "P@GRP         0.752    0.857    0.885    0.824    0.732   0.809    0.786   \n",
       "R@GRP         0.720    0.885    0.852    0.754    0.822   0.781    0.795   \n",
       "F1@GRP        0.736    0.871    0.868    0.787    0.775   0.794    0.791   \n",
       "P@CORP        0.706    0.890    0.870    0.842    0.661   0.725    0.778   \n",
       "R@CORP        0.756    0.890    0.870    0.885    0.716   0.731    0.832   \n",
       "F1@CORP       0.730    0.890    0.870    0.863    0.687   0.728    0.804   \n",
       "micro@P       0.738    0.891    0.873    0.836    0.731   0.751    0.751   \n",
       "micro@R       0.766    0.900    0.877    0.841    0.763   0.773    0.798   \n",
       "micro@F1      0.752    0.896    0.875    0.838    0.747   0.762    0.774   \n",
       "MD@R          0.848    0.943    0.914    0.876    0.787   0.862    0.843   \n",
       "MD@P          0.816    0.934    0.910    0.870    0.755   0.839    0.794   \n",
       "MD@F1         0.831    0.938    0.912    0.873    0.771   0.850    0.818   \n",
       "ALLTRUE     800.000 1195.000 1215.000 1166.000 1142.000 814.000 1149.000   \n",
       "ALLRECALLED 678.000 1127.000 1110.000 1021.000  899.000 702.000  969.000   \n",
       "ALLPRED     831.000 1207.000 1220.000 1174.000 1191.000 837.000 1221.000   \n",
       "\n",
       "                  nl       ru       tr       zh  \n",
       "P@PER          0.966    0.796    0.826    0.838  \n",
       "R@PER          0.938    0.833    0.901    0.927  \n",
       "F1@PER         0.951    0.814    0.862    0.880  \n",
       "P@LOC          0.925    0.697    0.840    0.893  \n",
       "R@LOC          0.899    0.748    0.896    0.918  \n",
       "F1@LOC         0.912    0.721    0.867    0.905  \n",
       "P@CW           0.846    0.765    0.753    0.684  \n",
       "R@CW           0.799    0.825    0.785    0.761  \n",
       "F1@CW          0.822    0.794    0.768    0.720  \n",
       "P@PROD         0.745    0.777    0.714    0.764  \n",
       "R@PROD         0.810    0.767    0.769    0.767  \n",
       "F1@PROD        0.776    0.772    0.741    0.766  \n",
       "P@GRP          0.856    0.795    0.866    0.682  \n",
       "R@GRP          0.920    0.773    0.861    0.625  \n",
       "F1@GRP         0.887    0.784    0.863    0.652  \n",
       "P@CORP         0.912    0.840    0.881    0.842  \n",
       "R@CORP         0.895    0.808    0.905    0.833  \n",
       "F1@CORP        0.903    0.824    0.893    0.838  \n",
       "micro@P        0.884    0.772    0.816    0.800  \n",
       "micro@R        0.882    0.792    0.860    0.833  \n",
       "micro@F1       0.883    0.782    0.837    0.817  \n",
       "MD@R           0.912    0.824    0.880    0.885  \n",
       "MD@P           0.914    0.803    0.835    0.850  \n",
       "MD@F1          0.913    0.813    0.857    0.868  \n",
       "ALLTRUE     1136.000 1032.000 1222.000 1213.000  \n",
       "ALLRECALLED 1036.000  850.000 1075.000 1074.000  \n",
       "ALLPRED     1133.000 1058.000 1288.000 1263.000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0134799d",
   "metadata": {
    "cellId": "72d2so17yxghm40nsti0lf"
   },
   "source": [
    "# Run and record predictions for train set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "7087be2d",
   "metadata": {
    "cellId": "l46p5m5n7d70kd6ft386xb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "model = RemBertForTokenClassification.from_pretrained(SAVE_PATH + '2', local_files_only=True)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "fe02916c",
   "metadata": {
    "cellId": "578yjmg7c0r06wk4yob6c2h"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def get_prediction_probs(c_model, batches, dev_words_enc):\n",
    "    pred_probs = []\n",
    "    for i, item in tqdm(enumerate(dev_batches)):\n",
    "        # get prediction probabilities\n",
    "        item = item[:, 0, :]\n",
    "        out = model(item.to(device))\n",
    "        logits = torch.nn.functional.softmax(out.logits, dim=2).cpu().detach().numpy()\n",
    "        # get tokens\n",
    "        words = dev_words_enc[i].tolist()\n",
    "        words_encoded = tokenizer.convert_ids_to_tokens(words)\n",
    "        if 0 in words:\n",
    "            cut_ind = words.index(0)\n",
    "        else:\n",
    "            cut_ind = max_len\n",
    "        # record needed predictions\n",
    "        pred = [logits[0, i, :] for i in range(cut_ind) if words_encoded[i].startswith('▁')]\n",
    "        pred_probs.append(pred)\n",
    "    return pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "c16285c4",
   "metadata": {
    "cellId": "r8h5xbxkizae5emfjp2vr"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf7d06d27964130aea151d09f35b76a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c86f7b26014a86a595fb96218f154b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2864231ad8e446c18a3e4f83ec499f57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a7907d3a1d4523afdb192f07ea8372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16937f10f53b4c66a9c900657288fffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd48a11d9ab74ca3a0382cc401cc1829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b43716f419e447a6baf926560d178c0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2326b48aa30b4250a48eb86f4ba0bcd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969f2b53581c41fab1fe4f4acf2cb324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99aab5cf1623450fbdd3906145f07c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "918f69be742d41fc8c700d252de82db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3643f8d329fe4cd68045be0375f50a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50684ca75a174258b7971f60521dc4b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce58becee8664b298a26d0462030eea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12618cf6107f40dd891c6478032d2e05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a2f50fca784a5a8b8631eb36c05594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c3e854190af4ea7afb8364938acb779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29a96322ed5e442d8bb55f5ac059b496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a12fc7e99f42e79147163ec132bf01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c8304d760bb460ea9cec5a936e887b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c932723df240618621e97c0a56d9f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce15360f449a43f1afe9734f85700f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "743ab2c6ec7c403a86d14076687759e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "prediction_probs = {'label2id': GLOBAL_LABEL2ID}\n",
    "for l in tqdm(languages):\n",
    "    # read data\n",
    "    dev_path = data_path + l.upper() + '/' + l + '_train.conll'\n",
    "    dev_texts, dev_labels = parse_conll(dev_path)\n",
    "    dev_words_enc, dev_labels_enc = prepare_data_for_ner(dev_texts, dev_labels)\n",
    "    dev_data = np.stack((dev_words_enc, dev_labels_enc), axis=1)\n",
    "    dev_batches = DataLoader(dev_data, batch_size=1, shuffle=False)\n",
    "    # get prediction probabilities\n",
    "    probs = get_prediction_probs(model, dev_batches, dev_words_enc)\n",
    "    # add to dictionary\n",
    "    sentences = []\n",
    "    for sent, sent_probs, labels in zip(dev_texts, probs, dev_labels):\n",
    "        sentence = []\n",
    "        for token, prob, label in zip(sent, sent_probs, labels):\n",
    "            sentence.append({'token': token, 'output_probs': prob, 'true_label': label})\n",
    "        sentences.append(sentence)\n",
    "    prediction_probs[l] = sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "adcf7871",
   "metadata": {
    "cellId": "1iitbry1pxrw5isu8zie1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-CORP': 1,\n",
       " 'B-CW': 2,\n",
       " 'B-GRP': 3,\n",
       " 'B-LOC': 4,\n",
       " 'B-PER': 5,\n",
       " 'B-PROD': 6,\n",
       " 'I-CORP': 7,\n",
       " 'I-CW': 8,\n",
       " 'I-GRP': 9,\n",
       " 'I-LOC': 10,\n",
       " 'I-PER': 11,\n",
       " 'I-PROD': 12}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "GLOBAL_LABEL2ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "4e77128f",
   "metadata": {
    "cellId": "v6v35uzd0hax4pmbctgqrl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'token': 'his',\n",
       "  'output_probs': array([0.9999131 , 0.00001098, 0.00003176, 0.00000448, 0.00000055,\n",
       "         0.00000759, 0.00000263, 0.0000053 , 0.00001694, 0.00000272,\n",
       "         0.00000034, 0.00000269, 0.00000095], dtype=float32),\n",
       "  'true_label': 'O'},\n",
       " {'token': 'playlist',\n",
       "  'output_probs': array([0.9449786 , 0.00615626, 0.04285581, 0.00146176, 0.00009282,\n",
       "         0.0001821 , 0.00148606, 0.00087573, 0.00157215, 0.00012599,\n",
       "         0.00004352, 0.00004381, 0.00012537], dtype=float32),\n",
       "  'true_label': 'O'},\n",
       " {'token': 'includes',\n",
       "  'output_probs': array([0.99995995, 0.00000028, 0.0000008 , 0.00000044, 0.00000007,\n",
       "         0.00000017, 0.00000006, 0.00000588, 0.0000193 , 0.00001099,\n",
       "         0.00000034, 0.00000107, 0.00000064], dtype=float32),\n",
       "  'true_label': 'O'},\n",
       " {'token': 'sonny',\n",
       "  'output_probs': array([0.00104131, 0.00101925, 0.00520316, 0.00461244, 0.0001945 ,\n",
       "         0.98684025, 0.00011484, 0.00005835, 0.00026113, 0.00016429,\n",
       "         0.00001456, 0.00044428, 0.0000315 ], dtype=float32),\n",
       "  'true_label': 'B-PER'},\n",
       " {'token': 'sharrock',\n",
       "  'output_probs': array([0.00080643, 0.0000579 , 0.00033103, 0.00018403, 0.00006662,\n",
       "         0.00124777, 0.00002757, 0.00050409, 0.00296497, 0.00608399,\n",
       "         0.00009939, 0.9875432 , 0.00008298], dtype=float32),\n",
       "  'true_label': 'I-PER'},\n",
       " {'token': ',',\n",
       "  'output_probs': array([0.99994576, 0.00000023, 0.00000056, 0.00000016, 0.00000022,\n",
       "         0.00000019, 0.00000009, 0.00000817, 0.00001976, 0.00001366,\n",
       "         0.00000321, 0.00000545, 0.0000025 ], dtype=float32),\n",
       "  'true_label': 'O'},\n",
       " {'token': 'gza',\n",
       "  'output_probs': array([0.00568077, 0.0074086 , 0.01890877, 0.05970295, 0.00138445,\n",
       "         0.90164196, 0.00085596, 0.00034638, 0.00109751, 0.00120242,\n",
       "         0.0001224 , 0.00142354, 0.00022443], dtype=float32),\n",
       "  'true_label': 'B-PER'},\n",
       " {'token': ',',\n",
       "  'output_probs': array([0.9999589 , 0.00000017, 0.00000044, 0.00000012, 0.0000002 ,\n",
       "         0.00000013, 0.00000008, 0.00000571, 0.00001536, 0.00000994,\n",
       "         0.00000316, 0.00000342, 0.00000235], dtype=float32),\n",
       "  'true_label': 'O'},\n",
       " {'token': 'country',\n",
       "  'output_probs': array([0.01369464, 0.02964852, 0.06044669, 0.79557437, 0.0017929 ,\n",
       "         0.09188344, 0.00137036, 0.00063306, 0.00144808, 0.00251471,\n",
       "         0.00019686, 0.00053858, 0.00025774], dtype=float32),\n",
       "  'true_label': 'B-GRP'},\n",
       " {'token': 'teasers',\n",
       "  'output_probs': array([0.00462833, 0.00103947, 0.00187031, 0.00520631, 0.00074884,\n",
       "         0.00231484, 0.00019042, 0.03125816, 0.02387521, 0.89546466,\n",
       "         0.00151294, 0.03041028, 0.00148019], dtype=float32),\n",
       "  'true_label': 'I-GRP'},\n",
       " {'token': 'and',\n",
       "  'output_probs': array([0.9999604 , 0.00000035, 0.00000106, 0.00000049, 0.00000023,\n",
       "         0.00000032, 0.00000026, 0.00000271, 0.00002072, 0.00000709,\n",
       "         0.00000215, 0.00000188, 0.00000233], dtype=float32),\n",
       "  'true_label': 'O'},\n",
       " {'token': 'the',\n",
       "  'output_probs': array([0.00079038, 0.00038122, 0.00474164, 0.00568008, 0.00003618,\n",
       "         0.98787874, 0.00004308, 0.00002189, 0.00014495, 0.0000959 ,\n",
       "         0.0000042 , 0.00016859, 0.00001311], dtype=float32),\n",
       "  'true_label': 'B-PER'},\n",
       " {'token': 'notorious',\n",
       "  'output_probs': array([0.00029822, 0.00001369, 0.00010656, 0.00007785, 0.000019  ,\n",
       "         0.00190125, 0.00000738, 0.00026843, 0.00350724, 0.01072202,\n",
       "         0.00008344, 0.9829298 , 0.00006514], dtype=float32),\n",
       "  'true_label': 'I-PER'},\n",
       " {'token': 'b.i.g.',\n",
       "  'output_probs': array([0.0002374 , 0.00001051, 0.00007025, 0.00004943, 0.00001831,\n",
       "         0.00081621, 0.00000656, 0.00017471, 0.00263796, 0.00460925,\n",
       "         0.00006233, 0.9912595 , 0.00004772], dtype=float32),\n",
       "  'true_label': 'I-PER'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "prediction_probs['en'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "d8cef8c6",
   "metadata": {
    "cellId": "nt5qqfzddph588ty1kglf6"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import pickle\n",
    "\n",
    "with open('train_probs.pickle', 'wb') as f:\n",
    "    pickle.dump(prediction_probs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "b84cb589",
   "metadata": {
    "cellId": "66on0qnwgq7d12p5hi8cl5"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-77a388ce9491>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprediction_probs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kernel/lib/python3.8/site-packages/ml_kernel/ignored_keyboard_interrupt.py:16: UserWarning: State committing stage cannot be interrupted. Please wait.\n",
      "  warnings.warn(self._warn_message)\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "with open('train_probs.pickle', 'rb') as f:\n",
    "    b = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "8dbab4ee",
   "metadata": {
    "cellId": "dwhjxnsu1q9fanlxkye0gf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'token': 'it',\n",
       "  'output_probs': array([0.99997985, 0.00000084, 0.00000811, 0.00000375, 0.00000045,\n",
       "         0.00000123, 0.0000003 , 0.00000026, 0.00000246, 0.00000203,\n",
       "         0.00000018, 0.00000037, 0.0000001 ], dtype=float32),\n",
       "  'true_label': 'O'},\n",
       " {'token': 'is',\n",
       "  'output_probs': array([0.99999785, 0.00000004, 0.00000009, 0.00000006, 0.00000002,\n",
       "         0.00000005, 0.00000006, 0.00000007, 0.00000054, 0.00000066,\n",
       "         0.00000011, 0.00000031, 0.00000014], dtype=float32),\n",
       "  'true_label': 'O'},\n",
       " {'token': 'a',\n",
       "  'output_probs': array([0.9999962 , 0.0000001 , 0.00000119, 0.00000021, 0.00000004,\n",
       "         0.00000016, 0.00000032, 0.00000004, 0.00000115, 0.00000024,\n",
       "         0.00000009, 0.00000009, 0.00000017], dtype=float32),\n",
       "  'true_label': 'O'},\n",
       " {'token': 'series',\n",
       "  'output_probs': array([0.9999081 , 0.00000929, 0.00001826, 0.00002181, 0.00000149,\n",
       "         0.00000084, 0.00000108, 0.0000031 , 0.00002243, 0.00001206,\n",
       "         0.00000084, 0.0000005 , 0.00000036], dtype=float32),\n",
       "  'true_label': 'O'},\n",
       " {'token': 'of',\n",
       "  'output_probs': array([0.999902  , 0.00000044, 0.00000339, 0.00000179, 0.00000014,\n",
       "         0.00000024, 0.00000026, 0.00000229, 0.00006481, 0.00002191,\n",
       "         0.00000083, 0.00000074, 0.00000104], dtype=float32),\n",
       "  'true_label': 'O'},\n",
       " {'token': 'badminton',\n",
       "  'output_probs': array([0.9928508 , 0.0001698 , 0.00370686, 0.00115854, 0.00007881,\n",
       "         0.0002414 , 0.00116496, 0.00004359, 0.00026825, 0.0001792 ,\n",
       "         0.0000139 , 0.00004587, 0.00007805], dtype=float32),\n",
       "  'true_label': 'O'},\n",
       " {'token': 'tournaments',\n",
       "  'output_probs': array([0.9995521 , 0.00005739, 0.00003484, 0.00010046, 0.00000376,\n",
       "         0.00000502, 0.00000341, 0.00002894, 0.00007531, 0.0001301 ,\n",
       "         0.0000032 , 0.00000274, 0.00000273], dtype=float32),\n",
       "  'true_label': 'O'},\n",
       " {'token': ',',\n",
       "  'output_probs': array([0.9999291 , 0.00000022, 0.00000076, 0.0000003 , 0.00000037,\n",
       "         0.00000029, 0.00000023, 0.00000623, 0.00002015, 0.00002666,\n",
       "         0.00000475, 0.00000553, 0.00000542], dtype=float32),\n",
       "  'true_label': 'O'},\n",
       " {'token': 'sanctioned',\n",
       "  'output_probs': array([0.9989504 , 0.00008638, 0.00002529, 0.00075401, 0.00000507,\n",
       "         0.00000407, 0.00000285, 0.00001452, 0.00000755, 0.00014611,\n",
       "         0.00000217, 0.00000119, 0.0000004 ], dtype=float32),\n",
       "  'true_label': 'O'},\n",
       " {'token': 'by',\n",
       "  'output_probs': array([0.99997485, 0.00000053, 0.00000016, 0.00000538, 0.00000009,\n",
       "         0.0000001 , 0.00000002, 0.00000053, 0.00000048, 0.00001746,\n",
       "         0.00000032, 0.00000012, 0.00000004], dtype=float32),\n",
       "  'true_label': 'O'},\n",
       " {'token': 'badminton',\n",
       "  'output_probs': array([0.0001835 , 0.00579234, 0.00008106, 0.99277323, 0.00025162,\n",
       "         0.000029  , 0.00000766, 0.00002632, 0.0000064 , 0.00083224,\n",
       "         0.00000514, 0.00000643, 0.00000514], dtype=float32),\n",
       "  'true_label': 'B-GRP'},\n",
       " {'token': 'world',\n",
       "  'output_probs': array([0.00023935, 0.00001416, 0.0000051 , 0.00025611, 0.00004643,\n",
       "         0.00000876, 0.00000134, 0.00336158, 0.00004894, 0.9956493 ,\n",
       "         0.00032037, 0.00003223, 0.00001626], dtype=float32),\n",
       "  'true_label': 'I-GRP'},\n",
       " {'token': 'federation',\n",
       "  'output_probs': array([0.00037002, 0.00000544, 0.00000191, 0.00008955, 0.00001739,\n",
       "         0.00000444, 0.0000006 , 0.00247093, 0.00003436, 0.9967013 ,\n",
       "         0.00024617, 0.00004403, 0.00001393], dtype=float32),\n",
       "  'true_label': 'I-GRP'},\n",
       " {'token': '(',\n",
       "  'output_probs': array([0.9997925 , 0.00000332, 0.00000145, 0.0000021 , 0.00000517,\n",
       "         0.00000147, 0.00000071, 0.00005449, 0.00001472, 0.00007052,\n",
       "         0.00003295, 0.00001106, 0.00000951], dtype=float32),\n",
       "  'true_label': 'O'},\n",
       " {'token': 'bwf',\n",
       "  'output_probs': array([0.99512404, 0.00031647, 0.0001265 , 0.00144787, 0.00023466,\n",
       "         0.00008985, 0.00004156, 0.00024621, 0.0002129 , 0.00187484,\n",
       "         0.00017552, 0.00006605, 0.00004355], dtype=float32),\n",
       "  'true_label': 'O'},\n",
       " {'token': ')',\n",
       "  'output_probs': array([0.9999244 , 0.00000027, 0.00000029, 0.00000028, 0.00000059,\n",
       "         0.00000018, 0.00000016, 0.00001099, 0.00000727, 0.00003313,\n",
       "         0.00001258, 0.00000388, 0.00000602], dtype=float32),\n",
       "  'true_label': 'O'},\n",
       " {'token': 'since',\n",
       "  'output_probs': array([0.99996877, 0.00000045, 0.0000008 , 0.00000123, 0.00000043,\n",
       "         0.0000004 , 0.00000017, 0.00000146, 0.00000411, 0.00001663,\n",
       "         0.00000324, 0.0000019 , 0.0000005 ], dtype=float32),\n",
       "  'true_label': 'O'},\n",
       " {'token': '2007',\n",
       "  'output_probs': array([0.9998759 , 0.00000342, 0.00009688, 0.00000432, 0.00000119,\n",
       "         0.0000036 , 0.00000058, 0.00000058, 0.00000911, 0.00000282,\n",
       "         0.00000034, 0.00000098, 0.00000027], dtype=float32),\n",
       "  'true_label': 'O'},\n",
       " {'token': '.',\n",
       "  'output_probs': array([0.99997413, 0.0000001 , 0.00000038, 0.00000014, 0.00000027,\n",
       "         0.00000019, 0.00000013, 0.00000234, 0.00000672, 0.00000855,\n",
       "         0.00000248, 0.00000256, 0.00000206], dtype=float32),\n",
       "  'true_label': 'O'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "prediction_probs['en'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "d03c8ca4",
   "metadata": {
    "cellId": "o80o3cwazypnhje6wmemuf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'token': 'it',\n",
       "  'output_probs': array([0.99997985, 0.00000084, 0.00000811, 0.00000375, 0.00000045,\n",
       "         0.00000123, 0.0000003 , 0.00000026, 0.00000246, 0.00000203,\n",
       "         0.00000018, 0.00000037, 0.0000001 ], dtype=float32),\n",
       "  'true_label': 'O'},\n",
       " {'token': 'is',\n",
       "  'output_probs': array([0.99999785, 0.00000004, 0.00000009, 0.00000006, 0.00000002,\n",
       "         0.00000005, 0.00000006, 0.00000007, 0.00000054, 0.00000066,\n",
       "         0.00000011, 0.00000031, 0.00000014], dtype=float32),\n",
       "  'true_label': 'O'},\n",
       " {'token': 'a',\n",
       "  'output_probs': array([0.9999962 , 0.0000001 , 0.00000119, 0.00000021, 0.00000004,\n",
       "         0.00000016, 0.00000032, 0.00000004, 0.00000115, 0.00000024,\n",
       "         0.00000009, 0.00000009, 0.00000017], dtype=float32),\n",
       "  'true_label': 'O'},\n",
       " {'token': 'series',\n",
       "  'output_probs': array([0.9999081 , 0.00000929, 0.00001826, 0.00002181, 0.00000149,\n",
       "         0.00000084, 0.00000108, 0.0000031 , 0.00002243, 0.00001206,\n",
       "         0.00000084, 0.0000005 , 0.00000036], dtype=float32),\n",
       "  'true_label': 'O'},\n",
       " {'token': 'of',\n",
       "  'output_probs': array([0.999902  , 0.00000044, 0.00000339, 0.00000179, 0.00000014,\n",
       "         0.00000024, 0.00000026, 0.00000229, 0.00006481, 0.00002191,\n",
       "         0.00000083, 0.00000074, 0.00000104], dtype=float32),\n",
       "  'true_label': 'O'},\n",
       " {'token': 'badminton',\n",
       "  'output_probs': array([0.9928508 , 0.0001698 , 0.00370686, 0.00115854, 0.00007881,\n",
       "         0.0002414 , 0.00116496, 0.00004359, 0.00026825, 0.0001792 ,\n",
       "         0.0000139 , 0.00004587, 0.00007805], dtype=float32),\n",
       "  'true_label': 'O'},\n",
       " {'token': 'tournaments',\n",
       "  'output_probs': array([0.9995521 , 0.00005739, 0.00003484, 0.00010046, 0.00000376,\n",
       "         0.00000502, 0.00000341, 0.00002894, 0.00007531, 0.0001301 ,\n",
       "         0.0000032 , 0.00000274, 0.00000273], dtype=float32),\n",
       "  'true_label': 'O'},\n",
       " {'token': ',',\n",
       "  'output_probs': array([0.9999291 , 0.00000022, 0.00000076, 0.0000003 , 0.00000037,\n",
       "         0.00000029, 0.00000023, 0.00000623, 0.00002015, 0.00002666,\n",
       "         0.00000475, 0.00000553, 0.00000542], dtype=float32),\n",
       "  'true_label': 'O'},\n",
       " {'token': 'sanctioned',\n",
       "  'output_probs': array([0.9989504 , 0.00008638, 0.00002529, 0.00075401, 0.00000507,\n",
       "         0.00000407, 0.00000285, 0.00001452, 0.00000755, 0.00014611,\n",
       "         0.00000217, 0.00000119, 0.0000004 ], dtype=float32),\n",
       "  'true_label': 'O'},\n",
       " {'token': 'by',\n",
       "  'output_probs': array([0.99997485, 0.00000053, 0.00000016, 0.00000538, 0.00000009,\n",
       "         0.0000001 , 0.00000002, 0.00000053, 0.00000048, 0.00001746,\n",
       "         0.00000032, 0.00000012, 0.00000004], dtype=float32),\n",
       "  'true_label': 'O'},\n",
       " {'token': 'badminton',\n",
       "  'output_probs': array([0.0001835 , 0.00579234, 0.00008106, 0.99277323, 0.00025162,\n",
       "         0.000029  , 0.00000766, 0.00002632, 0.0000064 , 0.00083224,\n",
       "         0.00000514, 0.00000643, 0.00000514], dtype=float32),\n",
       "  'true_label': 'B-GRP'},\n",
       " {'token': 'world',\n",
       "  'output_probs': array([0.00023935, 0.00001416, 0.0000051 , 0.00025611, 0.00004643,\n",
       "         0.00000876, 0.00000134, 0.00336158, 0.00004894, 0.9956493 ,\n",
       "         0.00032037, 0.00003223, 0.00001626], dtype=float32),\n",
       "  'true_label': 'I-GRP'},\n",
       " {'token': 'federation',\n",
       "  'output_probs': array([0.00037002, 0.00000544, 0.00000191, 0.00008955, 0.00001739,\n",
       "         0.00000444, 0.0000006 , 0.00247093, 0.00003436, 0.9967013 ,\n",
       "         0.00024617, 0.00004403, 0.00001393], dtype=float32),\n",
       "  'true_label': 'I-GRP'},\n",
       " {'token': '(',\n",
       "  'output_probs': array([0.9997925 , 0.00000332, 0.00000145, 0.0000021 , 0.00000517,\n",
       "         0.00000147, 0.00000071, 0.00005449, 0.00001472, 0.00007052,\n",
       "         0.00003295, 0.00001106, 0.00000951], dtype=float32),\n",
       "  'true_label': 'O'},\n",
       " {'token': 'bwf',\n",
       "  'output_probs': array([0.99512404, 0.00031647, 0.0001265 , 0.00144787, 0.00023466,\n",
       "         0.00008985, 0.00004156, 0.00024621, 0.0002129 , 0.00187484,\n",
       "         0.00017552, 0.00006605, 0.00004355], dtype=float32),\n",
       "  'true_label': 'O'},\n",
       " {'token': ')',\n",
       "  'output_probs': array([0.9999244 , 0.00000027, 0.00000029, 0.00000028, 0.00000059,\n",
       "         0.00000018, 0.00000016, 0.00001099, 0.00000727, 0.00003313,\n",
       "         0.00001258, 0.00000388, 0.00000602], dtype=float32),\n",
       "  'true_label': 'O'},\n",
       " {'token': 'since',\n",
       "  'output_probs': array([0.99996877, 0.00000045, 0.0000008 , 0.00000123, 0.00000043,\n",
       "         0.0000004 , 0.00000017, 0.00000146, 0.00000411, 0.00001663,\n",
       "         0.00000324, 0.0000019 , 0.0000005 ], dtype=float32),\n",
       "  'true_label': 'O'},\n",
       " {'token': '2007',\n",
       "  'output_probs': array([0.9998759 , 0.00000342, 0.00009688, 0.00000432, 0.00000119,\n",
       "         0.0000036 , 0.00000058, 0.00000058, 0.00000911, 0.00000282,\n",
       "         0.00000034, 0.00000098, 0.00000027], dtype=float32),\n",
       "  'true_label': 'O'},\n",
       " {'token': '.',\n",
       "  'output_probs': array([0.99997413, 0.0000001 , 0.00000038, 0.00000014, 0.00000027,\n",
       "         0.00000019, 0.00000013, 0.00000234, 0.00000672, 0.00000855,\n",
       "         0.00000248, 0.00000256, 0.00000206], dtype=float32),\n",
       "  'true_label': 'O'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "b['en'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64798daf",
   "metadata": {
    "cellId": "pipvq40w5gcplsi10a5ctq"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Yandex DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "caae890c-252f-4044-9984-a094dcfc7ff2",
  "notebookPath": "rembert.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
